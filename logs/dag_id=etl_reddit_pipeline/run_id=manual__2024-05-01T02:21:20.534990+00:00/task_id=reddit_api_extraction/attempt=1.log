[2024-05-01T02:21:22.368+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-05-01T02:21:22.377+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_api_extraction manual__2024-05-01T02:21:20.534990+00:00 [queued]>
[2024-05-01T02:21:22.380+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_api_extraction manual__2024-05-01T02:21:20.534990+00:00 [queued]>
[2024-05-01T02:21:22.380+0000] {taskinstance.py:2303} INFO - Starting attempt 1 of 1
[2024-05-01T02:21:22.385+0000] {taskinstance.py:2327} INFO - Executing <Task(PythonOperator): reddit_api_extraction> on 2024-05-01 02:21:20.534990+00:00
[2024-05-01T02:21:22.389+0000] {standard_task_runner.py:63} INFO - Started process 82 to run task
[2024-05-01T02:21:22.391+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'etl_reddit_pipeline', 'reddit_api_extraction', 'manual__2024-05-01T02:21:20.534990+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/redditDag.py', '--cfg-path', '/tmp/tmp7dqzodv5']
[2024-05-01T02:21:22.393+0000] {standard_task_runner.py:91} INFO - Job 17: Subtask reddit_api_extraction
[2024-05-01T02:21:22.421+0000] {task_command.py:426} INFO - Running <TaskInstance: etl_reddit_pipeline.reddit_api_extraction manual__2024-05-01T02:21:20.534990+00:00 [running]> on host 4ea0e7f2323e
[2024-05-01T02:21:22.462+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Vaibhav Sharma' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline' AIRFLOW_CTX_TASK_ID='reddit_api_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-05-01T02:21:20.534990+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-05-01T02:21:20.534990+00:00'
[2024-05-01T02:21:22.462+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-05-01T02:21:22.470+0000] {logging_mixin.py:188} INFO - connected to reddit!
[2024-05-01T02:21:22.470+0000] {logging_mixin.py:188} INFO - <class 'praw.models.listing.generator.ListingGenerator'>
[2024-05-01T02:21:23.143+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello,\n\nThroughout my career, I'd only worked in frontline operations, starting from a technical support advisor role in a call center to eventually leading various operational departments. Two years ago, I transitioned to a position in data engineering, which I find deeply engaging and where I believe I provide significant value.\n\nHowever, my experience in frontline operations was characterized by constant, intense pressure and a relentless focus on delivering service from the moment I began my shift until whenever it ended. It was a highly reactive environment with little time for strategic thinking.\n\nIn contrast, my current role in data engineering requires a different pace. It involves sitting, pondering a problem and its solution for hours, experimenting with lines of code within a larger codebase, taking breaks to refresh my mind, and experiencing moments of insight while away from the desk. This slower, more deliberate approach feels foreign to me after two decades of being accustomed to the fast-paced nature of operations.\n\nI often find myself feeling guilty for not coding continuously throughout the day, even though nobody expects me to do so. In operations, if you weren't constantly responding to emails, addressing issues, monitoring real-time performance metrics, and making decisions on the spot, it was considered that you weren't fully engaged.\n\nI'm reaching out to fellow data engineering professionals who have made a similar career transition for any advice or words of wisdom. It's not so much imposter syndrome that I'm experiencing, but rather a sense of guilt stemming from my ingrained operational mindset of always being fully occupied. While data engineering certainly has its busy periods, they pale in comparison to the constant intensity of a typical day in operations. Any insights would be greatly appreciated.\n\nThanks.", 'author_fullname': 't2_6mnzdcoh', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What tips have you got?? I came from frontline operations to DE. Culture shock = 1', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgx4kj', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 29, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 29, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714494566.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello,</p>\n\n<p>Throughout my career, I&#39;d only worked in frontline operations, starting from a technical support advisor role in a call center to eventually leading various operational departments. Two years ago, I transitioned to a position in data engineering, which I find deeply engaging and where I believe I provide significant value.</p>\n\n<p>However, my experience in frontline operations was characterized by constant, intense pressure and a relentless focus on delivering service from the moment I began my shift until whenever it ended. It was a highly reactive environment with little time for strategic thinking.</p>\n\n<p>In contrast, my current role in data engineering requires a different pace. It involves sitting, pondering a problem and its solution for hours, experimenting with lines of code within a larger codebase, taking breaks to refresh my mind, and experiencing moments of insight while away from the desk. This slower, more deliberate approach feels foreign to me after two decades of being accustomed to the fast-paced nature of operations.</p>\n\n<p>I often find myself feeling guilty for not coding continuously throughout the day, even though nobody expects me to do so. In operations, if you weren&#39;t constantly responding to emails, addressing issues, monitoring real-time performance metrics, and making decisions on the spot, it was considered that you weren&#39;t fully engaged.</p>\n\n<p>I&#39;m reaching out to fellow data engineering professionals who have made a similar career transition for any advice or words of wisdom. It&#39;s not so much imposter syndrome that I&#39;m experiencing, but rather a sense of guilt stemming from my ingrained operational mindset of always being fully occupied. While data engineering certainly has its busy periods, they pale in comparison to the constant intensity of a typical day in operations. Any insights would be greatly appreciated.</p>\n\n<p>Thanks.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1cgx4kj', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='_DESTRUCTION'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgx4kj/what_tips_have_you_got_i_came_from_frontline/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgx4kj/what_tips_have_you_got_i_came_from_frontline/', 'subreddit_subscribers': 180295, 'created_utc': 1714494566.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.145+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': " Okay, I've got a situation where a customer wants to set up his 'Data Warehouse'. His data will be sourced from his accounting software API, a couple of Excel spreadsheets (maybe 2 or 3), and from his bank's CAMT.053 .csv file. The data is meant to be updated daily. It is intended that only new data will be added incrementally, which will probably result in around 100 new transactions each day.\n\nThe solution I want to build is that all this data will eventually be stored in Azure Synapse Analytics. However, to transfer the data to the data warehouse, some kind of ETL process must be in place. Is this use case significant enough to use Azure Data Factory, or is it also possible to handle the ETL via Azure Functions? (with the costing in mind) I would like to hear your opinions!", 'author_fullname': 't2_2o75st16', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'ETL; Azure Data Factory or Azure Functions', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgr0l6', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 20, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 20, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714478252.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Okay, I&#39;ve got a situation where a customer wants to set up his &#39;Data Warehouse&#39;. His data will be sourced from his accounting software API, a couple of Excel spreadsheets (maybe 2 or 3), and from his bank&#39;s CAMT.053 .csv file. The data is meant to be updated daily. It is intended that only new data will be added incrementally, which will probably result in around 100 new transactions each day.</p>\n\n<p>The solution I want to build is that all this data will eventually be stored in Azure Synapse Analytics. However, to transfer the data to the data warehouse, some kind of ETL process must be in place. Is this use case significant enough to use Azure Data Factory, or is it also possible to handle the ETL via Azure Functions? (with the costing in mind) I would like to hear your opinions!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cgr0l6', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='maarten20012001'), 'discussion_type': None, 'num_comments': 17, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgr0l6/etl_azure_data_factory_or_azure_functions/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgr0l6/etl_azure_data_factory_or_azure_functions/', 'subreddit_subscribers': 180295, 'created_utc': 1714478252.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.146+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'To add to the above, does it differ between analytics and non-analytics pipelines?\n\nFor example, if you get data from an FTP site, drop it to S3, then pick it up, blend it with other data and send it to an API, is that one pipeline? Does the pipeline then include the full lineage of the ‘other’ data? Or is each step an individual pipeline?\n\nTaking a data warehouse, where lots of data wrangling may occur across a number of levels that might be used for multiple purposes and multiple fact/dim tables, do you consider each transitory table created to be a pipeline that comes from multiple sources, or may the pipelines through and consider a pipeline to be everything including and prior to a dim or fact as a pipeline?\n\nCurious how people divvy this up in their minds.', 'author_fullname': 't2_ojr03vx2i', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What is your team definition of ‘a pipeline’?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgii4w', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': 'transparent', 'subreddit_type': 'public', 'ups': 15, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': '9ecf3c88-e787-11ed-957e-de1616aeae13', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 15, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714446284.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>To add to the above, does it differ between analytics and non-analytics pipelines?</p>\n\n<p>For example, if you get data from an FTP site, drop it to S3, then pick it up, blend it with other data and send it to an API, is that one pipeline? Does the pipeline then include the full lineage of the ‘other’ data? Or is each step an individual pipeline?</p>\n\n<p>Taking a data warehouse, where lots of data wrangling may occur across a number of levels that might be used for multiple purposes and multiple fact/dim tables, do you consider each transitory table created to be a pipeline that comes from multiple sources, or may the pipelines through and consider a pipeline to be everything including and prior to a dim or fact as a pipeline?</p>\n\n<p>Curious how people divvy this up in their minds.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Data Engineering Manager', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cgii4w', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='nydasco'), 'discussion_type': None, 'num_comments': 13, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1cgii4w/what_is_your_team_definition_of_a_pipeline/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgii4w/what_is_your_team_definition_of_a_pipeline/', 'subreddit_subscribers': 180295, 'created_utc': 1714446284.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.146+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Suggestion to buy Mac or Windows for masters program in Data Science\nSo, I'll be starting soon my masters program in Data science and wanted your opinion on which to buy-Mac or Windows based on your experience with software and all for Data related work. \nAlso, I'm kind of new to this field so yeah any kind of suggestions on how to get started will be appreciated \nMy study starts at September. ", 'author_fullname': 't2_2nr7pfdo', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Mac or Windows? ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgmfap', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714460025.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Suggestion to buy Mac or Windows for masters program in Data Science\nSo, I&#39;ll be starting soon my masters program in Data science and wanted your opinion on which to buy-Mac or Windows based on your experience with software and all for Data related work. \nAlso, I&#39;m kind of new to this field so yeah any kind of suggestions on how to get started will be appreciated \nMy study starts at September. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cgmfap', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Chemical-Current6391'), 'discussion_type': None, 'num_comments': 38, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgmfap/mac_or_windows/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgmfap/mac_or_windows/', 'subreddit_subscribers': 180295, 'created_utc': 1714460025.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.147+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I have been working as a data engineer for over a year and our company acquired another company which has been using mysql database hosted on their local server. They want to task me migrating this database to PostgreSQL on Azure.\n\nTheir database doesn’t seem huge and complicated. However, i have never done database migration before and I assume this should be different than consolidating multiple data sources into data warehouse as I usually do.\n\nI am aware l will be wearing different hats like data architect, modeller etc. but our company is not that big so I will have to do all by myself.\n\nCan someone recommend any best practises, things to consider, any article or book ?\n\nWhat is my plan? I will use Python for building data pipelines and airflow for orchestration. Too simple? Probably. What do you think?\n', 'author_fullname': 't2_2b7yft3b', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Migrating mysql db to azure ***ql', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgxn8r', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.79, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714495853.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I have been working as a data engineer for over a year and our company acquired another company which has been using mysql database hosted on their local server. They want to task me migrating this database to PostgreSQL on Azure.</p>\n\n<p>Their database doesn’t seem huge and complicated. However, i have never done database migration before and I assume this should be different than consolidating multiple data sources into data warehouse as I usually do.</p>\n\n<p>I am aware l will be wearing different hats like data architect, modeller etc. but our company is not that big so I will have to do all by myself.</p>\n\n<p>Can someone recommend any best practises, things to consider, any article or book ?</p>\n\n<p>What is my plan? I will use Python for building data pipelines and airflow for orchestration. Too simple? Probably. What do you think?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cgxn8r', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='DznFatih'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgxn8r/migrating_mysql_db_to_azure_***ql/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgxn8r/migrating_mysql_db_to_azure_***ql/', 'subreddit_subscribers': 180295, 'created_utc': 1714495853.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.148+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'So, I\'m working on a fairly large project that involves process improvement by ingesting a lot of source system data along with data that will likely come from spreadsheets (ultimately going into Snowflake).  \nI have pushed hard to get to the root data source to ingest any and all data (using Fivetran since this is our data ingestion tool) but it seems like there are going to be some instances where we simply cannot get to the root source (as in source system) of the client data and therefore we will need a "secondary" way of file ingestion where we manage flat files being sent to us and then leverage Fivetran to daily load those files into Snowflake.  \nInitially the thought was to do this via AWS s3 and have each type of file (in this case reports) sent from the client (via SFTP) to a specific s3 bucket and have Fivetran load these files from their specified buckets to a related Snowflake table.   \nThis was before we realized that you need to set up a specific connector per each s3 location in Fivetran if you want the destination for each set of daily files to go to it\'s own Snowflake table (which results in a LOT of overhead in Fivetran).  \nNow I\'m considering all potential options of how to best ingest these files. I\'m wondering if maybe using a sharepoint directory or google sheets (for the client to drop the files in) would be better, or if there\'s an even better option I\'ve yet to think of.  \nHas anyone dealt with a lot of file ingestion in the past when it comes to data warehousing? What did your end solution end up looking like?', 'author_fullname': 't2_556jqozb', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Managing file ingestion via Fivetran into Snowflake... best path forward?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgxdwe', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.8, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1714497176.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714495219.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>So, I&#39;m working on a fairly large project that involves process improvement by ingesting a lot of source system data along with data that will likely come from spreadsheets (ultimately going into Snowflake).<br/>\nI have pushed hard to get to the root data source to ingest any and all data (using Fivetran since this is our data ingestion tool) but it seems like there are going to be some instances where we simply cannot get to the root source (as in source system) of the client data and therefore we will need a &quot;secondary&quot; way of file ingestion where we manage flat files being sent to us and then leverage Fivetran to daily load those files into Snowflake.<br/>\nInitially the thought was to do this via AWS s3 and have each type of file (in this case reports) sent from the client (via SFTP) to a specific s3 bucket and have Fivetran load these files from their specified buckets to a related Snowflake table.<br/>\nThis was before we realized that you need to set up a specific connector per each s3 location in Fivetran if you want the destination for each set of daily files to go to it&#39;s own Snowflake table (which results in a LOT of overhead in Fivetran).<br/>\nNow I&#39;m considering all potential options of how to best ingest these files. I&#39;m wondering if maybe using a sharepoint directory or google sheets (for the client to drop the files in) would be better, or if there&#39;s an even better option I&#39;ve yet to think of.<br/>\nHas anyone dealt with a lot of file ingestion in the past when it comes to data warehousing? What did your end solution end up looking like?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cgxdwe', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='MasterKluch'), 'discussion_type': None, 'num_comments': 12, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgxdwe/managing_file_ingestion_via_fivetran_into/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgxdwe/managing_file_ingestion_via_fivetran_into/', 'subreddit_subscribers': 180295, 'created_utc': 1714495219.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.149+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi all, I have been tasked with creating a data model to hang all of our reports off of, unfortunately our old system was quite outdated and not the most reliable. \n\nI was told to go into Synapse and make the data model, so I did that, I made a data model that takes the raw data from our on prem SQL server to the reports. Although it works (I recreated one of our reports using the Synapse model instead of our old Visual studio model), I am no pro, I have made a lot of mistakes during this entire process (Expensive dataflows... Etc) and no doubt there are ways that I should could or need to improve it. \n\nA lot of this was trapsing through dead forum posts and YouTube videos, so if any of you guys could give me some feedback so I could update it I'd really appreciate it.\n\n[Datamodel](https://preview.redd.it/ux49dy033nxc1.png?width=1228&format=png&auto=webp&s=114692e62e4ab6df75c2ea46b0f142aeb84787c5)\n\n", 'author_fullname': 't2_sihhnnlo', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'My First Data Model', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 90, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'ux49dy033nxc1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 69, 'x': 108, 'u': 'https://preview.redd.it/ux49dy033nxc1.png?width=108&crop=smart&auto=webp&s=bcc23c344d7edec2ac179effcdc91f2ec850bf15'}, {'y': 138, 'x': 216, 'u': 'https://preview.redd.it/ux49dy033nxc1.png?width=216&crop=smart&auto=webp&s=67826ca7b9b27310c76b0e01ad97c6512a60d024'}, {'y': 205, 'x': 320, 'u': 'https://preview.redd.it/ux49dy033nxc1.png?width=320&crop=smart&auto=webp&s=e8ea04554bc1a4b8d546de81c9372a5bdff6a368'}, {'y': 411, 'x': 640, 'u': 'https://preview.redd.it/ux49dy033nxc1.png?width=640&crop=smart&auto=webp&s=ecae6cb56b6d3a2b73529fdc2fc9d6e669d3142e'}, {'y': 617, 'x': 960, 'u': 'https://preview.redd.it/ux49dy033nxc1.png?width=960&crop=smart&auto=webp&s=6ea986c6393781b97b2dc25d09ed70f0fc23dfb1'}, {'y': 694, 'x': 1080, 'u': 'https://preview.redd.it/ux49dy033nxc1.png?width=1080&crop=smart&auto=webp&s=c47fa848a20b1a16537b218ec2eb77ea439e6cb3'}], 's': {'y': 790, 'x': 1228, 'u': 'https://preview.redd.it/ux49dy033nxc1.png?width=1228&format=png&auto=webp&s=114692e62e4ab6df75c2ea46b0f142aeb84787c5'}, 'id': 'ux49dy033nxc1'}}, 'name': 't3_1cgwev0', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.8, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/tsy8oCMPRlhPn-z8FEA-EsVifH7G2Luy1WTxgfcbv3c.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714492814.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi all, I have been tasked with creating a data model to hang all of our reports off of, unfortunately our old system was quite outdated and not the most reliable. </p>\n\n<p>I was told to go into Synapse and make the data model, so I did that, I made a data model that takes the raw data from our on prem SQL server to the reports. Although it works (I recreated one of our reports using the Synapse model instead of our old Visual studio model), I am no pro, I have made a lot of mistakes during this entire process (Expensive dataflows... Etc) and no doubt there are ways that I should could or need to improve it. </p>\n\n<p>A lot of this was trapsing through dead forum posts and YouTube videos, so if any of you guys could give me some feedback so I could update it I&#39;d really appreciate it.</p>\n\n<p><a href="https://preview.redd.it/ux49dy033nxc1.png?width=1228&amp;format=png&amp;auto=webp&amp;s=114692e62e4ab6df75c2ea46b0f142aeb84787c5">Datamodel</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cgwev0', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Mathlete7'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgwev0/my_first_data_model/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgwev0/my_first_data_model/', 'subreddit_subscribers': 180295, 'created_utc': 1714492814.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.149+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi all,\n\nI know this is a common question here, but still in need of some guidance: I landed my first position as the sole data engineer for a small project, but funding may run out on me before I hit the 5-7 years that I often see required for mid-senior level data engineering positions in various job descriptions. \n\nI know the tech job market is currently on a huge downswing, so should I take advantage of the ~$5k off per calendar year I get from my work and invest in an MS in Data Architecture and Management? My thinking is that a higher degree may help make up for some of the experience I’d be lacking if/when my funding runs out in 2-3 years. I can afford it, just not sure if it’s worth the extra time and energy. There’s also the option of going for a pure MS in Software Development (which would be cheaper), but less interesting and relevant. \n\nAny advice would be appreciated! ', 'author_fullname': 't2_4xul99ve', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Worth doing a part-time masters while working in this job market?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgvmh8', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.71, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714490829.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi all,</p>\n\n<p>I know this is a common question here, but still in need of some guidance: I landed my first position as the sole data engineer for a small project, but funding may run out on me before I hit the 5-7 years that I often see required for mid-senior level data engineering positions in various job descriptions. </p>\n\n<p>I know the tech job market is currently on a huge downswing, so should I take advantage of the ~$5k off per calendar year I get from my work and invest in an MS in Data Architecture and Management? My thinking is that a higher degree may help make up for some of the experience I’d be lacking if/when my funding runs out in 2-3 years. I can afford it, just not sure if it’s worth the extra time and energy. There’s also the option of going for a pure MS in Software Development (which would be cheaper), but less interesting and relevant. </p>\n\n<p>Any advice would be appreciated! </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1cgvmh8', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='RichOkra'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgvmh8/worth_doing_a_parttime_masters_while_working_in/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgvmh8/worth_doing_a_parttime_masters_while_working_in/', 'subreddit_subscribers': 180295, 'created_utc': 1714490829.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.150+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey everyone, \n\nI have a pipeline that I need to run everyday at an interval of 30 minutes. I know I can create a schedule trigger that will create an trigger required.\n\nBut now the pipeline needs to run everyday at an interval of 30 minutes but starting at 9 am and stopping to run at 11 pm.\n\nWhat kind of trigger would be beneficial?\n\nNote: I do not want to change the notebook as it will be a big deal to the team and need to wait till next week till changes in the notebook can be pushed to prod.\n\nThanks', 'author_fullname': 't2_9q81w2wf', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'ADF trigger options', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgylvh', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714498346.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey everyone, </p>\n\n<p>I have a pipeline that I need to run everyday at an interval of 30 minutes. I know I can create a schedule trigger that will create an trigger required.</p>\n\n<p>But now the pipeline needs to run everyday at an interval of 30 minutes but starting at 9 am and stopping to run at 11 pm.</p>\n\n<p>What kind of trigger would be beneficial?</p>\n\n<p>Note: I do not want to change the notebook as it will be a big deal to the team and need to wait till next week till changes in the notebook can be pushed to prod.</p>\n\n<p>Thanks</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cgylvh', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Character_Phone8812'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgylvh/adf_trigger_options/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgylvh/adf_trigger_options/', 'subreddit_subscribers': 180295, 'created_utc': 1714498346.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.151+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi r/dataengineering,\n\nOver the last couple of months, My friend and I, and more recently some helpful contributors from the community have been working on a Terraform Provider for Apache Pinot, the provider is aimed at making it easier for developers and data engineers to integrate Apache Pinot into their infrastructure as code practices.\n\nWe believe this provider will be a game changer for those of you looking to streamline your data infrastructure and focus more on data insights rather than maintenance.\n\nThe key benefits of using a Terraform provider for Pinot are:\n\n1. Infrastructure Automation: You can utilise Terraform's powerful Infrastructure as Code capabilities to automate the setup, configuration and deployment of Apache Pinot\n2. Simplify the management of Apache Pinot Clusters: Using the provider makes it easy to Create, Update or Delete configured Apache clusters within your own Infrastructure\n3. Scalability Support: The Apache Pinot Terraform provider allows you to easily configure new Pinot nodes in response to changing demand and utilisation.\n\nFor a deeper dive into this provider and a practical example, check out the blog post: [Introducing the Apache Pinot Terraform Provider](https://blog.azaurus.dev/introducing-the-apache-pinot-terraform-provider/).\n\nCurrently the Provider has Terraform resources for:\n\n* Users\n* Schemas\n* Tables\n\nAnd many more objects as Data sources.\n\nYou can find it on the Terraform registry: [here](https://registry.terraform.io/providers/azaurus1/pinot/latest)\n\nAnd for the Go developers, there has been concurrent development on a Pinot controller library for Go, you can check it out on: [Github](https://github.com/azaurus1/go-pinot-api)\n\nWe're excited to see what you'll build with this and welcome any feedback, questions, or contributions to the project!", 'author_fullname': 't2_h9q0j', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Introducing the Apache Pinot Terraform Provider', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgnh72', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714464417.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi <a href="/r/dataengineering">r/dataengineering</a>,</p>\n\n<p>Over the last couple of months, My friend and I, and more recently some helpful contributors from the community have been working on a Terraform Provider for Apache Pinot, the provider is aimed at making it easier for developers and data engineers to integrate Apache Pinot into their infrastructure as code practices.</p>\n\n<p>We believe this provider will be a game changer for those of you looking to streamline your data infrastructure and focus more on data insights rather than maintenance.</p>\n\n<p>The key benefits of using a Terraform provider for Pinot are:</p>\n\n<ol>\n<li>Infrastructure Automation: You can utilise Terraform&#39;s powerful Infrastructure as Code capabilities to automate the setup, configuration and deployment of Apache Pinot</li>\n<li>Simplify the management of Apache Pinot Clusters: Using the provider makes it easy to Create, Update or Delete configured Apache clusters within your own Infrastructure</li>\n<li>Scalability Support: The Apache Pinot Terraform provider allows you to easily configure new Pinot nodes in response to changing demand and utilisation.</li>\n</ol>\n\n<p>For a deeper dive into this provider and a practical example, check out the blog post: <a href="https://blog.azaurus.dev/introducing-the-apache-pinot-terraform-provider/">Introducing the Apache Pinot Terraform Provider</a>.</p>\n\n<p>Currently the Provider has Terraform resources for:</p>\n\n<ul>\n<li>Users</li>\n<li>Schemas</li>\n<li>Tables</li>\n</ul>\n\n<p>And many more objects as Data sources.</p>\n\n<p>You can find it on the Terraform registry: <a href="https://registry.terraform.io/providers/azaurus1/pinot/latest">here</a></p>\n\n<p>And for the Go developers, there has been concurrent development on a Pinot controller library for Go, you can check it out on: <a href="https://github.com/azaurus1/go-pinot-api">Github</a></p>\n\n<p>We&#39;re excited to see what you&#39;ll build with this and welcome any feedback, questions, or contributions to the project!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1cgnh72', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Azaurus'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgnh72/introducing_the_apache_pinot_terraform_provider/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgnh72/introducing_the_apache_pinot_terraform_provider/', 'subreddit_subscribers': 180295, 'created_utc': 1714464417.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.151+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "**Attention Data Engineers,**\n\nI'm seeking expert advice for a data project I'm working on. I aim to set up a single table (One Big table)in my AWS RDS, which will serve as a data source for my BI tool.\n\nCurrently, my approach involves building a table through ETL, joining multiple other tables into one materialized view by SQL join operations, then renaming it from '**prod\\_table\\_temp**' to '**prod\\_table**' after dropping the previous '**prod\\_table**'.\n\nIs there a more efficient way to handle this process? The current data store is AWS RDS.\n\n  \nUpgrade possible with Redshift + DBT (We do not have Dbt expert atm )\n\nThanks in advance!", 'author_fullname': 't2_7nvr4m4i4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'seeking expert advice on creating BI tables on RDS by join operations', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cglpy1', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714457208.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p><strong>Attention Data Engineers,</strong></p>\n\n<p>I&#39;m seeking expert advice for a data project I&#39;m working on. I aim to set up a single table (One Big table)in my AWS RDS, which will serve as a data source for my BI tool.</p>\n\n<p>Currently, my approach involves building a table through ETL, joining multiple other tables into one materialized view by SQL join operations, then renaming it from &#39;<strong>prod_table_temp</strong>&#39; to &#39;<strong>prod_table</strong>&#39; after dropping the previous &#39;<strong>prod_table</strong>&#39;.</p>\n\n<p>Is there a more efficient way to handle this process? The current data store is AWS RDS.</p>\n\n<p>Upgrade possible with Redshift + DBT (We do not have Dbt expert atm )</p>\n\n<p>Thanks in advance!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cglpy1', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Flimsy-Mirror974'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cglpy1/seeking_expert_advice_on_creating_bi_tables_on/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cglpy1/seeking_expert_advice_on_creating_bi_tables_on/', 'subreddit_subscribers': 180295, 'created_utc': 1714457208.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.152+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hey all, been replying to tons of things on here lately and figured it's time I ask a question. Learning AirFlow right now in my spare time and I have a few questions on how to best go about designing DAG's (although I'm sure the example could be extended to mostly any other orchestration tools). I'm going to use a simple EL example use case for the sake of simplicity. Lets imagine I'm creating a DAG that sucks some data out of MySQL and dumps it into Postgres in a staging schema (for DBT to have it's way with after). And lets imagine there's 20 tables that need to be transferred like this daily.\n\n1. What's the best way to go about making this scalable from a maintainability perspective? For example if 20 tables might become 200 in a year. Do you manually create a new task for each table in your DAG (for example 200 Python operators)? Is it a bad idea to just maintain a Python list of tables that I iterate over and dynamically generate these tasks based on the task list? The first option seems good if I have tables that are somehow dependent on each other later down the road, but the second option seems very tempting (and possible hacky). Just trying to avoid sharp edges and footguns for building big and robust pipelines eventually.\n2. It seems like a no brainer to not have any business logic inside your DAG's but to keep them in another repo for separation of concerns between business and orchestration logic. But I see Airflow has hooks for connecting to databases instead of using something like psycopg2 or MySqlDB. Do people generally like/recommend the use of hooks? Not sure if it's possible to import airflow hooks into the business logic repo without being in the same codebase as your DAG's, it does seem slightly leaky to do that though but maybe not.\n\nThanks for any and all replies, using google to search through this subreddit has been really great to get info on things. I feel like the data engineering community has been very fragmented over the past decade but this subreddit feels like a breath of fresh air.", 'author_fullname': 't2_5zjj8', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Airflow DAG Composition Question', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1cha7ub', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1714529439.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714528100.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey all, been replying to tons of things on here lately and figured it&#39;s time I ask a question. Learning AirFlow right now in my spare time and I have a few questions on how to best go about designing DAG&#39;s (although I&#39;m sure the example could be extended to mostly any other orchestration tools). I&#39;m going to use a simple EL example use case for the sake of simplicity. Lets imagine I&#39;m creating a DAG that sucks some data out of MySQL and dumps it into Postgres in a staging schema (for DBT to have it&#39;s way with after). And lets imagine there&#39;s 20 tables that need to be transferred like this daily.</p>\n\n<ol>\n<li>What&#39;s the best way to go about making this scalable from a maintainability perspective? For example if 20 tables might become 200 in a year. Do you manually create a new task for each table in your DAG (for example 200 Python operators)? Is it a bad idea to just maintain a Python list of tables that I iterate over and dynamically generate these tasks based on the task list? The first option seems good if I have tables that are somehow dependent on each other later down the road, but the second option seems very tempting (and possible hacky). Just trying to avoid sharp edges and footguns for building big and robust pipelines eventually.</li>\n<li>It seems like a no brainer to not have any business logic inside your DAG&#39;s but to keep them in another repo for separation of concerns between business and orchestration logic. But I see Airflow has hooks for connecting to databases instead of using something like psycopg2 or MySqlDB. Do people generally like/recommend the use of hooks? Not sure if it&#39;s possible to import airflow hooks into the business logic repo without being in the same codebase as your DAG&#39;s, it does seem slightly leaky to do that though but maybe not.</li>\n</ol>\n\n<p>Thanks for any and all replies, using google to search through this subreddit has been really great to get info on things. I feel like the data engineering community has been very fragmented over the past decade but this subreddit feels like a breath of fresh air.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cha7ub', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='IrresistibleMittens'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cha7ub/airflow_dag_composition_question/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cha7ub/airflow_dag_composition_question/', 'subreddit_subscribers': 180295, 'created_utc': 1714528100.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.152+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hello data people!\n\nI\'m (still!) building an open source data visualisation site and am having lots of fun learning about all the amazing tools on the market.\n\nI have the "end" of the stack nicely set up (I\'m using Metabase for data visualisation and have a nice managed PostgreSQL server feeding into it).\n\nMost of the data that I\'m adding to this open-source library is "small" data - think CSVs of a few hundred rows. Frequently containing typos, other imperfections, and just generally needing a bit of attention before showing it publicly.\n\nI\'ve toyed with the idea of doing this locally but for scaling/collaboration I feel like doing this work somewhere in the cloud makes much more sense. As I already have infra set up, self-hosting is a preference.\n\nI gather that what I\'m looking for is something like an ETL tool. Are there any of them that aren\'t super-intimidating, are low code, and are just friendly and easy to come to grips with?\n\nKey functions I\'d like (ideally): ability to upload data from local environment; validating datasets; seeing the data; staging while it\'s being worked on; finally the ability to push it out to the database when it\'s ready.\n\nTIA!\n\n', 'author_fullname': 't2_poc45', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Looking for a cloud-hosted tool to work on CSVs before push to PostgreSQL', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgz7rk', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1714501859.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714499888.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello data people!</p>\n\n<p>I&#39;m (still!) building an open source data visualisation site and am having lots of fun learning about all the amazing tools on the market.</p>\n\n<p>I have the &quot;end&quot; of the stack nicely set up (I&#39;m using Metabase for data visualisation and have a nice managed PostgreSQL server feeding into it).</p>\n\n<p>Most of the data that I&#39;m adding to this open-source library is &quot;small&quot; data - think CSVs of a few hundred rows. Frequently containing typos, other imperfections, and just generally needing a bit of attention before showing it publicly.</p>\n\n<p>I&#39;ve toyed with the idea of doing this locally but for scaling/collaboration I feel like doing this work somewhere in the cloud makes much more sense. As I already have infra set up, self-hosting is a preference.</p>\n\n<p>I gather that what I&#39;m looking for is something like an ETL tool. Are there any of them that aren&#39;t super-intimidating, are low code, and are just friendly and easy to come to grips with?</p>\n\n<p>Key functions I&#39;d like (ideally): ability to upload data from local environment; validating datasets; seeing the data; staging while it&#39;s being worked on; finally the ability to push it out to the database when it&#39;s ready.</p>\n\n<p>TIA!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1cgz7rk', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='danielrosehill'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgz7rk/looking_for_a_cloudhosted_tool_to_work_on_csvs/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgz7rk/looking_for_a_cloudhosted_tool_to_work_on_csvs/', 'subreddit_subscribers': 180295, 'created_utc': 1714499888.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.153+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm a data analyst and I was getting sick of having to edit my `Preferences.tps`  \nfile every time I wanted to make a change to a custom colour palette... so I created a TUI to make it easier, quicker and prettier:\n\n[https://github.com/ben-n93/tab-pal](https://github.com/ben-n93/tab-pal)\n\n## Usage\n\nYou can launch the application from the command-line and make whatever changes you want and it will be reflected in Tableau.\n\nNote that you have to have Python installed.\n\n## Configuration\n\nConfiguration may be necessary if tab-pal can't find your Preferences file - create an environmental variable called `TAB_PAL_FILE` which points to the location of your Preferences file.", 'author_fullname': 't2_qlsr9p64h', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Not DE but perhaps useful for professionals here: tab-pal, a command-line app for creating and editing custom colour palettes in Tableau', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgirhu', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714447100.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m a data analyst and I was getting sick of having to edit my <code>Preferences.tps</code><br/>\nfile every time I wanted to make a change to a custom colour palette... so I created a TUI to make it easier, quicker and prettier:</p>\n\n<p><a href="https://github.com/ben-n93/tab-pal">https://github.com/ben-n93/tab-pal</a></p>\n\n<h2>Usage</h2>\n\n<p>You can launch the application from the command-line and make whatever changes you want and it will be reflected in Tableau.</p>\n\n<p>Note that you have to have Python installed.</p>\n\n<h2>Configuration</h2>\n\n<p>Configuration may be necessary if tab-pal can&#39;t find your Preferences file - create an environmental variable called <code>TAB_PAL_FILE</code> which points to the location of your Preferences file.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/PQ0Ug3yJYqijPlfdImX3hCWbCvLgQ5R5mTmrIAfHOYM.jpg?auto=webp&s=afc868a2b33e8c53707aeb78eb200257ed78e525', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/PQ0Ug3yJYqijPlfdImX3hCWbCvLgQ5R5mTmrIAfHOYM.jpg?width=108&crop=smart&auto=webp&s=7d7480b6a57df97dd9b4cbf7ea4b5367d623f1d4', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/PQ0Ug3yJYqijPlfdImX3hCWbCvLgQ5R5mTmrIAfHOYM.jpg?width=216&crop=smart&auto=webp&s=69299747b25a29d53f1e9e069d6b48368fe34b80', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/PQ0Ug3yJYqijPlfdImX3hCWbCvLgQ5R5mTmrIAfHOYM.jpg?width=320&crop=smart&auto=webp&s=3657dee58fe962e3a0512bd619a70b8666f1e791', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/PQ0Ug3yJYqijPlfdImX3hCWbCvLgQ5R5mTmrIAfHOYM.jpg?width=640&crop=smart&auto=webp&s=5d40a15c0e7fd764197fc79d83cd87077a2b9ee4', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/PQ0Ug3yJYqijPlfdImX3hCWbCvLgQ5R5mTmrIAfHOYM.jpg?width=960&crop=smart&auto=webp&s=123909928d4f02ff41280dc9ab698c6070562b97', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/PQ0Ug3yJYqijPlfdImX3hCWbCvLgQ5R5mTmrIAfHOYM.jpg?width=1080&crop=smart&auto=webp&s=ea5268be7f2cd705543ea0a1d6fb05637caeafaf', 'width': 1080, 'height': 540}], 'variants': {}, 'id': '-XUsBspkCSZHI5Cf0SmYug4HCeDB8Vq97tgGOjaP9DA'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1cgirhu', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Ok-Frosting7364'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgirhu/not_de_but_perhaps_useful_for_professionals_here/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgirhu/not_de_but_perhaps_useful_for_professionals_here/', 'subreddit_subscribers': 180295, 'created_utc': 1714447100.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.153+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi all,\n\nI need some help on my career. I am starting to get frustrated although I know I should be happy where I am at now. \n\nI started my tech career at a research organization where I learned how to build pipelines - sometimes it was parsimg large collections of PDF’s / images and validating the results, other times and towards the end I wS pulling data from API using luigi and loading into DB on a batch schedule. \n\nsome data was API, other data scraped, then used DBT to clean and transform. \n\nAnyways I built all this stuff on my own and while it was rewarding there was no career growth.\n\nIt was hard to pivot out of research but now I work as a Senior consultant with Data Engineer in title. I am on a client job and starting to get really frustratedated.  Been on this project since January and it seems liek they are purposely blocking me from doing work. My manager wants me to get experience in the things I want to do but Senior Dev’s are much more cautious it seems.\n\nWe are going through a migration and everyone is learning but it is so frustrating not to be able to do work. And it is Agile so I can’t just do the work I want to do. I am also blocked by permissions and stuff.\n\nI am literally spending my days doing trainings. I brought up testing data models multiple times N wveryone is like “yeah that can wait” - I have experience with DBT and they are using a similiar tool.\n\nAnyways I am just ranting - Just let me coook !!!!\n\nAre all jobs like this I feel so lost in my career. ', 'author_fullname': 't2_88vvqsgc', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Career Advicd', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1ch9rsl', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714526807.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi all,</p>\n\n<p>I need some help on my career. I am starting to get frustrated although I know I should be happy where I am at now. </p>\n\n<p>I started my tech career at a research organization where I learned how to build pipelines - sometimes it was parsimg large collections of PDF’s / images and validating the results, other times and towards the end I wS pulling data from API using luigi and loading into DB on a batch schedule. </p>\n\n<p>some data was API, other data scraped, then used DBT to clean and transform. </p>\n\n<p>Anyways I built all this stuff on my own and while it was rewarding there was no career growth.</p>\n\n<p>It was hard to pivot out of research but now I work as a Senior consultant with Data Engineer in title. I am on a client job and starting to get really frustratedated.  Been on this project since January and it seems liek they are purposely blocking me from doing work. My manager wants me to get experience in the things I want to do but Senior Dev’s are much more cautious it seems.</p>\n\n<p>We are going through a migration and everyone is learning but it is so frustrating not to be able to do work. And it is Agile so I can’t just do the work I want to do. I am also blocked by permissions and stuff.</p>\n\n<p>I am literally spending my days doing trainings. I brought up testing data models multiple times N wveryone is like “yeah that can wait” - I have experience with DBT and they are using a similiar tool.</p>\n\n<p>Anyways I am just ranting - Just let me coook !!!!</p>\n\n<p>Are all jobs like this I feel so lost in my career. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1ch9rsl', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='roastmecerebrally'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ch9rsl/career_advicd/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ch9rsl/career_advicd/', 'subreddit_subscribers': 180295, 'created_utc': 1714526807.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.153+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Love how [Snowflake](https://www.linkedin.com/company/snowflake-computing/) customers are using the new Cortex AI & LLM Functions in creative ways to solve even basic DataEngineering problems which would normally require custom UDF functions to be written in Python, Java & Scala  \nHere is an example of using Cortex COMPLETE() functiin via MISTRAL-7B to convert standard US Addresses into an array of individual parts (Street No, City, Zip & etc.) then using **PARSE\\_JSON()** + **GET(Array, N)** functions to split array attributes into different columns.  \nIf there is ever a concept of TOO SIMPLE to use, I think Snowflake is borderline there.\n\n\n\n>select\n\n>RawAddress,\n\n>PARSE\\_JSON(\n\n>SNOWFLAKE.CORTEX.COMPLETE(\n\n>'mistral-7b', \n\n>'Parse the given address into following array of values without any comments:\n\n>\\[address number, street name, unit number, city, state, zip\\]' || ' content: ' \n\n>|| RawAddress))::array as AddressArray,\n\n>GET(AddressArray, 0)::string as BuildingNo,\n\n>GET(AddressArray, 1)::string as Street,\n\n>GET(AddressArray, 2)::string as UnitNo,\n\n>GET(AddressArray, 3)::string as City,\n\n>GET(AddressArray, 4)::string as State,\n\n>GET(AddressArray, 5)::string as Zip\n\n>from AddressList;  \n\n\nhttps://preview.redd.it/jmv91wrsspxc1.png?width=2632&format=png&auto=webp&s=8dc1fee0ebc2b51e37c263628f9ee523d8a9f903", 'author_fullname': 't2_thx3bwe2', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Using Cortex AI & LLM Functions in Snowflake for basic data engineering tasks', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': True, 'media_metadata': {'jmv91wrsspxc1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 43, 'x': 108, 'u': 'https://preview.redd.it/jmv91wrsspxc1.png?width=108&crop=smart&auto=webp&s=00126fd92294cac5c347fa9a5d3a7393ba3b8b4d'}, {'y': 87, 'x': 216, 'u': 'https://preview.redd.it/jmv91wrsspxc1.png?width=216&crop=smart&auto=webp&s=e053386d61e3249e0d5613a3754ae11265088bf5'}, {'y': 130, 'x': 320, 'u': 'https://preview.redd.it/jmv91wrsspxc1.png?width=320&crop=smart&auto=webp&s=f9c493dc3d01d201f2b2cdf938171412687b9391'}, {'y': 260, 'x': 640, 'u': 'https://preview.redd.it/jmv91wrsspxc1.png?width=640&crop=smart&auto=webp&s=5fb0380a92c9eee855d099111015ca4faf3c72bc'}, {'y': 390, 'x': 960, 'u': 'https://preview.redd.it/jmv91wrsspxc1.png?width=960&crop=smart&auto=webp&s=8c78abd86e76984b3688bc3e9c8e65ae3017ce27'}, {'y': 439, 'x': 1080, 'u': 'https://preview.redd.it/jmv91wrsspxc1.png?width=1080&crop=smart&auto=webp&s=221d438b341eb235ddf97d12cba7acaf440c65c3'}], 's': {'y': 1070, 'x': 2632, 'u': 'https://preview.redd.it/jmv91wrsspxc1.png?width=2632&format=png&auto=webp&s=8dc1fee0ebc2b51e37c263628f9ee523d8a9f903'}, 'id': 'jmv91wrsspxc1'}}, 'name': 't3_1ch9caq', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/nwCytaRkjO-y5J0Ska73Vdlq443WTSaNhcdu3vjr6rU.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'subreddit_type': 'public', 'created': 1714525590.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Love how <a href="https://www.linkedin.com/company/snowflake-computing/">Snowflake</a> customers are using the new Cortex AI &amp; LLM Functions in creative ways to solve even basic DataEngineering problems which would normally require custom UDF functions to be written in Python, Java &amp; Scala<br/>\nHere is an example of using Cortex COMPLETE() functiin via MISTRAL-7B to convert standard US Addresses into an array of individual parts (Street No, City, Zip &amp; etc.) then using <strong>PARSE_JSON()</strong> + <strong>GET(Array, N)</strong> functions to split array attributes into different columns.<br/>\nIf there is ever a concept of TOO SIMPLE to use, I think Snowflake is borderline there.</p>\n\n<blockquote>\n<p>select</p>\n\n<p>RawAddress,</p>\n\n<p>PARSE_JSON(</p>\n\n<p>SNOWFLAKE.CORTEX.COMPLETE(</p>\n\n<p>&#39;mistral-7b&#39;, </p>\n\n<p>&#39;Parse the given address into following array of values without any comments:</p>\n\n<p>[address number, street name, unit number, city, state, zip]&#39; || &#39; content: &#39; </p>\n\n<p>|| RawAddress))::array as AddressArray,</p>\n\n<p>GET(AddressArray, 0)::string as BuildingNo,</p>\n\n<p>GET(AddressArray, 1)::string as Street,</p>\n\n<p>GET(AddressArray, 2)::string as UnitNo,</p>\n\n<p>GET(AddressArray, 3)::string as City,</p>\n\n<p>GET(AddressArray, 4)::string as State,</p>\n\n<p>GET(AddressArray, 5)::string as Zip</p>\n\n<p>from AddressList;  </p>\n</blockquote>\n\n<p><a href="https://preview.redd.it/jmv91wrsspxc1.png?width=2632&amp;format=png&amp;auto=webp&amp;s=8dc1fee0ebc2b51e37c263628f9ee523d8a9f903">https://preview.redd.it/jmv91wrsspxc1.png?width=2632&amp;format=png&amp;auto=webp&amp;s=8dc1fee0ebc2b51e37c263628f9ee523d8a9f903</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/trIo55631AWYzAt3q4i5Z41DSDMiDy1Sw504w5gcHyY.jpg?auto=webp&s=0b5f50c7dd1198444b26b5d950c7ecd44c2c6f5d', 'width': 200, 'height': 200}, 'resolutions': [{'url': 'https://external-preview.redd.it/trIo55631AWYzAt3q4i5Z41DSDMiDy1Sw504w5gcHyY.jpg?width=108&crop=smart&auto=webp&s=ad025de2c0fcdaae08d025df9c720314412107a0', 'width': 108, 'height': 108}], 'variants': {}, 'id': 'Bno3PsWuhSe8B6tbBTq6xtXc8L894lnWBX6K78icLrA'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1ch9caq', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Mr_Nickster_'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ch9caq/using_cortex_ai_llm_functions_in_snowflake_for/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ch9caq/using_cortex_ai_llm_functions_in_snowflake_for/', 'subreddit_subscribers': 180295, 'created_utc': 1714525590.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.154+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "How does employers/interviewers actually test the proficiency of applicants with regards to Cloud Services like AWS or Azure? I'm planning to apply to several companies as a Data Engineer and I just wanna have an idea with regards to this. Do they ask for demos on how you work with the said services (interaction with these services are mostly done with GUI)? Or do they just ask for a high-level perspective on how you would do a project on these services? Thank you so much in advance.", 'author_fullname': 't2_idfyrck7', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Cloud Services Job Screening', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ch4dx5', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.8, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714512548.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>How does employers/interviewers actually test the proficiency of applicants with regards to Cloud Services like AWS or Azure? I&#39;m planning to apply to several companies as a Data Engineer and I just wanna have an idea with regards to this. Do they ask for demos on how you work with the said services (interaction with these services are mostly done with GUI)? Or do they just ask for a high-level perspective on how you would do a project on these services? Thank you so much in advance.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1ch4dx5', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='This_Can_6639'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ch4dx5/cloud_services_job_screening/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ch4dx5/cloud_services_job_screening/', 'subreddit_subscribers': 180295, 'created_utc': 1714512548.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.154+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi guys,\n\nI've been working as DE for 2 years, but all the time in Foundry platform. It's my first job, and actually a good job for me, I had chance to onsite, tackle real business challenges. However, my experience has been focused mainly on Spark, a bit SQL, and tools which are highly customized for this platform.\n\nNow when I look at any JD, I don't have actual experience with these tech requirement, most of which I've only learned through self-study.\n\nI'm quite concerned about this and would appreciate any advice or experiences you might share about preparing for a transition or enhancing my skills to be more versatile in the tech industry.\n\nThank you :<", 'author_fullname': 't2_nmv5ovl0h', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Seeking advice for job change after Foundry', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgvn7f', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714490881.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi guys,</p>\n\n<p>I&#39;ve been working as DE for 2 years, but all the time in Foundry platform. It&#39;s my first job, and actually a good job for me, I had chance to onsite, tackle real business challenges. However, my experience has been focused mainly on Spark, a bit SQL, and tools which are highly customized for this platform.</p>\n\n<p>Now when I look at any JD, I don&#39;t have actual experience with these tech requirement, most of which I&#39;ve only learned through self-study.</p>\n\n<p>I&#39;m quite concerned about this and would appreciate any advice or experiences you might share about preparing for a transition or enhancing my skills to be more versatile in the tech industry.</p>\n\n<p>Thank you :&lt;</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1cgvn7f', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='lllutb'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgvn7f/seeking_advice_for_job_change_after_foundry/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgvn7f/seeking_advice_for_job_change_after_foundry/', 'subreddit_subscribers': 180295, 'created_utc': 1714490881.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.154+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I need to choose what major to go into and I am really stuck between Data and Aerospace Engineering because they both seem very fun to me. I really want to work on spacecraft outside of just coding, but at the same time I also want to learn Data Engineering because I really like math and Data seems to be full of it, not to mention I feel it also offers the flexibility of being able to enter the business world to.. Is it possible for me to get a masters in Aerospace later on or would I need to grab a separate major in Aerospace if I chose Data? Would it be a good idea to combine it with a Aerospace Engineering minor? Or would it be better to go into Aerospace and get a Data Engineering Certificate? Or would it be better to get an aerospace degree with a minor in cs and a masters in data science or engineering later on?\n\nData Engineering Certificate:\n\n[https://catalog.tamu.edu/undergraduate/engineering/industrial-systems/data-engineering-certificate/#programrequirementstext](https://catalog.tamu.edu/undergraduate/engineering/industrial-systems/data-engineering-certificate/#programrequirementstext)\n\nData Engineering Major:\n\n[https://catalog.tamu.edu/undergraduate/engineering/industrial-systems/data-engineering-bs/#programrequirementstext](https://catalog.tamu.edu/undergraduate/engineering/industrial-systems/data-engineering-bs/#programrequirementstext)\n\nAero Major:\n\n[https://catalog.tamu.edu/undergraduate/engineering/aerospace/bs/#programrequirementstext](https://catalog.tamu.edu/undergraduate/engineering/aerospace/bs/#programrequirementstext)\n\nAero Minor:\n\n[https://catalog.tamu.edu/undergraduate/engineering/aerospace/minor/](https://catalog.tamu.edu/undergraduate/engineering/aerospace/minor/)\n\nCS Minor:\n\n[https://catalog.tamu.edu/undergraduate/engineering/computer-science/minor/#programrequirementstext](https://catalog.tamu.edu/undergraduate/engineering/computer-science/minor/#programrequirementstext)', 'author_fullname': 't2_64rysrha', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data Engineering and Aerospace Engineering', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgmfbc', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1714462719.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714460027.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I need to choose what major to go into and I am really stuck between Data and Aerospace Engineering because they both seem very fun to me. I really want to work on spacecraft outside of just coding, but at the same time I also want to learn Data Engineering because I really like math and Data seems to be full of it, not to mention I feel it also offers the flexibility of being able to enter the business world to.. Is it possible for me to get a masters in Aerospace later on or would I need to grab a separate major in Aerospace if I chose Data? Would it be a good idea to combine it with a Aerospace Engineering minor? Or would it be better to go into Aerospace and get a Data Engineering Certificate? Or would it be better to get an aerospace degree with a minor in cs and a masters in data science or engineering later on?</p>\n\n<p>Data Engineering Certificate:</p>\n\n<p><a href="https://catalog.tamu.edu/undergraduate/engineering/industrial-systems/data-engineering-certificate/#programrequirementstext">https://catalog.tamu.edu/undergraduate/engineering/industrial-systems/data-engineering-certificate/#programrequirementstext</a></p>\n\n<p>Data Engineering Major:</p>\n\n<p><a href="https://catalog.tamu.edu/undergraduate/engineering/industrial-systems/data-engineering-bs/#programrequirementstext">https://catalog.tamu.edu/undergraduate/engineering/industrial-systems/data-engineering-bs/#programrequirementstext</a></p>\n\n<p>Aero Major:</p>\n\n<p><a href="https://catalog.tamu.edu/undergraduate/engineering/aerospace/bs/#programrequirementstext">https://catalog.tamu.edu/undergraduate/engineering/aerospace/bs/#programrequirementstext</a></p>\n\n<p>Aero Minor:</p>\n\n<p><a href="https://catalog.tamu.edu/undergraduate/engineering/aerospace/minor/">https://catalog.tamu.edu/undergraduate/engineering/aerospace/minor/</a></p>\n\n<p>CS Minor:</p>\n\n<p><a href="https://catalog.tamu.edu/undergraduate/engineering/computer-science/minor/#programrequirementstext">https://catalog.tamu.edu/undergraduate/engineering/computer-science/minor/#programrequirementstext</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1cgmfbc', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Karma-4U'), 'discussion_type': None, 'num_comments': 11, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgmfbc/data_engineering_and_aerospace_engineering/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgmfbc/data_engineering_and_aerospace_engineering/', 'subreddit_subscribers': 180295, 'created_utc': 1714460027.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.155+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hello, \n\nI have to create an erd of around 700 tables with Erwin. The DDL for the tables is not descriptive. No PKs or FKs. Can Erwin infer the keys? If so, how? I am able to use the Athena bridge to get the tables into Erwin, however I don’t know how to automate the relationships. Any help here would be greatly appreciated. \n\n', 'author_fullname': 't2_5z43s', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Entity Relationship Diagram with Erwin. Infer relationships? ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1ch4u7m', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714513630.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello, </p>\n\n<p>I have to create an erd of around 700 tables with Erwin. The DDL for the tables is not descriptive. No PKs or FKs. Can Erwin infer the keys? If so, how? I am able to use the Athena bridge to get the tables into Erwin, however I don’t know how to automate the relationships. Any help here would be greatly appreciated. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1ch4u7m', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='boboshoes'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1ch4u7m/entity_relationship_diagram_with_erwin_infer/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1ch4u7m/entity_relationship_diagram_with_erwin_infer/', 'subreddit_subscribers': 180295, 'created_utc': 1714513630.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.155+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey Im setting up a CI pipeline for data \nFactory. I’m super new a definitely shouldn’t be setting this stuff up, but that’s the way it is unfortunately.\n\none thing I don’t get is do I need sql server in the dev and test resource groups. Can I just have a sql server in prod with multiple databases one for dev, test and prod.\n\nThere’s tutorials for data factory that I’ve went through and for setting up Git lab but nothing really covers putting it all together.. any diagrams, support / input would be appreciated.\n\nAlso would it be better to use data factory for the full etl or notebooks for transformations?', 'author_fullname': 't2_wf3q5q8', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data factory git lab question', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgwgs9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714492932.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey Im setting up a CI pipeline for data \nFactory. I’m super new a definitely shouldn’t be setting this stuff up, but that’s the way it is unfortunately.</p>\n\n<p>one thing I don’t get is do I need sql server in the dev and test resource groups. Can I just have a sql server in prod with multiple databases one for dev, test and prod.</p>\n\n<p>There’s tutorials for data factory that I’ve went through and for setting up Git lab but nothing really covers putting it all together.. any diagrams, support / input would be appreciated.</p>\n\n<p>Also would it be better to use data factory for the full etl or notebooks for transformations?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cgwgs9', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='connoza'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgwgs9/data_factory_git_lab_question/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgwgs9/data_factory_git_lab_question/', 'subreddit_subscribers': 180295, 'created_utc': 1714492932.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.155+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi!\n\nI'm currently using Kobo Toolbox for in-field data capture and have found its offline capabilities and robustness quite effective for my needs. However, I'm facing a significant challenge with updating forms, particularly drop-down lists, which requires manually editing an XLS form and uploading it each time a change is needed. This process is cumbersome and I'm hoping to streamline it.\n\nI'm looking for suggestions on two fronts:\n\nAlternative Tools: Are there any in-field data capture tools similar to Kobo that allow for automatic updating of forms? The tool should be cost-effective, robust, and capable of working offline.\n\nKobo Toolbox Solutions: If you know of any integrations or techniques to automate form updates within Kobo Toolbox, that information would be highly valuable.\n\nAny advice or recommendations you could share would be greatly appreciated. Thanks in advance for your help!", 'author_fullname': 't2_t1mvr7oy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Seeking Recommendations: In-field Data Capture Tools with Auto-Updating Forms Similar to Kobo Toolbox', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgtjq5', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714485614.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi!</p>\n\n<p>I&#39;m currently using Kobo Toolbox for in-field data capture and have found its offline capabilities and robustness quite effective for my needs. However, I&#39;m facing a significant challenge with updating forms, particularly drop-down lists, which requires manually editing an XLS form and uploading it each time a change is needed. This process is cumbersome and I&#39;m hoping to streamline it.</p>\n\n<p>I&#39;m looking for suggestions on two fronts:</p>\n\n<p>Alternative Tools: Are there any in-field data capture tools similar to Kobo that allow for automatic updating of forms? The tool should be cost-effective, robust, and capable of working offline.</p>\n\n<p>Kobo Toolbox Solutions: If you know of any integrations or techniques to automate form updates within Kobo Toolbox, that information would be highly valuable.</p>\n\n<p>Any advice or recommendations you could share would be greatly appreciated. Thanks in advance for your help!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cgtjq5', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='yaksurf'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgtjq5/seeking_recommendations_infield_data_capture/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgtjq5/seeking_recommendations_infield_data_capture/', 'subreddit_subscribers': 180295, 'created_utc': 1714485614.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.155+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'SO, I am trying to create a simple pipeline between Zoho and our Warehouse using Hevo. Since Zoho only allows OAuth Authentication, I set everything up in Zoho and created a client with a client id and a secret. I got a **grant token** and this allowed me to get an **access token and a refresh token** with POSTMAN \n\nI put Hevo\'s redirect URL into my Zoho client\'s redirect URLS!\n\nThen, I went to Hevo, select my endpoint: \n\n    https://books.zoho.eu/api/v3/invoices\n\nSelected OAuth as authentication. Created a token using my client id and secret, used zoho\'s auth url: [https://accounts.zoho.com/oauth/v2/auth](https://accounts.zoho.com/oauth/v2/auth)  \nand zoho\'s toke URL: [https://accounts.zoho.com/oauth/v2/token](https://accounts.zoho.com/oauth/v2/token)\n\nselected the CORRECT scope, but when I test i get an error.\n\nReceived HTTP Status: 400 and response: <html> <head> <title>Zoho Accounts</title> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <link href="[https://static.zohocdn.com/iam/v2/components/css/zoho](https://static.zohocdn.com/iam/v2/components/css/zohoPuvi.c86bbb480e4a4fbc379fd8e7298bbde5.css)......  \n. This request is incorrect or corrupt. Please check the API documentation.\n\n\n\nDoes anyone here has experience setting the Zoho API as a Hevo source?', 'author_fullname': 't2_ppyprrvz', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How to connect Zoho and Hevo via OAUTH? I am entering my keys but I keep getting errorrs.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgqr6r', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.6, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714477378.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>SO, I am trying to create a simple pipeline between Zoho and our Warehouse using Hevo. Since Zoho only allows OAuth Authentication, I set everything up in Zoho and created a client with a client id and a secret. I got a <strong>grant token</strong> and this allowed me to get an <strong>access token and a refresh token</strong> with POSTMAN </p>\n\n<p>I put Hevo&#39;s redirect URL into my Zoho client&#39;s redirect URLS!</p>\n\n<p>Then, I went to Hevo, select my endpoint: </p>\n\n<pre><code>https://books.zoho.eu/api/v3/invoices\n</code></pre>\n\n<p>Selected OAuth as authentication. Created a token using my client id and secret, used zoho&#39;s auth url: <a href="https://accounts.zoho.com/oauth/v2/auth">https://accounts.zoho.com/oauth/v2/auth</a><br/>\nand zoho&#39;s toke URL: <a href="https://accounts.zoho.com/oauth/v2/token">https://accounts.zoho.com/oauth/v2/token</a></p>\n\n<p>selected the CORRECT scope, but when I test i get an error.</p>\n\n<p>Received HTTP Status: 400 and response: &lt;html&gt; &lt;head&gt; &lt;title&gt;Zoho Accounts&lt;/title&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;link href=&quot;<a href="https://static.zohocdn.com/iam/v2/components/css/zohoPuvi.c86bbb480e4a4fbc379fd8e7298bbde5.css">https://static.zohocdn.com/iam/v2/components/css/zoho</a>......<br/>\n. This request is incorrect or corrupt. Please check the API documentation.</p>\n\n<p>Does anyone here has experience setting the Zoho API as a Hevo source?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cgqr6r', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Realistic_Salary_942'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgqr6r/how_to_connect_zoho_and_hevo_via_oauth_i_am/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgqr6r/how_to_connect_zoho_and_hevo_via_oauth_i_am/', 'subreddit_subscribers': 180295, 'created_utc': 1714477378.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.156+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "My company has this old approach built by a contractor of moving data from Redshift using IICS to orchestrate moving data from Redshift into S3. There's another redshift database that houses all the change capture data. Then with Databricks they do ALTER TABLE to repoint to the new table in s3 to sync.\n\nIt's a super clunky process and we're looking to rebuild it in house. Has anyone tried syncing data from redshift into databricks? Seems like we're not utilizing the databricks functionality the best way. These tables are also parquets.", 'author_fullname': 't2_7saronit', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Syncing Redshift to Databricks advice', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgvk33', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714490666.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>My company has this old approach built by a contractor of moving data from Redshift using IICS to orchestrate moving data from Redshift into S3. There&#39;s another redshift database that houses all the change capture data. Then with Databricks they do ALTER TABLE to repoint to the new table in s3 to sync.</p>\n\n<p>It&#39;s a super clunky process and we&#39;re looking to rebuild it in house. Has anyone tried syncing data from redshift into databricks? Seems like we&#39;re not utilizing the databricks functionality the best way. These tables are also parquets.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cgvk33', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='crossfirex35'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgvk33/syncing_redshift_to_databricks_advice/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgvk33/syncing_redshift_to_databricks_advice/', 'subreddit_subscribers': 180295, 'created_utc': 1714490666.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.156+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I was curious to know about the volume of data handled. What is the highest volume of data you have handled in your data engineering projects?\nAlso if it interests you, please do share your experience in how do you handle data, tools and technology etc\n\n[View Poll](https://www.reddit.com/poll/1cgy01u)', 'author_fullname': 't2_ci308gob', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What volume of data is handled in data engineering projects?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgy01u', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.33, 'author_flair_background_color': 'transparent', 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': 'fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714496874.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I was curious to know about the volume of data handled. What is the highest volume of data you have handled in your data engineering projects?\nAlso if it interests you, please do share your experience in how do you handle data, tools and technology etc</p>\n\n<p><a href="https://www.reddit.com/poll/1cgy01u">View Poll</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Data Engineer', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cgy01u', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Delicious_Attempt_99'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'poll_data': <praw.models.reddit.poll.PollData object at 0xffff94e06400>, 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1cgy01u/what_volume_of_data_is_handled_in_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'mod_reports': [], 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgy01u/what_volume_of_data_is_handled_in_data/', 'subreddit_subscribers': 180295, 'created_utc': 1714496874.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.156+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "[C++ DataFrame](https://github.com/hosseinmoein/DataFrame) was designed to extend the C++ ecosystem and provide a tool for efficient processing of large datasets. It is widely used in Financial and AI industries. You don't need an advanced knowledge of C++ to use it. There is also comprehensive documentation with code samples.\n\nDataFrame is designed using modern C++, extensive multithreading, and SIMD techniques. It also provides a large collection of built-in analytical routines.", 'author_fullname': 't2_jeyhnly', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'C++ DataFrame for efficient analysis of large datasets', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgtewr', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.4, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714485258.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p><a href="https://github.com/hosseinmoein/DataFrame">C++ DataFrame</a> was designed to extend the C++ ecosystem and provide a tool for efficient processing of large datasets. It is widely used in Financial and AI industries. You don&#39;t need an advanced knowledge of C++ to use it. There is also comprehensive documentation with code samples.</p>\n\n<p>DataFrame is designed using modern C++, extensive multithreading, and SIMD techniques. It also provides a large collection of built-in analytical routines.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/_uuFpP_nkFIX8_cG1UOKlV8bMX7flKq6kltguFuXJ34.jpg?auto=webp&s=80b7e39259f18f59d3522fe34d884d768ccf059d', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/_uuFpP_nkFIX8_cG1UOKlV8bMX7flKq6kltguFuXJ34.jpg?width=108&crop=smart&auto=webp&s=f709bc1b8f1dbcba4c903142587b31011b752996', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/_uuFpP_nkFIX8_cG1UOKlV8bMX7flKq6kltguFuXJ34.jpg?width=216&crop=smart&auto=webp&s=383df985392be8ad15f1bac307b2492530396433', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/_uuFpP_nkFIX8_cG1UOKlV8bMX7flKq6kltguFuXJ34.jpg?width=320&crop=smart&auto=webp&s=b072004e583333e2986eae512c049171af80adb9', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'BhRrik4lWsvKvlY42tNQHqju8uNOFg3QY2K4_xlC4N8'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1cgtewr', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='hmoein'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgtewr/c_dataframe_for_efficient_analysis_of_large/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgtewr/c_dataframe_for_efficient_analysis_of_large/', 'subreddit_subscribers': 180295, 'created_utc': 1714485258.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.157+0000] {logging_mixin.py:188} INFO - post dict {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff94ed59a0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Would it be a bad career choice to to get into data engineering and only apply to Scala/Java jobs?', 'author_fullname': 't2_2ws5ddue', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'I would like to get into Data engineering but I despise working with Python.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgo8ty', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.29, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714467702.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Would it be a bad career choice to to get into data engineering and only apply to Scala/Java jobs?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1cgo8ty', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='God_of_failure'), 'discussion_type': None, 'num_comments': 39, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgo8ty/i_would_like_to_get_into_data_engineering_but_i/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgo8ty/i_would_like_to_get_into_data_engineering_but_i/', 'subreddit_subscribers': 180295, 'created_utc': 1714467702.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-05-01T02:21:23.219+0000] {python.py:237} INFO - Done. Returned value was: /opt/airflow/data/output
[2024-05-01T02:21:23.220+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-05-01T02:21:23.264+0000] {taskinstance.py:1205} INFO - Marking task as SUCCESS. dag_id=etl_reddit_pipeline, task_id=reddit_api_extraction, execution_date=20240501T022120, start_date=20240501T022122, end_date=20240501T022123
[2024-05-01T02:21:23.302+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-05-01T02:21:23.332+0000] {taskinstance.py:3482} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-05-01T02:21:23.475+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
