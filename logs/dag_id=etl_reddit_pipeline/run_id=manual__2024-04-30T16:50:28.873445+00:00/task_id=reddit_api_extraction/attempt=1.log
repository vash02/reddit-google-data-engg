[2024-04-30T16:54:26.143+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-04-30T16:54:26.180+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_api_extraction manual__2024-04-30T16:50:28.873445+00:00 [queued]>
[2024-04-30T16:54:26.191+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_api_extraction manual__2024-04-30T16:50:28.873445+00:00 [queued]>
[2024-04-30T16:54:26.192+0000] {taskinstance.py:2303} INFO - Starting attempt 1 of 1
[2024-04-30T16:54:26.218+0000] {taskinstance.py:2327} INFO - Executing <Task(PythonOperator): reddit_api_extraction> on 2024-04-30 16:50:28.873445+00:00
[2024-04-30T16:54:26.234+0000] {standard_task_runner.py:63} INFO - Started process 66 to run task
[2024-04-30T16:54:26.241+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'etl_reddit_pipeline', 'reddit_api_extraction', 'manual__2024-04-30T16:50:28.873445+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/redditDag.py', '--cfg-path', '/tmp/tmpdwbcmln8']
[2024-04-30T16:54:26.247+0000] {standard_task_runner.py:91} INFO - Job 5: Subtask reddit_api_extraction
[2024-04-30T16:54:26.375+0000] {task_command.py:426} INFO - Running <TaskInstance: etl_reddit_pipeline.reddit_api_extraction manual__2024-04-30T16:50:28.873445+00:00 [running]> on host 8b3d06fd7e37
[2024-04-30T16:54:26.520+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Vaibhav Sharma' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline' AIRFLOW_CTX_TASK_ID='reddit_api_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-04-30T16:50:28.873445+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-04-30T16:50:28.873445+00:00'
[2024-04-30T16:54:26.526+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-04-30T16:54:26.551+0000] {logging_mixin.py:188} INFO - connected to reddit!
[2024-04-30T16:54:27.716+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi everyone, not long ago I found this post (link below) about using DuckDB to import data from S3, transform it and load it back into S3, I'm trying to build a DWH in the company where I work, and I think object storage would be great for our use case since there are different kinds of sources and formats (Not sure tho, I'm new to this kind of tasks, I'm just a Data Analyst :/). Based on my research I could use Polars instead of DuckDB for this, but the question is, would it be the best idea? \n\nThank you all and sorry if I cannot explain myself properly, I'm not a DE, but trying to become one using my company's situation. \n\nPost: https://www.reddit.com/r/dataengineering/s/VqsOU6Si0l \n\nEDIT \nSorry for not detailing more context but here it is: \n\n- Data size is small, less than 100GB considering *ALL* our data, and the files I work with range from 20MB all the way up to few hundred MBs \n\n- IT team (Yes, whole dept) is comprised of only 2 SEs, and me, DA. Most data projects fall in my hands and I'm competent both in Python and SQL but nowhere near expert level\n\n- Currently no infrastructure (No cloud, everything data analytics is done locally in my machine) but management is looking to improve all data processes, meaning creating a DWH, improve reporting and self-service for more people ", 'author_fullname': 't2_x5ocjrnsw', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': "Poor man's Data Lake using Polars (¿?)", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cge0dw', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.98, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 50, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 50, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1714490491.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714433541.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone, not long ago I found this post (link below) about using DuckDB to import data from S3, transform it and load it back into S3, I&#39;m trying to build a DWH in the company where I work, and I think object storage would be great for our use case since there are different kinds of sources and formats (Not sure tho, I&#39;m new to this kind of tasks, I&#39;m just a Data Analyst :/). Based on my research I could use Polars instead of DuckDB for this, but the question is, would it be the best idea? </p>\n\n<p>Thank you all and sorry if I cannot explain myself properly, I&#39;m not a DE, but trying to become one using my company&#39;s situation. </p>\n\n<p>Post: <a href="https://www.reddit.com/r/dataengineering/s/VqsOU6Si0l">https://www.reddit.com/r/dataengineering/s/VqsOU6Si0l</a> </p>\n\n<p>EDIT \nSorry for not detailing more context but here it is: </p>\n\n<ul>\n<li><p>Data size is small, less than 100GB considering <em>ALL</em> our data, and the files I work with range from 20MB all the way up to few hundred MBs </p></li>\n<li><p>IT team (Yes, whole dept) is comprised of only 2 SEs, and me, DA. Most data projects fall in my hands and I&#39;m competent both in Python and SQL but nowhere near expert level</p></li>\n<li><p>Currently no infrastructure (No cloud, everything data analytics is done locally in my machine) but management is looking to improve all data processes, meaning creating a DWH, improve reporting and self-service for more people </p></li>\n</ul>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cge0dw', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Bavender-Lrown'), 'discussion_type': None, 'num_comments': 28, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cge0dw/poor_mans_data_lake_using_polars/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cge0dw/poor_mans_data_lake_using_polars/', 'subreddit_subscribers': 180197, 'created_utc': 1714433541.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.718+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I want to understand how difficult is data modelling. I've worked on data management projects, planning to move to a data modelling project. But everyone have been discouraging that it might be difficult for me. \n\nCan any experienced folks guide me how to excel in data modelling?", 'author_fullname': 't2_i96qhyw4r', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How difficult is data modelling?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cg5afw', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.94, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 23, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 23, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714412542.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I want to understand how difficult is data modelling. I&#39;ve worked on data management projects, planning to move to a data modelling project. But everyone have been discouraging that it might be difficult for me. </p>\n\n<p>Can any experienced folks guide me how to excel in data modelling?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cg5afw', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='mindmybusine55'), 'discussion_type': None, 'num_comments': 24, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cg5afw/how_difficult_is_data_modelling/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cg5afw/how_difficult_is_data_modelling/', 'subreddit_subscribers': 180197, 'created_utc': 1714412542.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.718+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm working on a hobby project and trying to apply DE concepts and stuff to it to learn. My project involves parsing text from news websites with an NLP processor, getting words, characters, pos, etc from each text so I can compare them later.\n\nI could easily do this by saving my extracted data to Postgres directly but I wanted to play with parquet files and BigQuery external tables from those.   \n  \nSo I started saving the dataframe with results of the initial parsing of each article to its own parquet file in GCS, then these are read together in a BigQuery table. I could also easily do this by saving the tables as csv, for instance (The files have 2 with columns 200-300 rows each, and I like the idea of having each article generating its own file for later processing).\n\nIs this a smart way to use parquet? Or, for instance, when there are hundreds of thousands of files, will this become an issue for reading into the external table? Is there a better way to organize this?", 'author_fullname': 't2_m3tn2vhhn', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Does it make sense to use small parquet files?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgbmcq', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 15, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 15, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714427545.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m working on a hobby project and trying to apply DE concepts and stuff to it to learn. My project involves parsing text from news websites with an NLP processor, getting words, characters, pos, etc from each text so I can compare them later.</p>\n\n<p>I could easily do this by saving my extracted data to Postgres directly but I wanted to play with parquet files and BigQuery external tables from those.   </p>\n\n<p>So I started saving the dataframe with results of the initial parsing of each article to its own parquet file in GCS, then these are read together in a BigQuery table. I could also easily do this by saving the tables as csv, for instance (The files have 2 with columns 200-300 rows each, and I like the idea of having each article generating its own file for later processing).</p>\n\n<p>Is this a smart way to use parquet? Or, for instance, when there are hundreds of thousands of files, will this become an issue for reading into the external table? Is there a better way to organize this?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cgbmcq', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Time_Simple_3250'), 'discussion_type': None, 'num_comments': 13, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgbmcq/does_it_make_sense_to_use_small_parquet_files/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgbmcq/does_it_make_sense_to_use_small_parquet_files/', 'subreddit_subscribers': 180197, 'created_utc': 1714427545.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.719+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'To add to the above, does it differ between analytics and non-analytics pipelines?\n\nFor example, if you get data from an FTP site, drop it to S3, then pick it up, blend it with other data and send it to an API, is that one pipeline? Does the pipeline then include the full lineage of the ‘other’ data? Or is each step an individual pipeline?\n\nTaking a data warehouse, where lots of data wrangling may occur across a number of levels that might be used for multiple purposes and multiple fact/dim tables, do you consider each transitory table created to be a pipeline that comes from multiple sources, or may the pipelines through and consider a pipeline to be everything including and prior to a dim or fact as a pipeline?\n\nCurious how people divvy this up in their minds.', 'author_fullname': 't2_ojr03vx2i', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What is your team definition of ‘a pipeline’?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgii4w', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': 'transparent', 'subreddit_type': 'public', 'ups': 13, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': '9ecf3c88-e787-11ed-957e-de1616aeae13', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 13, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714446284.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>To add to the above, does it differ between analytics and non-analytics pipelines?</p>\n\n<p>For example, if you get data from an FTP site, drop it to S3, then pick it up, blend it with other data and send it to an API, is that one pipeline? Does the pipeline then include the full lineage of the ‘other’ data? Or is each step an individual pipeline?</p>\n\n<p>Taking a data warehouse, where lots of data wrangling may occur across a number of levels that might be used for multiple purposes and multiple fact/dim tables, do you consider each transitory table created to be a pipeline that comes from multiple sources, or may the pipelines through and consider a pipeline to be everything including and prior to a dim or fact as a pipeline?</p>\n\n<p>Curious how people divvy this up in their minds.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Data Engineering Manager', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cgii4w', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='nydasco'), 'discussion_type': None, 'num_comments': 12, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1cgii4w/what_is_your_team_definition_of_a_pipeline/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgii4w/what_is_your_team_definition_of_a_pipeline/', 'subreddit_subscribers': 180197, 'created_utc': 1714446284.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.719+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello guys,\n\nSeeking your help on this age long problem that I have... I've been doing people management for about \\~6 years now, but have more than 10+ years experience in the data engineering field (either as an ETL developer using Informatica, Datastage, SSIS, unix, RDBMS SQL or someone whose task is to test, debug, and migrate datawarehousing solutions). My problem is I don't feel happy and fulfilled doing people management and would like to go back to the technology space... Back to my roots which is data engineering. Given the technology landscape for data engineering nowadays, how do you suggest that I restart my learning journey?", 'author_fullname': 't2_rpbzu3gz', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'From people management back to data engineering', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgfixv', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.87, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 11, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 11, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714437648.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello guys,</p>\n\n<p>Seeking your help on this age long problem that I have... I&#39;ve been doing people management for about ~6 years now, but have more than 10+ years experience in the data engineering field (either as an ETL developer using Informatica, Datastage, SSIS, unix, RDBMS SQL or someone whose task is to test, debug, and migrate datawarehousing solutions). My problem is I don&#39;t feel happy and fulfilled doing people management and would like to go back to the technology space... Back to my roots which is data engineering. Given the technology landscape for data engineering nowadays, how do you suggest that I restart my learning journey?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1cgfixv', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='rmdcss'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgfixv/from_people_management_back_to_data_engineering/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgfixv/from_people_management_back_to_data_engineering/', 'subreddit_subscribers': 180197, 'created_utc': 1714437648.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.719+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': " Okay, I've got a situation where a customer wants to set up his 'Data Warehouse'. His data will be sourced from his accounting software API, a couple of Excel spreadsheets (maybe 2 or 3), and from his bank's CAMT.053 .csv file. The data is meant to be updated daily. It is intended that only new data will be added incrementally, which will probably result in around 100 new transactions each day.\n\nThe solution I want to build is that all this data will eventually be stored in Azure Synapse Analytics. However, to transfer the data to the data warehouse, some kind of ETL process must be in place. Is this use case significant enough to use Azure Data Factory, or is it also possible to handle the ETL via Azure Functions? (with the costing in mind) I would like to hear your opinions!", 'author_fullname': 't2_2o75st16', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'ETL; Azure Data Factory or Azure Functions', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgr0l6', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 12, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 12, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714478252.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Okay, I&#39;ve got a situation where a customer wants to set up his &#39;Data Warehouse&#39;. His data will be sourced from his accounting software API, a couple of Excel spreadsheets (maybe 2 or 3), and from his bank&#39;s CAMT.053 .csv file. The data is meant to be updated daily. It is intended that only new data will be added incrementally, which will probably result in around 100 new transactions each day.</p>\n\n<p>The solution I want to build is that all this data will eventually be stored in Azure Synapse Analytics. However, to transfer the data to the data warehouse, some kind of ETL process must be in place. Is this use case significant enough to use Azure Data Factory, or is it also possible to handle the ETL via Azure Functions? (with the costing in mind) I would like to hear your opinions!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1cgr0l6', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='maarten20012001'), 'discussion_type': None, 'num_comments': 13, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgr0l6/etl_azure_data_factory_or_azure_functions/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgr0l6/etl_azure_data_factory_or_azure_functions/', 'subreddit_subscribers': 180197, 'created_utc': 1714478252.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.719+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I have over 15 years of experience as a SQL Server DBA and Business Intelligence developer. I know about performance tuning, data modeling and I\'m looking to transition to Data Engineering.\n\nI had the chance to work for a client as a Data Engineer for two years, I am proficient in Python and Snowflake, and recently I completed the Data Engineer Associate Certification from Data Camp (which I consider as "good" for a starting point).\n\nWhat can I do to be more attractive to potential employers? All advice is welcome.\n\n  \nEdit: Forgot to mention, all of these is reflected on my resumé and LinkedIn profile.', 'author_fullname': 't2_21ed3v64', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '15 YoE DBA struggling to transition to Data Engineering', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgarz0', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 9, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 9, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714425503.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I have over 15 years of experience as a SQL Server DBA and Business Intelligence developer. I know about performance tuning, data modeling and I&#39;m looking to transition to Data Engineering.</p>\n\n<p>I had the chance to work for a client as a Data Engineer for two years, I am proficient in Python and Snowflake, and recently I completed the Data Engineer Associate Certification from Data Camp (which I consider as &quot;good&quot; for a starting point).</p>\n\n<p>What can I do to be more attractive to potential employers? All advice is welcome.</p>\n\n<p>Edit: Forgot to mention, all of these is reflected on my resumé and LinkedIn profile.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1cgarz0', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='sqldevmty'), 'discussion_type': None, 'num_comments': 7, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgarz0/15_yoe_dba_struggling_to_transition_to_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgarz0/15_yoe_dba_struggling_to_transition_to_data/', 'subreddit_subscribers': 180197, 'created_utc': 1714425503.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.720+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'With traditional ERP/MES (Oracle, SAP, etc.), data gets moved data to a data warehouse. Then it either gets pulled into a report, a data mart, or users pull it directly for use in ad hoc analysis.\n\nHowever historian data is different. It’s typically time series data with millions of rows generated per week (size depends on number of pieces of equipment in a facility). This includes production metrics, machine parameters, vision data, and sometimes condition monitoring data.\n\nProduction metrics for business folks are easy. Query/Aggregate using the proprietary querying tool made by the historian and feed that data to a data mart or data warehouse.\n\nBut engineers often need all the details. It seems unreasonable to move all the data into a traditional DB for individuals to be able to query into analysis tools (Excel, Minitab, etc). But I’d also like to allow people to use the tools they are familiar with instead of learning how to navigate historian viewing screens or using their proprietary querying software. Is there a time series DB that would allow users to query like a traditional SQL DB through an ODBC connection? Is there some visualization  software that would allow people to look through data easily and pull what they actually need? How can I connect such large quantities of data for ML use?\n\nLooking for any advice on how users (engineers, project leaders, etc.) tend to interact with large time series datasets in manufacturing or outside of manufacturing.', 'author_fullname': 't2_fgc39', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Those who have worked in manufacturing, how do you expose data to users from a historian (Time series DB)?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cge00i', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.74, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714433516.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>With traditional ERP/MES (Oracle, SAP, etc.), data gets moved data to a data warehouse. Then it either gets pulled into a report, a data mart, or users pull it directly for use in ad hoc analysis.</p>\n\n<p>However historian data is different. It’s typically time series data with millions of rows generated per week (size depends on number of pieces of equipment in a facility). This includes production metrics, machine parameters, vision data, and sometimes condition monitoring data.</p>\n\n<p>Production metrics for business folks are easy. Query/Aggregate using the proprietary querying tool made by the historian and feed that data to a data mart or data warehouse.</p>\n\n<p>But engineers often need all the details. It seems unreasonable to move all the data into a traditional DB for individuals to be able to query into analysis tools (Excel, Minitab, etc). But I’d also like to allow people to use the tools they are familiar with instead of learning how to navigate historian viewing screens or using their proprietary querying software. Is there a time series DB that would allow users to query like a traditional SQL DB through an ODBC connection? Is there some visualization  software that would allow people to look through data easily and pull what they actually need? How can I connect such large quantities of data for ML use?</p>\n\n<p>Looking for any advice on how users (engineers, project leaders, etc.) tend to interact with large time series datasets in manufacturing or outside of manufacturing.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cge00i', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='TheOnlinePolak'), 'discussion_type': None, 'num_comments': 7, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cge00i/those_who_have_worked_in_manufacturing_how_do_you/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cge00i/those_who_have_worked_in_manufacturing_how_do_you/', 'subreddit_subscribers': 180197, 'created_utc': 1714433516.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.720+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi everyone, I have seen many resources here in reddit about bigdata,\nbut if you had to re-learn  it, what would you do? which resources/books would you use? What project would you pick as first hands on project/tutorial just for learning\n', 'author_fullname': 't2_u7fsj6m26', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How do you learn Big data from scratch?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgexvz', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714436057.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone, I have seen many resources here in reddit about bigdata,\nbut if you had to re-learn  it, what would you do? which resources/books would you use? What project would you pick as first hands on project/tutorial just for learning</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cgexvz', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ubiond'), 'discussion_type': None, 'num_comments': 13, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgexvz/how_do_you_learn_big_data_from_scratch/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgexvz/how_do_you_learn_big_data_from_scratch/', 'subreddit_subscribers': 180197, 'created_utc': 1714436057.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.720+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "**Attention Data Engineers,**\n\nI'm seeking expert advice for a data project I'm working on. I aim to set up a single table (One Big table)in my AWS RDS, which will serve as a data source for my BI tool.\n\nCurrently, my approach involves building a table through ETL, joining multiple other tables into one materialized view by SQL join operations, then renaming it from '**prod\\_table\\_temp**' to '**prod\\_table**' after dropping the previous '**prod\\_table**'.\n\nIs there a more efficient way to handle this process? The current data store is AWS RDS.\n\n  \nUpgrade possible with Redshift + DBT (We do not have Dbt expert atm )\n\nThanks in advance!", 'author_fullname': 't2_7nvr4m4i4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'seeking expert advice on creating BI tables on RDS by join operations', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cglpy1', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.84, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714457208.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p><strong>Attention Data Engineers,</strong></p>\n\n<p>I&#39;m seeking expert advice for a data project I&#39;m working on. I aim to set up a single table (One Big table)in my AWS RDS, which will serve as a data source for my BI tool.</p>\n\n<p>Currently, my approach involves building a table through ETL, joining multiple other tables into one materialized view by SQL join operations, then renaming it from &#39;<strong>prod_table_temp</strong>&#39; to &#39;<strong>prod_table</strong>&#39; after dropping the previous &#39;<strong>prod_table</strong>&#39;.</p>\n\n<p>Is there a more efficient way to handle this process? The current data store is AWS RDS.</p>\n\n<p>Upgrade possible with Redshift + DBT (We do not have Dbt expert atm )</p>\n\n<p>Thanks in advance!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cglpy1', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Flimsy-Mirror974'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cglpy1/seeking_expert_advice_on_creating_bi_tables_on/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cglpy1/seeking_expert_advice_on_creating_bi_tables_on/', 'subreddit_subscribers': 180197, 'created_utc': 1714457208.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.720+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi r/dataengineering,\n\nOver the last couple of months, My friend and I, and more recently some helpful contributors from the community have been working on a Terraform Provider for Apache Pinot, the provider is aimed at making it easier for developers and data engineers to integrate Apache Pinot into their infrastructure as code practices.\n\nWe believe this provider will be a game changer for those of you looking to streamline your data infrastructure and focus more on data insights rather than maintenance.\n\nThe key benefits of using a Terraform provider for Pinot are:\n\n1. Infrastructure Automation: You can utilise Terraform's powerful Infrastructure as Code capabilities to automate the setup, configuration and deployment of Apache Pinot\n2. Simplify the management of Apache Pinot Clusters: Using the provider makes it easy to Create, Update or Delete configured Apache clusters within your own Infrastructure\n3. Scalability Support: The Apache Pinot Terraform provider allows you to easily configure new Pinot nodes in response to changing demand and utilisation.\n\nFor a deeper dive into this provider and a practical example, check out the blog post: [Introducing the Apache Pinot Terraform Provider](https://blog.azaurus.dev/introducing-the-apache-pinot-terraform-provider/).\n\nCurrently the Provider has Terraform resources for:\n\n* Users\n* Schemas\n* Tables\n\nAnd many more objects as Data sources.\n\nYou can find it on the Terraform registry: [here](https://registry.terraform.io/providers/azaurus1/pinot/latest)\n\nAnd for the Go developers, there has been concurrent development on a Pinot controller library for Go, you can check it out on: [Github](https://github.com/azaurus1/go-pinot-api)\n\nWe're excited to see what you'll build with this and welcome any feedback, questions, or contributions to the project!", 'author_fullname': 't2_h9q0j', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Introducing the Apache Pinot Terraform Provider', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgnh72', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714464417.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi <a href="/r/dataengineering">r/dataengineering</a>,</p>\n\n<p>Over the last couple of months, My friend and I, and more recently some helpful contributors from the community have been working on a Terraform Provider for Apache Pinot, the provider is aimed at making it easier for developers and data engineers to integrate Apache Pinot into their infrastructure as code practices.</p>\n\n<p>We believe this provider will be a game changer for those of you looking to streamline your data infrastructure and focus more on data insights rather than maintenance.</p>\n\n<p>The key benefits of using a Terraform provider for Pinot are:</p>\n\n<ol>\n<li>Infrastructure Automation: You can utilise Terraform&#39;s powerful Infrastructure as Code capabilities to automate the setup, configuration and deployment of Apache Pinot</li>\n<li>Simplify the management of Apache Pinot Clusters: Using the provider makes it easy to Create, Update or Delete configured Apache clusters within your own Infrastructure</li>\n<li>Scalability Support: The Apache Pinot Terraform provider allows you to easily configure new Pinot nodes in response to changing demand and utilisation.</li>\n</ol>\n\n<p>For a deeper dive into this provider and a practical example, check out the blog post: <a href="https://blog.azaurus.dev/introducing-the-apache-pinot-terraform-provider/">Introducing the Apache Pinot Terraform Provider</a>.</p>\n\n<p>Currently the Provider has Terraform resources for:</p>\n\n<ul>\n<li>Users</li>\n<li>Schemas</li>\n<li>Tables</li>\n</ul>\n\n<p>And many more objects as Data sources.</p>\n\n<p>You can find it on the Terraform registry: <a href="https://registry.terraform.io/providers/azaurus1/pinot/latest">here</a></p>\n\n<p>And for the Go developers, there has been concurrent development on a Pinot controller library for Go, you can check it out on: <a href="https://github.com/azaurus1/go-pinot-api">Github</a></p>\n\n<p>We&#39;re excited to see what you&#39;ll build with this and welcome any feedback, questions, or contributions to the project!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1cgnh72', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Azaurus'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgnh72/introducing_the_apache_pinot_terraform_provider/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgnh72/introducing_the_apache_pinot_terraform_provider/', 'subreddit_subscribers': 180197, 'created_utc': 1714464417.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.721+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I need to choose what major to go into and I am really stuck between Data and Aerospace Engineering because they both seem very fun to me. I really want to work on spacecraft outside of just coding, but at the same time I also want to learn Data Engineering because I really like math and Data seems to be full of it, not to mention I feel it also offers the flexibility of being able to enter the business world to.. Is it possible for me to get a masters in Aerospace later on or would I need to grab a separate major in Aerospace if I chose Data? Would it be a good idea to combine it with a Aerospace Engineering minor? Or would it be better to go into Aerospace and get a Data Engineering Certificate? Or would it be better to get an aerospace degree with a minor in cs and a masters in data science or engineering later on?\n\nData Engineering Certificate:\n\n[https://catalog.tamu.edu/undergraduate/engineering/industrial-systems/data-engineering-certificate/#programrequirementstext](https://catalog.tamu.edu/undergraduate/engineering/industrial-systems/data-engineering-certificate/#programrequirementstext)\n\nData Engineering Major:\n\n[https://catalog.tamu.edu/undergraduate/engineering/industrial-systems/data-engineering-bs/#programrequirementstext](https://catalog.tamu.edu/undergraduate/engineering/industrial-systems/data-engineering-bs/#programrequirementstext)\n\nAero Major:\n\n[https://catalog.tamu.edu/undergraduate/engineering/aerospace/bs/#programrequirementstext](https://catalog.tamu.edu/undergraduate/engineering/aerospace/bs/#programrequirementstext)\n\nAero Minor:\n\n[https://catalog.tamu.edu/undergraduate/engineering/aerospace/minor/](https://catalog.tamu.edu/undergraduate/engineering/aerospace/minor/)\n\nCS Minor:\n\n[https://catalog.tamu.edu/undergraduate/engineering/computer-science/minor/#programrequirementstext](https://catalog.tamu.edu/undergraduate/engineering/computer-science/minor/#programrequirementstext)', 'author_fullname': 't2_64rysrha', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data Engineering and Aerospace Engineering', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgmfbc', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1714462719.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714460027.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I need to choose what major to go into and I am really stuck between Data and Aerospace Engineering because they both seem very fun to me. I really want to work on spacecraft outside of just coding, but at the same time I also want to learn Data Engineering because I really like math and Data seems to be full of it, not to mention I feel it also offers the flexibility of being able to enter the business world to.. Is it possible for me to get a masters in Aerospace later on or would I need to grab a separate major in Aerospace if I chose Data? Would it be a good idea to combine it with a Aerospace Engineering minor? Or would it be better to go into Aerospace and get a Data Engineering Certificate? Or would it be better to get an aerospace degree with a minor in cs and a masters in data science or engineering later on?</p>\n\n<p>Data Engineering Certificate:</p>\n\n<p><a href="https://catalog.tamu.edu/undergraduate/engineering/industrial-systems/data-engineering-certificate/#programrequirementstext">https://catalog.tamu.edu/undergraduate/engineering/industrial-systems/data-engineering-certificate/#programrequirementstext</a></p>\n\n<p>Data Engineering Major:</p>\n\n<p><a href="https://catalog.tamu.edu/undergraduate/engineering/industrial-systems/data-engineering-bs/#programrequirementstext">https://catalog.tamu.edu/undergraduate/engineering/industrial-systems/data-engineering-bs/#programrequirementstext</a></p>\n\n<p>Aero Major:</p>\n\n<p><a href="https://catalog.tamu.edu/undergraduate/engineering/aerospace/bs/#programrequirementstext">https://catalog.tamu.edu/undergraduate/engineering/aerospace/bs/#programrequirementstext</a></p>\n\n<p>Aero Minor:</p>\n\n<p><a href="https://catalog.tamu.edu/undergraduate/engineering/aerospace/minor/">https://catalog.tamu.edu/undergraduate/engineering/aerospace/minor/</a></p>\n\n<p>CS Minor:</p>\n\n<p><a href="https://catalog.tamu.edu/undergraduate/engineering/computer-science/minor/#programrequirementstext">https://catalog.tamu.edu/undergraduate/engineering/computer-science/minor/#programrequirementstext</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1cgmfbc', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Karma-4U'), 'discussion_type': None, 'num_comments': 8, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgmfbc/data_engineering_and_aerospace_engineering/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgmfbc/data_engineering_and_aerospace_engineering/', 'subreddit_subscribers': 180197, 'created_utc': 1714460027.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.721+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Suggestion to buy Mac or Windows for masters program in Data Science\nSo, I'll be starting soon my masters program in Data science and wanted your opinion on which to buy-Mac or Windows based on your experience with software and all for Data related work. \nAlso, I'm kind of new to this field so yeah any kind of suggestions on how to get started will be appreciated \nMy study starts at September. ", 'author_fullname': 't2_2nr7pfdo', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Mac or Windows? ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgmfap', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.7, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714460025.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Suggestion to buy Mac or Windows for masters program in Data Science\nSo, I&#39;ll be starting soon my masters program in Data science and wanted your opinion on which to buy-Mac or Windows based on your experience with software and all for Data related work. \nAlso, I&#39;m kind of new to this field so yeah any kind of suggestions on how to get started will be appreciated \nMy study starts at September. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cgmfap', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Chemical-Current6391'), 'discussion_type': None, 'num_comments': 25, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgmfap/mac_or_windows/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgmfap/mac_or_windows/', 'subreddit_subscribers': 180197, 'created_utc': 1714460025.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.721+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm a data analyst and I was getting sick of having to edit my `Preferences.tps`  \nfile every time I wanted to make a change to a custom colour palette... so I created a TUI to make it easier, quicker and prettier:\n\n[https://github.com/ben-n93/tab-pal](https://github.com/ben-n93/tab-pal)\n\n## Usage\n\nYou can launch the application from the command-line and make whatever changes you want and it will be reflected in Tableau.\n\nNote that you have to have Python installed.\n\n## Configuration\n\nConfiguration may be necessary if tab-pal can't find your Preferences file - create an environmental variable called `TAB_PAL_FILE` which points to the location of your Preferences file.", 'author_fullname': 't2_qlsr9p64h', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Not DE but perhaps useful for professionals here: tab-pal, a command-line app for creating and editing custom colour palettes in Tableau', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgirhu', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714447100.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m a data analyst and I was getting sick of having to edit my <code>Preferences.tps</code><br/>\nfile every time I wanted to make a change to a custom colour palette... so I created a TUI to make it easier, quicker and prettier:</p>\n\n<p><a href="https://github.com/ben-n93/tab-pal">https://github.com/ben-n93/tab-pal</a></p>\n\n<h2>Usage</h2>\n\n<p>You can launch the application from the command-line and make whatever changes you want and it will be reflected in Tableau.</p>\n\n<p>Note that you have to have Python installed.</p>\n\n<h2>Configuration</h2>\n\n<p>Configuration may be necessary if tab-pal can&#39;t find your Preferences file - create an environmental variable called <code>TAB_PAL_FILE</code> which points to the location of your Preferences file.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/PQ0Ug3yJYqijPlfdImX3hCWbCvLgQ5R5mTmrIAfHOYM.jpg?auto=webp&s=afc868a2b33e8c53707aeb78eb200257ed78e525', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/PQ0Ug3yJYqijPlfdImX3hCWbCvLgQ5R5mTmrIAfHOYM.jpg?width=108&crop=smart&auto=webp&s=7d7480b6a57df97dd9b4cbf7ea4b5367d623f1d4', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/PQ0Ug3yJYqijPlfdImX3hCWbCvLgQ5R5mTmrIAfHOYM.jpg?width=216&crop=smart&auto=webp&s=69299747b25a29d53f1e9e069d6b48368fe34b80', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/PQ0Ug3yJYqijPlfdImX3hCWbCvLgQ5R5mTmrIAfHOYM.jpg?width=320&crop=smart&auto=webp&s=3657dee58fe962e3a0512bd619a70b8666f1e791', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/PQ0Ug3yJYqijPlfdImX3hCWbCvLgQ5R5mTmrIAfHOYM.jpg?width=640&crop=smart&auto=webp&s=5d40a15c0e7fd764197fc79d83cd87077a2b9ee4', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/PQ0Ug3yJYqijPlfdImX3hCWbCvLgQ5R5mTmrIAfHOYM.jpg?width=960&crop=smart&auto=webp&s=123909928d4f02ff41280dc9ab698c6070562b97', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/PQ0Ug3yJYqijPlfdImX3hCWbCvLgQ5R5mTmrIAfHOYM.jpg?width=1080&crop=smart&auto=webp&s=ea5268be7f2cd705543ea0a1d6fb05637caeafaf', 'width': 1080, 'height': 540}], 'variants': {}, 'id': '-XUsBspkCSZHI5Cf0SmYug4HCeDB8Vq97tgGOjaP9DA'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1cgirhu', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Ok-Frosting7364'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgirhu/not_de_but_perhaps_useful_for_professionals_here/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgirhu/not_de_but_perhaps_useful_for_professionals_here/', 'subreddit_subscribers': 180197, 'created_utc': 1714447100.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.721+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I would like to know what database or index to use for storing documents for information retrieval and possible rag. I want to try a hybrid aproach with sparse and dense vectors. I might use an llm later for rag. What would you guys suggest? I have about 200-400k documents (most of them only up to 1 page long) I was looking at opensearch and meilisearch, but wanted to get some suggestions first.', 'author_fullname': 't2_izo59', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Storing documents for information retrieval', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cg9hpp', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714422506.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I would like to know what database or index to use for storing documents for information retrieval and possible rag. I want to try a hybrid aproach with sparse and dense vectors. I might use an llm later for rag. What would you guys suggest? I have about 200-400k documents (most of them only up to 1 page long) I was looking at opensearch and meilisearch, but wanted to get some suggestions first.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cg9hpp', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='CaptainSnackbar'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cg9hpp/storing_documents_for_information_retrieval/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cg9hpp/storing_documents_for_information_retrieval/', 'subreddit_subscribers': 180197, 'created_utc': 1714422506.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.722+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi all,\n\nI know this is a common question here, but still in need of some guidance: I landed my first position as the sole data engineer for a small project, but funding may run out on me before I hit the 5-7 years that I often see required for mid-senior level data engineering positions in various job descriptions. \n\nI know the tech job market is currently on a huge downswing, so should I take advantage of the ~$5k off per calendar year I get from my work and invest in an MS in Data Architecture and Management? My thinking is that a higher degree may help make up for some of the experience I’d be lacking if/when my funding runs out in 2-3 years. I can afford it, just not sure if it’s worth the extra time and energy. There’s also the option of going for a pure MS in Software Development (which would be cheaper), but less interesting and relevant. \n\nAny advice would be appreciated! ', 'author_fullname': 't2_4xul99ve', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Worth doing a part-time masters while working in this job market?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1cgvmh8', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714490829.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi all,</p>\n\n<p>I know this is a common question here, but still in need of some guidance: I landed my first position as the sole data engineer for a small project, but funding may run out on me before I hit the 5-7 years that I often see required for mid-senior level data engineering positions in various job descriptions. </p>\n\n<p>I know the tech job market is currently on a huge downswing, so should I take advantage of the ~$5k off per calendar year I get from my work and invest in an MS in Data Architecture and Management? My thinking is that a higher degree may help make up for some of the experience I’d be lacking if/when my funding runs out in 2-3 years. I can afford it, just not sure if it’s worth the extra time and energy. There’s also the option of going for a pure MS in Software Development (which would be cheaper), but less interesting and relevant. </p>\n\n<p>Any advice would be appreciated! </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1cgvmh8', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='RichOkra'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgvmh8/worth_doing_a_parttime_masters_while_working_in/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgvmh8/worth_doing_a_parttime_masters_while_working_in/', 'subreddit_subscribers': 180197, 'created_utc': 1714490829.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.722+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi everyone! I wrote my first ever blog post on prompt-based feature engineering! Would\nlove to hear what everyone thinks! :)\n\n[edit: reposted with original title now]\n', 'author_fullname': 't2_wqqdgabdt', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Prompt-Based Feature Engineering (Part 1) - Generative AI Generates Data', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 86, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgamcw', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/AusaG_IIjOY-r3I6wPkyzwvlHMvIM4AVVs8LfAWruPY.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1714425141.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'nqln.substack.com', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone! I wrote my first ever blog post on prompt-based feature engineering! Would\nlove to hear what everyone thinks! :)</p>\n\n<p>[edit: reposted with original title now]</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://nqln.substack.com/p/prompt-based-feature-engineering', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/A9fKv4LXwtVvkiDWQqgC2V76YYfWQrMDe4clgqEeTFg.jpg?auto=webp&s=9f30604a5ba4ee688809a04535214756dc3382e0', 'width': 970, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/A9fKv4LXwtVvkiDWQqgC2V76YYfWQrMDe4clgqEeTFg.jpg?width=108&crop=smart&auto=webp&s=b620015f9e42051bde04577987e1c0cf4d523fb4', 'width': 108, 'height': 66}, {'url': 'https://external-preview.redd.it/A9fKv4LXwtVvkiDWQqgC2V76YYfWQrMDe4clgqEeTFg.jpg?width=216&crop=smart&auto=webp&s=a02b88e0c37e42bedf1c174f111f6cd2ed946ed3', 'width': 216, 'height': 133}, {'url': 'https://external-preview.redd.it/A9fKv4LXwtVvkiDWQqgC2V76YYfWQrMDe4clgqEeTFg.jpg?width=320&crop=smart&auto=webp&s=fc53a3c86db291123625ed8cd49eb9abac263c5c', 'width': 320, 'height': 197}, {'url': 'https://external-preview.redd.it/A9fKv4LXwtVvkiDWQqgC2V76YYfWQrMDe4clgqEeTFg.jpg?width=640&crop=smart&auto=webp&s=a37cc30332d61b4b37adb2b2a6d52b5d8c487a58', 'width': 640, 'height': 395}, {'url': 'https://external-preview.redd.it/A9fKv4LXwtVvkiDWQqgC2V76YYfWQrMDe4clgqEeTFg.jpg?width=960&crop=smart&auto=webp&s=a48453aeef52521f727f8d7a2d265fcfdb37c3b3', 'width': 960, 'height': 593}], 'variants': {}, 'id': 'BkxuEypjqwSDmW89yshrMwIfdWenknWFIWjTTGWt4_4'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1cgamcw', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='yin_se'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgamcw/promptbased_feature_engineering_part_1_generative/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://nqln.substack.com/p/prompt-based-feature-engineering', 'subreddit_subscribers': 180197, 'created_utc': 1714425141.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.722+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_cbh6ollo', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Customer Journey Analysis using dbt', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 70, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cg4by8', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/gpqN54UBMj7oFvwWW23e9bFK-mPcNrLyiz2HyAavvD4.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1714410222.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'github.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://github.com/rudderlabs/dbt-customer-journey-analysis', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/ZIM7s2CnCWGyzDpSXlbzUxhuWwTF9ncv2uEcCcRAQLU.jpg?auto=webp&s=ddf5d9da3c7206f21d47ce88e0a3870190bc3e15', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/ZIM7s2CnCWGyzDpSXlbzUxhuWwTF9ncv2uEcCcRAQLU.jpg?width=108&crop=smart&auto=webp&s=fb6496fd51dbb46eecae5ab68e88dda8d48e058a', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/ZIM7s2CnCWGyzDpSXlbzUxhuWwTF9ncv2uEcCcRAQLU.jpg?width=216&crop=smart&auto=webp&s=897da9c3a162f01c73869b306ba7f4c3f619833d', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/ZIM7s2CnCWGyzDpSXlbzUxhuWwTF9ncv2uEcCcRAQLU.jpg?width=320&crop=smart&auto=webp&s=40d5651708eb94c92a67c5a2c65314f32ff46600', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/ZIM7s2CnCWGyzDpSXlbzUxhuWwTF9ncv2uEcCcRAQLU.jpg?width=640&crop=smart&auto=webp&s=19a24bad6a681076b0028054e55a1f388075b137', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/ZIM7s2CnCWGyzDpSXlbzUxhuWwTF9ncv2uEcCcRAQLU.jpg?width=960&crop=smart&auto=webp&s=08b0904efb9fb66f7e8b6a59cbf4381d4423167b', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/ZIM7s2CnCWGyzDpSXlbzUxhuWwTF9ncv2uEcCcRAQLU.jpg?width=1080&crop=smart&auto=webp&s=69ea067c788fac01414bad4fa7a0c4d5884e3fe8', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'Qp9TnfwvGj3Kw45FY0AWHWOVr2G-RNAITBL61U2Pq3Y'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1cg4by8', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ephemeral404'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cg4by8/customer_journey_analysis_using_dbt/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://github.com/rudderlabs/dbt-customer-journey-analysis', 'subreddit_subscribers': 180197, 'created_utc': 1714410222.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.722+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey Im setting up a CI pipeline for data \nFactory. I’m super new a definitely shouldn’t be setting this stuff up, but that’s the way it is unfortunately.\n\none thing I don’t get is do I need sql server in the dev and test resource groups. Can I just have a sql server in prod with multiple databases one for dev, test and prod.\n\nThere’s tutorials for data factory that I’ve went through and for setting up Git lab but nothing really covers putting it all together.. any diagrams, support / input would be appreciated.\n\nAlso would it be better to use data factory for the full etl or notebooks for transformations?', 'author_fullname': 't2_wf3q5q8', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data factory git lab question', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1cgwgs9', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714492932.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey Im setting up a CI pipeline for data \nFactory. I’m super new a definitely shouldn’t be setting this stuff up, but that’s the way it is unfortunately.</p>\n\n<p>one thing I don’t get is do I need sql server in the dev and test resource groups. Can I just have a sql server in prod with multiple databases one for dev, test and prod.</p>\n\n<p>There’s tutorials for data factory that I’ve went through and for setting up Git lab but nothing really covers putting it all together.. any diagrams, support / input would be appreciated.</p>\n\n<p>Also would it be better to use data factory for the full etl or notebooks for transformations?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cgwgs9', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='connoza'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgwgs9/data_factory_git_lab_question/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgwgs9/data_factory_git_lab_question/', 'subreddit_subscribers': 180197, 'created_utc': 1714492932.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.722+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi all, I have been tasked with creating a data model to hang all of our reports off of, unfortunately our old system was quite outdated and not the most reliable. \n\nI was told to go into Synapse and make the data model, so I did that, I made a data model that takes the raw data from our on prem SQL server to the reports. Although it works (I recreated one of our reports using the Synapse model instead of our old Visual studio model), I am no pro, I have made a lot of mistakes during this entire process (Expensive dataflows... Etc) and no doubt there are ways that I should could or need to improve it. \n\nA lot of this was trapsing through dead forum posts and YouTube videos, so if any of you guys could give me some feedback so I could update it I'd really appreciate it.\n\n[Datamodel](https://preview.redd.it/ux49dy033nxc1.png?width=1228&format=png&auto=webp&s=114692e62e4ab6df75c2ea46b0f142aeb84787c5)\n\n", 'author_fullname': 't2_sihhnnlo', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'My First Data Model', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 90, 'top_awarded_type': None, 'hide_score': True, 'media_metadata': {'ux49dy033nxc1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 69, 'x': 108, 'u': 'https://preview.redd.it/ux49dy033nxc1.png?width=108&crop=smart&auto=webp&s=bcc23c344d7edec2ac179effcdc91f2ec850bf15'}, {'y': 138, 'x': 216, 'u': 'https://preview.redd.it/ux49dy033nxc1.png?width=216&crop=smart&auto=webp&s=67826ca7b9b27310c76b0e01ad97c6512a60d024'}, {'y': 205, 'x': 320, 'u': 'https://preview.redd.it/ux49dy033nxc1.png?width=320&crop=smart&auto=webp&s=e8ea04554bc1a4b8d546de81c9372a5bdff6a368'}, {'y': 411, 'x': 640, 'u': 'https://preview.redd.it/ux49dy033nxc1.png?width=640&crop=smart&auto=webp&s=ecae6cb56b6d3a2b73529fdc2fc9d6e669d3142e'}, {'y': 617, 'x': 960, 'u': 'https://preview.redd.it/ux49dy033nxc1.png?width=960&crop=smart&auto=webp&s=6ea986c6393781b97b2dc25d09ed70f0fc23dfb1'}, {'y': 694, 'x': 1080, 'u': 'https://preview.redd.it/ux49dy033nxc1.png?width=1080&crop=smart&auto=webp&s=c47fa848a20b1a16537b218ec2eb77ea439e6cb3'}], 's': {'y': 790, 'x': 1228, 'u': 'https://preview.redd.it/ux49dy033nxc1.png?width=1228&format=png&auto=webp&s=114692e62e4ab6df75c2ea46b0f142aeb84787c5'}, 'id': 'ux49dy033nxc1'}}, 'name': 't3_1cgwev0', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/tsy8oCMPRlhPn-z8FEA-EsVifH7G2Luy1WTxgfcbv3c.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714492814.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi all, I have been tasked with creating a data model to hang all of our reports off of, unfortunately our old system was quite outdated and not the most reliable. </p>\n\n<p>I was told to go into Synapse and make the data model, so I did that, I made a data model that takes the raw data from our on prem SQL server to the reports. Although it works (I recreated one of our reports using the Synapse model instead of our old Visual studio model), I am no pro, I have made a lot of mistakes during this entire process (Expensive dataflows... Etc) and no doubt there are ways that I should could or need to improve it. </p>\n\n<p>A lot of this was trapsing through dead forum posts and YouTube videos, so if any of you guys could give me some feedback so I could update it I&#39;d really appreciate it.</p>\n\n<p><a href="https://preview.redd.it/ux49dy033nxc1.png?width=1228&amp;format=png&amp;auto=webp&amp;s=114692e62e4ab6df75c2ea46b0f142aeb84787c5">Datamodel</a></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cgwev0', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Mathlete7'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgwev0/my_first_data_model/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgwev0/my_first_data_model/', 'subreddit_subscribers': 180197, 'created_utc': 1714492814.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.723+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi guys,\n\nI've been working as DE for 2 years, but all the time in Foundry platform. It's my first job, and actually a good job for me, I had chance to onsite, tackle real business challenges. However, my experience has been focused mainly on Spark, a bit SQL, and tools which are highly customized for this platform.\n\nNow when I look at any JD, I don't have actual experience with these tech requirement, most of which I've only learned through self-study.\n\nI'm quite concerned about this and would appreciate any advice or experiences you might share about preparing for a transition or enhancing my skills to be more versatile in the tech industry.\n\nThank you :<", 'author_fullname': 't2_nmv5ovl0h', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Seeking advice for job change after Foundry', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1cgvn7f', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714490881.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi guys,</p>\n\n<p>I&#39;ve been working as DE for 2 years, but all the time in Foundry platform. It&#39;s my first job, and actually a good job for me, I had chance to onsite, tackle real business challenges. However, my experience has been focused mainly on Spark, a bit SQL, and tools which are highly customized for this platform.</p>\n\n<p>Now when I look at any JD, I don&#39;t have actual experience with these tech requirement, most of which I&#39;ve only learned through self-study.</p>\n\n<p>I&#39;m quite concerned about this and would appreciate any advice or experiences you might share about preparing for a transition or enhancing my skills to be more versatile in the tech industry.</p>\n\n<p>Thank you :&lt;</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1cgvn7f', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='lllutb'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgvn7f/seeking_advice_for_job_change_after_foundry/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgvn7f/seeking_advice_for_job_change_after_foundry/', 'subreddit_subscribers': 180197, 'created_utc': 1714490881.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.723+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "My company has this old approach built by a contractor of moving data from Redshift using IICS to orchestrate moving data from Redshift into S3. There's another redshift database that houses all the change capture data. Then with Databricks they do ALTER TABLE to repoint to the new table in s3 to sync.\n\nIt's a super clunky process and we're looking to rebuild it in house. Has anyone tried syncing data from redshift into databricks? Seems like we're not utilizing the databricks functionality the best way. These tables are also parquets.", 'author_fullname': 't2_7saronit', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Syncing Redshift to Databricks advice', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1cgvk33', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714490666.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>My company has this old approach built by a contractor of moving data from Redshift using IICS to orchestrate moving data from Redshift into S3. There&#39;s another redshift database that houses all the change capture data. Then with Databricks they do ALTER TABLE to repoint to the new table in s3 to sync.</p>\n\n<p>It&#39;s a super clunky process and we&#39;re looking to rebuild it in house. Has anyone tried syncing data from redshift into databricks? Seems like we&#39;re not utilizing the databricks functionality the best way. These tables are also parquets.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cgvk33', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='crossfirex35'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgvk33/syncing_redshift_to_databricks_advice/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgvk33/syncing_redshift_to_databricks_advice/', 'subreddit_subscribers': 180197, 'created_utc': 1714490666.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.723+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi!\n\nI'm currently using Kobo Toolbox for in-field data capture and have found its offline capabilities and robustness quite effective for my needs. However, I'm facing a significant challenge with updating forms, particularly drop-down lists, which requires manually editing an XLS form and uploading it each time a change is needed. This process is cumbersome and I'm hoping to streamline it.\n\nI'm looking for suggestions on two fronts:\n\nAlternative Tools: Are there any in-field data capture tools similar to Kobo that allow for automatic updating of forms? The tool should be cost-effective, robust, and capable of working offline.\n\nKobo Toolbox Solutions: If you know of any integrations or techniques to automate form updates within Kobo Toolbox, that information would be highly valuable.\n\nAny advice or recommendations you could share would be greatly appreciated. Thanks in advance for your help!", 'author_fullname': 't2_t1mvr7oy', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Seeking Recommendations: In-field Data Capture Tools with Auto-Updating Forms Similar to Kobo Toolbox', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgtjq5', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714485614.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi!</p>\n\n<p>I&#39;m currently using Kobo Toolbox for in-field data capture and have found its offline capabilities and robustness quite effective for my needs. However, I&#39;m facing a significant challenge with updating forms, particularly drop-down lists, which requires manually editing an XLS form and uploading it each time a change is needed. This process is cumbersome and I&#39;m hoping to streamline it.</p>\n\n<p>I&#39;m looking for suggestions on two fronts:</p>\n\n<p>Alternative Tools: Are there any in-field data capture tools similar to Kobo that allow for automatic updating of forms? The tool should be cost-effective, robust, and capable of working offline.</p>\n\n<p>Kobo Toolbox Solutions: If you know of any integrations or techniques to automate form updates within Kobo Toolbox, that information would be highly valuable.</p>\n\n<p>Any advice or recommendations you could share would be greatly appreciated. Thanks in advance for your help!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cgtjq5', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='yaksurf'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgtjq5/seeking_recommendations_infield_data_capture/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgtjq5/seeking_recommendations_infield_data_capture/', 'subreddit_subscribers': 180197, 'created_utc': 1714485614.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.723+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "[C++ DataFrame](https://github.com/hosseinmoein/DataFrame) was designed to extend the C++ ecosystem and provide a tool for efficient processing of large datasets. It is widely used in Financial and AI industries. You don't need an advanced knowledge of C++ to use it. There is also comprehensive documentation with code samples.\n\nDataFrame is designed using modern C++, extensive multithreading, and SIMD techniques. It also provides a large collection of built-in analytical routines.", 'author_fullname': 't2_jeyhnly', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'C++ DataFrame for efficient analysis of large datasets', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgtewr', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Open Source', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714485258.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p><a href="https://github.com/hosseinmoein/DataFrame">C++ DataFrame</a> was designed to extend the C++ ecosystem and provide a tool for efficient processing of large datasets. It is widely used in Financial and AI industries. You don&#39;t need an advanced knowledge of C++ to use it. There is also comprehensive documentation with code samples.</p>\n\n<p>DataFrame is designed using modern C++, extensive multithreading, and SIMD techniques. It also provides a large collection of built-in analytical routines.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/_uuFpP_nkFIX8_cG1UOKlV8bMX7flKq6kltguFuXJ34.jpg?auto=webp&s=80b7e39259f18f59d3522fe34d884d768ccf059d', 'width': 400, 'height': 400}, 'resolutions': [{'url': 'https://external-preview.redd.it/_uuFpP_nkFIX8_cG1UOKlV8bMX7flKq6kltguFuXJ34.jpg?width=108&crop=smart&auto=webp&s=f709bc1b8f1dbcba4c903142587b31011b752996', 'width': 108, 'height': 108}, {'url': 'https://external-preview.redd.it/_uuFpP_nkFIX8_cG1UOKlV8bMX7flKq6kltguFuXJ34.jpg?width=216&crop=smart&auto=webp&s=383df985392be8ad15f1bac307b2492530396433', 'width': 216, 'height': 216}, {'url': 'https://external-preview.redd.it/_uuFpP_nkFIX8_cG1UOKlV8bMX7flKq6kltguFuXJ34.jpg?width=320&crop=smart&auto=webp&s=b072004e583333e2986eae512c049171af80adb9', 'width': 320, 'height': 320}], 'variants': {}, 'id': 'BhRrik4lWsvKvlY42tNQHqju8uNOFg3QY2K4_xlC4N8'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '3957ca64-3440-11ed-8329-2aa6ad243a59', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#005ba1', 'id': '1cgtewr', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='hmoein'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgtewr/c_dataframe_for_efficient_analysis_of_large/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgtewr/c_dataframe_for_efficient_analysis_of_large/', 'subreddit_subscribers': 180197, 'created_utc': 1714485258.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.723+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I recently integrated Rudderstack for my company (nextjs app), and most of the integrations have worked well. The only issue has been that our sessions + unique users have magically doubled as reflected in all of our integrations. Has anyone else run into this? I followed the guide [here](https://www.rudderstack.com/docs/user-guides/how-to-guides/rudderstack-jamstack-integration/v3/nextjs/), and tech support hasn't been able to figure it out :/ \n\nAppreciate any insight/help 🙏\n\n&#x200B;", 'author_fullname': 't2_11tw0u', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Rudderstack double counting sessions + unique users', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cg4nbb', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714411001.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I recently integrated Rudderstack for my company (nextjs app), and most of the integrations have worked well. The only issue has been that our sessions + unique users have magically doubled as reflected in all of our integrations. Has anyone else run into this? I followed the guide <a href="https://www.rudderstack.com/docs/user-guides/how-to-guides/rudderstack-jamstack-integration/v3/nextjs/">here</a>, and tech support hasn&#39;t been able to figure it out :/ </p>\n\n<p>Appreciate any insight/help 🙏</p>\n\n<p>&#x200B;</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/qXFNKKMEe3GwCb0Td3-37vyl7rG3xs8jvinjVOGMY08.jpg?auto=webp&s=7db20f1c772b422ea028c2b01625582bede3954c', 'width': 1200, 'height': 628}, 'resolutions': [{'url': 'https://external-preview.redd.it/qXFNKKMEe3GwCb0Td3-37vyl7rG3xs8jvinjVOGMY08.jpg?width=108&crop=smart&auto=webp&s=61ac9619a73ec27950746b0467a894bcc893cc59', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/qXFNKKMEe3GwCb0Td3-37vyl7rG3xs8jvinjVOGMY08.jpg?width=216&crop=smart&auto=webp&s=0978f3b69fccdc67871f1d6fd50b492144e52157', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/qXFNKKMEe3GwCb0Td3-37vyl7rG3xs8jvinjVOGMY08.jpg?width=320&crop=smart&auto=webp&s=39a5f32e52f8a249782c021334d9e3c3f7bc1022', 'width': 320, 'height': 167}, {'url': 'https://external-preview.redd.it/qXFNKKMEe3GwCb0Td3-37vyl7rG3xs8jvinjVOGMY08.jpg?width=640&crop=smart&auto=webp&s=082c93278f496eba117572e975b947e2c1d5e225', 'width': 640, 'height': 334}, {'url': 'https://external-preview.redd.it/qXFNKKMEe3GwCb0Td3-37vyl7rG3xs8jvinjVOGMY08.jpg?width=960&crop=smart&auto=webp&s=80fa34d4f6f12218c4f6a3de6bc075d4e7a1a5d7', 'width': 960, 'height': 502}, {'url': 'https://external-preview.redd.it/qXFNKKMEe3GwCb0Td3-37vyl7rG3xs8jvinjVOGMY08.jpg?width=1080&crop=smart&auto=webp&s=0c846a2e94042822deaa70c909dcd5a18362eeb3', 'width': 1080, 'height': 565}], 'variants': {}, 'id': 'MkCW_e1iR_eHLD_0ITHkXFuNmJTjp7poE9M9M_RlwTQ'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cg4nbb', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='waveyrico'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cg4nbb/rudderstack_double_counting_sessions_unique_users/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cg4nbb/rudderstack_double_counting_sessions_unique_users/', 'subreddit_subscribers': 180197, 'created_utc': 1714411001.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.724+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I graduated with a Bachelors in Biology degree  (3 years) last year and luckily got a data analyst job in a good company. Life is pretty good, its WFH, 40 hours/week work and my managers are super nice. I have been doing well at my job too and will be promoted later this year. Well, now I am little confused. I want to internally shift to data engineering, maybe next year or later after I know something but I am worried if my biology degree will cause any problem or future hurdles in my career. I am thinking of doing OMSCS (Online Master of Science in Computer Science) by Georgia Tech next year but I don't know if its worth the efforts, time & money. Will it help me breaking in or would I be better off doing self study for DE? I have felt that there will be some opportunity cost as very few courses from that degree will apply to DE. \n\n  \ntl;dr I have a bachelors in biology and I am a data analyst right now, want to shift to DE in a year or two and considering OMSCS, is it worth it (especially for DE/SWE)?", 'author_fullname': 't2_6gj0mtr0', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Trying to transition from DA to DE. Worth pursuing OMSCS after Bio undergrad?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cg3n5i', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.6, 'author_flair_background_color': 'transparent', 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': '19bba012-ac9d-11eb-b77b-0eec37c01719', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714408573.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I graduated with a Bachelors in Biology degree  (3 years) last year and luckily got a data analyst job in a good company. Life is pretty good, its WFH, 40 hours/week work and my managers are super nice. I have been doing well at my job too and will be promoted later this year. Well, now I am little confused. I want to internally shift to data engineering, maybe next year or later after I know something but I am worried if my biology degree will cause any problem or future hurdles in my career. I am thinking of doing OMSCS (Online Master of Science in Computer Science) by Georgia Tech next year but I don&#39;t know if its worth the efforts, time &amp; money. Will it help me breaking in or would I be better off doing self study for DE? I have felt that there will be some opportunity cost as very few courses from that degree will apply to DE. </p>\n\n<p>tl;dr I have a bachelors in biology and I am a data analyst right now, want to shift to DE in a year or two and considering OMSCS, is it worth it (especially for DE/SWE)?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Data Analyst', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1cg3n5i', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='pyschduck'), 'discussion_type': None, 'num_comments': 10, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1cg3n5i/trying_to_transition_from_da_to_de_worth_pursuing/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cg3n5i/trying_to_transition_from_da_to_de_worth_pursuing/', 'subreddit_subscribers': 180197, 'created_utc': 1714408573.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.724+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'SO, I am trying to create a simple pipeline between Zoho and our Warehouse using Hevo. Since Zoho only allows OAuth Authentication, I set everything up in Zoho and created a client with a client id and a secret. I got a **grant token** and this allowed me to get an **access token and a refresh token** with POSTMAN \n\nI put Hevo\'s redirect URL into my Zoho client\'s redirect URLS!\n\nThen, I went to Hevo, select my endpoint: \n\n    https://books.zoho.eu/api/v3/invoices\n\nSelected OAuth as authentication. Created a token using my client id and secret, used zoho\'s auth url: [https://accounts.zoho.com/oauth/v2/auth](https://accounts.zoho.com/oauth/v2/auth)  \nand zoho\'s toke URL: [https://accounts.zoho.com/oauth/v2/token](https://accounts.zoho.com/oauth/v2/token)\n\nselected the CORRECT scope, but when I test i get an error.\n\nReceived HTTP Status: 400 and response: <html> <head> <title>Zoho Accounts</title> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <link href="[https://static.zohocdn.com/iam/v2/components/css/zoho](https://static.zohocdn.com/iam/v2/components/css/zohoPuvi.c86bbb480e4a4fbc379fd8e7298bbde5.css)......  \n. This request is incorrect or corrupt. Please check the API documentation.\n\n\n\nDoes anyone here has experience setting the Zoho API as a Hevo source?', 'author_fullname': 't2_ppyprrvz', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How to connect Zoho and Hevo via OAUTH? I am entering my keys but I keep getting errorrs.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgqr6r', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714477378.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>SO, I am trying to create a simple pipeline between Zoho and our Warehouse using Hevo. Since Zoho only allows OAuth Authentication, I set everything up in Zoho and created a client with a client id and a secret. I got a <strong>grant token</strong> and this allowed me to get an <strong>access token and a refresh token</strong> with POSTMAN </p>\n\n<p>I put Hevo&#39;s redirect URL into my Zoho client&#39;s redirect URLS!</p>\n\n<p>Then, I went to Hevo, select my endpoint: </p>\n\n<pre><code>https://books.zoho.eu/api/v3/invoices\n</code></pre>\n\n<p>Selected OAuth as authentication. Created a token using my client id and secret, used zoho&#39;s auth url: <a href="https://accounts.zoho.com/oauth/v2/auth">https://accounts.zoho.com/oauth/v2/auth</a><br/>\nand zoho&#39;s toke URL: <a href="https://accounts.zoho.com/oauth/v2/token">https://accounts.zoho.com/oauth/v2/token</a></p>\n\n<p>selected the CORRECT scope, but when I test i get an error.</p>\n\n<p>Received HTTP Status: 400 and response: &lt;html&gt; &lt;head&gt; &lt;title&gt;Zoho Accounts&lt;/title&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;link href=&quot;<a href="https://static.zohocdn.com/iam/v2/components/css/zohoPuvi.c86bbb480e4a4fbc379fd8e7298bbde5.css">https://static.zohocdn.com/iam/v2/components/css/zoho</a>......<br/>\n. This request is incorrect or corrupt. Please check the API documentation.</p>\n\n<p>Does anyone here has experience setting the Zoho API as a Hevo source?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1cgqr6r', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Realistic_Salary_942'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgqr6r/how_to_connect_zoho_and_hevo_via_oauth_i_am/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgqr6r/how_to_connect_zoho_and_hevo_via_oauth_i_am/', 'subreddit_subscribers': 180197, 'created_utc': 1714477378.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:27.724+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff7ad9c3d0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Would it be a bad career choice to to get into data engineering and only apply to Scala/Java jobs?', 'author_fullname': 't2_2ws5ddue', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'I would like to get into Data engineering but I despise working with Python.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1cgo8ty', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.31, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1714467702.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Would it be a bad career choice to to get into data engineering and only apply to Scala/Java jobs?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1cgo8ty', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='God_of_failure'), 'discussion_type': None, 'num_comments': 34, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1cgo8ty/i_would_like_to_get_into_data_engineering_but_i/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1cgo8ty/i_would_like_to_get_into_data_engineering_but_i/', 'subreddit_subscribers': 180197, 'created_utc': 1714467702.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-04-30T16:54:28.837+0000] {python.py:237} INFO - Done. Returned value was: /opt/airflow/data/output
[2024-04-30T16:54:28.842+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-04-30T16:54:28.986+0000] {taskinstance.py:1205} INFO - Marking task as SUCCESS. dag_id=etl_reddit_pipeline, task_id=reddit_api_extraction, execution_date=20240430T165028, start_date=20240430T165426, end_date=20240430T165428
[2024-04-30T16:54:29.082+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-04-30T16:54:29.205+0000] {taskinstance.py:3482} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-04-30T16:54:29.206+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
