id,title,selftext,score,num_comments,author,created_utc,url,over_18,edited,spoiler,stickied
1aieu3j,Facts,,1312,40,OldParticular2326,2024-02-04 03:59:33,https://i.redd.it/zv1x2jyeshgc1.jpeg,False,False,False,False
14442pi,"""We have great datasets""",,1092,130,OverratedDataScience,2023-06-08 09:08:22,https://i.redd.it/0bv7bxlsfr4b1.jpg,False,False,False,False
12t22p4,i just want sleep,,1015,75,Straight_House8628,2023-04-20 14:52:52,https://i.redd.it/u3lx5ziwy1va1.png,False,False,False,False
16mnj2y,I've finally built the perfect data pipeline!,,1015,90,GreenSquid,2023-09-19 10:40:10,https://i.redd.it/0uv934osx6pb1.png,False,False,False,False
12zhvtk,PSA: Learn Vendor Agnostic Technologies!,,1002,102,smashmaps,2023-04-26 13:39:02,https://i.redd.it/bdjqa87ue8wa1.png,False,False,False,False
1arwf4u,"Had an onsite interview with one of FAANG, all 6 interviewers were Indian ","7 if I count the person who did phone screen. Had a positive experience with majority of the interviewers but hiring manager and another interviewer appeared very uninterested and seems didn’t even read my resume. Almost 0 coding and majority was behavioral questions despite the fact that this is mid level data eng position. 
With this much skewed perceived diversity, I can’t help thinking they’re looking for another person from their own culture. 


Edit:
Seems like many other also witness this trend: https://www.reddit.com/r/cscareerquestions/s/pnt5Zidl1X 

",972,69,HiroKifa,2024-02-16 01:35:03,https://www.reddit.com/r/dataengineering/comments/1arwf4u/had_an_onsite_interview_with_one_of_faang_all_6/,False,False,False,False
15ae6kp,The data engineer came to me... tears in his eyes,"Turns out databases are ""relational"" or something",836,67,shed_antlers,2023-07-26 18:48:33,https://i.redd.it/1wm37l33vceb1.png,False,False,False,False
12m8ml7,Exporting to excel is always a people pleaser...,,838,58,audiologician,2023-04-14 18:52:24,https://i.redd.it/6vtglxi2cwta1.png,False,False,False,False
1bd5wv8,It’s happening guys,,819,204,marclamberti,2024-03-12 19:20:07,https://i.redd.it/x2u0nprdeync1.jpeg,False,False,False,False
s054b4,2022 Mood,,758,122,theporterhaus,2022-01-09 23:43:39,https://i.redd.it/s7olw2f01ra81.jpg,False,False,False,False
1brqa92,Is this chart accurate?,,744,67,WadieXkiller,2024-03-30 19:37:33,https://i.redd.it/otv5gxbwxirc1.jpeg,False,False,False,False
xis5vv,Data driven organisations,,730,42,adzsroka,2022-09-19 23:22:08,https://i.redd.it/cj8htjosfwo91.jpg,False,False,False,False
12l9mzx,Who owns data quality?,,717,88,Top-Substance2185,2023-04-13 22:40:57,https://i.redd.it/8ma1yb67cqta1.jpg,False,False,False,False
1aujhqw,How true is this!,Source: twitter,624,44,_areebpasha,2024-02-19 09:39:22,https://i.redd.it/rs9mkuppiijc1.jpeg,False,False,False,False
oyju56,DataEngineering 2021 in one pic,,609,55,Legitimate-Cry2837,2021-08-05 14:46:15,https://i.redd.it/pdnuk1r0yjf71.jpg,False,False,False,False
o210i3,I wrote a children's book / illustrated guide to Apache Kafka,"hi fellow Data Engineers, just wanted to share a beginner-friendly resource that I've been working on for Apache Kafka. I spent 3 months illustrating this digital book, which I'm calling Gently Down the Stream: [http://www.gentlydownthe.stream/](http://www.gentlydownthe.stream/)

I know that normal tech books that span hundreds of pages are often intimidating for beginners (I wrote one of those too, btw: [http://kafka-streams-book.com/](http://kafka-streams-book.com/)). So I just wanted to create something a little simpler, more colorful, playful, and fun. Hope you enjoy it!

&#x200B;

edit: wow, thanks for the kind words everyone!! I'll be creating more books like this in the future, and will be announcing them on my new Twitter account if you're interested in following: [https://twitter.com/\_round\_robin/](https://twitter.com/_round_robin/)",610,46,mitchum_,2021-06-17 16:15:50,https://www.reddit.com/r/dataengineering/comments/o210i3/i_wrote_a_childrens_book_illustrated_guide_to/,False,False,False,False
1420fjz,I’ve had the definition wrong this entire time…,,579,46,Straight_House8628,2023-06-06 02:19:44,https://i.redd.it/9uviprh35b4b1.jpg,False,False,False,False
udboyq,I've been a big data engineer since 2015. I've worked at FAANG for 6 years and grew from L3 to L6. AMA,"See title.

Follow me on YouTube here. I talk a lot about data engineering in much more depth and detail!  [https://www.youtube.com/c/datawithzach](https://www.youtube.com/c/datawithzach)  


Follow me on Twitter here [https://www.twitter.com/EcZachly](https://www.twitter.com/EcZachly)  


Follow me on LinkedIn here [https://www.linkedin.com/in/eczachly](https://www.linkedin.com/in/eczachly)

&#x200B;",577,463,eczachly,2022-04-27 19:29:47,https://www.reddit.com/r/dataengineering/comments/udboyq/ive_been_a_big_data_engineer_since_2015_ive/,False,False,False,False
151xsis,"Data Scientists -- Ok, now I get it.",[DELETED] ` this message was mass deleted/edited with redact.dev `,573,219,None,2023-07-17 10:11:47,https://www.reddit.com/r/dataengineering/comments/151xsis/data_scientists_ok_now_i_get_it/,False,False,False,False
1bc0bkv,"ELI5: what is ""Self-service Analytics"" (comic)",,575,107,InitiativeOk6728,2024-03-11 10:47:22,https://www.reddit.com/gallery/1bc0bkv,False,False,False,False
y0qipu,Your Snowflake credits at work.,,564,65,droppedorphan,2022-10-10 21:24:03,https://i.redd.it/2otvyplsp1t91.jpg,False,False,False,False
y3rxoe,It's amazing how many organizations workflows still revolve around Excel. I've seen CFOs and COOs folders filled with 20 different versions of the same Excel file.,,558,95,DrRedmondNYC,2022-10-14 11:56:44,https://i.redd.it/a893r4wagrt91.jpg,False,False,False,False
p8crow,The struggle is real.,,559,43,ali_azg,2021-08-20 20:01:39,https://i.redd.it/7wv74skyjki71.jpg,False,False,False,False
10mk6bc,The current data landscape,,546,101,stchena,2023-01-27 12:57:49,https://i.redd.it/ny9pf4mfkmea1.png,False,False,False,False
18ffzmx,I Was Happier Being a Bartender Compared to Being a 6 Figure DE,"Used to be a resort bartender during college for 4 years. Been a DE for 3 years. Went to college for Information Science, graduated, got a job as a data engineer making $85K. Since then I’ve been working on my MSDS part time, and 3 shitty data jobs later I am making $110K. 

Data is interesting at its core but the industry ruins it. Layoffs everywhere, shitty tech stacks, bad stakeholders, boomer bosses, bait and switch job opportunities, remote roles being replaced with 100% on site. 

Overall this career path has been the worst decision of my life, despite looking good on paper. To say my life has declined in every way since going from bartender to DE is an understatement - I am absolutely miserable. Used to have friends, go on dates, socialize all day, have tons of free time. All of that is gone in favor of long hours, boring shit jobs, leetcode, living with parents after being thrown around the country for different toxic roles. 

Anyone else go into data and realize the grass wasn’t greener? I am dropping out of my MSDS and leaving this industry in January. This whole journey was a waste of ~6 years of work. ",541,176,None,2023-12-10 23:02:27,https://www.reddit.com/r/dataengineering/comments/18ffzmx/i_was_happier_being_a_bartender_compared_to_being/,False,False,False,False
1945s14,Guess the data type ಠ_ಠ,,533,47,Thinker_Assignment,2024-01-11 16:31:27,https://i.redd.it/t9nn0bq88ubc1.png,False,False,False,False
198kif0,"My company just put out 3 data engineering jobs last year, guess who we got?","As per title, my company put out 3 entry level data engineer jobs last year. The pay range was terrible, 60 - 80k. 

We ended up hiring a data engineer with 3 yoe at a Fortune 100, a data engineer with 1 yoe and a masters in machine learning, and a self taught engineer who has built applications that literally make my applications look like children's books. 

They've jumped on projects with some of our previous entry level hires from 2019-2022 and made them look like chumps. 

All of them were looking for jobs for at least 4-6 months. 

Just wanted to share a data point on the state of the market last year in 2023. 

Funny thing is that I don't expect any of them to stay when the job market picks up, and we may have a mass exodus on our hands. ",524,108,Justanotherguy2022,2024-01-17 01:44:50,https://www.reddit.com/r/dataengineering/comments/198kif0/my_company_just_put_out_3_data_engineering_jobs/,False,False,False,False
p88ujn,"{""null""}",,511,13,adgezaza87,2021-08-20 16:37:01,https://i.redd.it/gta0zsoejji71.jpg,False,False,False,False
18ix6hd,How Netflix does Data Engineering,"A collection of videos shared by Netflix from their [Data Engineering Summit](https://netflixtechblog.com/our-first-netflix-data-engineering-summit-f326b0589102)

* [The Netflix Data Engineering Stack](https://youtu.be/QxaOlmv79ls)
* [Data Processing Patterns](https://www.youtube.com/watch?v=vuyjK2TFZNk&list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&index=3)
* [Streaming SQL on Data Mesh using Apache Flink](https://www.youtube.com/watch?v=TwcWvwU7B64&list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&index=4)
* [Building Reliable Data Pipelines](https://www.youtube.com/watch?v=uWmJxbhI304&list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&index=5)
* [Knowledge Management — Leveraging Institutional Data](https://www.youtube.com/watch?v=F4N8AmScZ-w&list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&index=6)
* [Psyberg, An Incremental ETL Framework Using Iceberg](https://www.youtube.com/watch?v=jRckeOedtx0&list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&index=8)
* [Start/Stop/Continue for optimizing complex ETL jobs](https://www.youtube.com/watch?v=Dr8LMn-nJGc&list=PLSECvWLlUYeF06QK5FOOELvgKdap3cQf0&index=9)
* [Media Data for ML Studio Creative Production](https://youtu.be/1gGi3NBZk7M)",504,111,rmoff,2023-12-15 10:32:06,https://www.reddit.com/r/dataengineering/comments/18ix6hd/how_netflix_does_data_engineering/,False,False,False,False
109fhh8,Happy (or not so happy) Wednesday! What part of your technical work do you dread the most? What are you doing about it?,,491,69,tchungry,2023-01-11 20:47:16,https://i.redd.it/f1srh5yeygba1.jpg,False,False,False,False
p31jni,Was the data clean??,,492,32,kuwala-io,2021-08-12 14:58:18,https://v.redd.it/j1uota2iyxg71,False,False,False,False
szrg3i,Yep,,448,50,caksters,2022-02-23 20:43:40,https://i.redd.it/oolb3j4x9nj81.jpg,False,False,False,False
tuobs4,"Completed my first Data Engineering project with Kafka, Spark, GCP, Airflow, dbt, Terraform, Docker and more!","[Dashboard](https://github.com/ankurchavda/streamify/blob/main/images/dashboard.png?raw=true)

First of all, I'd like to start with thanking the instructors at the [DataTalks.Club](https://DataTalks.Club) for setting up a completely free [course](https://github.com/DataTalksClub/data-engineering-zoomcamp#data-engineering-zoomcamp). This was the best course that I took and the project I did was all because of what I learnt there :D.

TL;DR below.

# Git Repo:

[Streamify](https://github.com/ankurchavda/streamify)

# About The Project:

The project streams events generated from a fake music streaming service (like Spotify) and creates a data pipeline that consumes real-time data. The data coming in would is similar to an event of a user listening to a song, navigating on the website, authenticating. The data is then processed in real-time and stored to the data lake periodically (every two minutes). The hourly batch job then consumes this data, applies transformations, and creates the desired tables for our dashboard to generate analytics. We try to analyze metrics like popular songs, active users, user demographics etc.

# The Dataset:

[Eventsim](https://github.com/Interana/eventsim) is a program that generates event data to replicate page requests for a fake music web site. The results look like real use data, but are totally fake. The docker image is borrowed from [viirya's fork](https://github.com/viirya/eventsim) of it, as the original project has gone without maintenance for a few years now.

Eventsim uses song data from [Million Songs Dataset](http://millionsongdataset.com/) to generate events. I have used a [subset](http://millionsongdataset.com/pages/getting-dataset/#subset) of 10000 songs.

# Tools & Technologies

* Cloud - [**Google Cloud Platform**](https://cloud.google.com/)
* Infrastructure as Code software - [**Terraform**](https://www.terraform.io/)
* Containerization - [**Docker**](https://www.docker.com/), [**Docker Compose**](https://docs.docker.com/compose/)
* Stream Processing - [**Kafka**](https://kafka.apache.org/), [**Spark Streaming**](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
* Orchestration - [**Airflow**](https://airflow.apache.org/)
* Transformation - [**dbt**](https://www.getdbt.com/)
* Data Lake - [**Google Cloud Storage**](https://cloud.google.com/storage)
* Data Warehouse - [**BigQuery**](https://cloud.google.com/bigquery)
* Data Visualization - [**Data Studio**](https://datastudio.google.com/overview)
* Language - [**Python**](https://www.python.org/)

# Architecture

[Streamify Architecture](https://preview.redd.it/lnpruyl305r81.jpg?width=1222&format=pjpg&auto=webp&s=48f5993d4a8e142b7c32846535ad1ae501e2a332)

# Final Dashboard

[Streamify Dashboard](https://preview.redd.it/0r3yfde005r81.png?width=1436&format=png&auto=webp&s=4d21c6f47ebbfe6f9201afdd4284ca83643e4e3b)

You can check the actual dashboard [here](https://datastudio.google.com/s/uVy77npmwvg). I stopped it a couple of days back so the data might not be recent.

# Feedback:

There are lot of experienced folks here and I would love to hear some constructive criticism on what things could be done in a better way. Please share your comments.

# Reproduce:

I have tried to document the project thoroughly, and be really elaborate about the setup process. If you chose to learn from this project and face any issues, feel free to drop me a message.

&#x200B;

**TL;DR:** Built a project that consumes real-time data and then ran hourly batch jobs to transform the data into a dimensional model for the data to be consumed by the dashboard.",433,89,ankurchavda,2022-04-02 17:30:38,https://www.reddit.com/r/dataengineering/comments/tuobs4/completed_my_first_data_engineering_project_with/,False,False,False,False
ygieh8,"Data engineering projects with template: Airflow, dbt, Docker, Terraform (IAC), Github actions (CI/CD) & more","Hello everyone,

Some of my posts about DE projects (for portfolio) were well received in this subreddit. (e.g. [this](https://www.reddit.com/r/dataengineering/comments/nto0nd/data_engineering_project_for_beginners_v2/) and [this](https://www.reddit.com/r/dataengineering/comments/om3wl5/data_engineering_project_with_a_live_dashboard/))

   But many readers reached out with difficulties in setting up the infrastructure, CI/CD, automated testing, and database changes. With that in mind, I wrote this article [https://www.startdataengineering.com/post/data-engineering-projects-with-free-template/](https://www.startdataengineering.com/post/data-engineering-projects-with-free-template/) which sets up an Airflow + Postgres + Metabase stack and can also set up AWS infra to run them, with the following tools

1. **`local development`**: [Docker](https://www.docker.com/) & [Docker compose](https://docs.docker.com/compose/)
2. **`DB Migrations`**: [yoyo-migrations](https://ollycope.com/software/yoyo/latest/)
3. **`IAC`**: [Terraform](https://www.terraform.io/)
4. **`CI/CD`**: [Github Actions](https://github.com/features/actions)
5. **`Testing`**: [Pytest](https://docs.pytest.org/en/7.1.x/)
6. **`Formatting`**: [isort](https://pycqa.github.io/isort/) & [black](https://github.com/psf/black)
7. **`Lint check`**: [flake8](https://github.com/pycqa/flake8)
8. **`Type check`**: [mypy](http://mypy-lang.org/)

I also updated the below projects from my website to use these tools for easier setup.

1. [DE Project Batch edition](https://www.startdataengineering.com/post/data-engineering-project-for-beginners-batch-edition/) Airflow, Redshift, EMR, S3, Metabase
2. [DE Project to impress Hiring Manager](https://www.startdataengineering.com/post/data-engineering-project-to-impress-hiring-managers/) Cron, Postgres, Metabase
3. [End-to-end DE project](https://www.startdataengineering.com/post/data-engineering-project-e2e/) Dagster, dbt, Postgres, Metabase

An easy-to-use template helps people start building data engineering projects (for portfolio) & providing a good understanding of commonly used development practices. Any feedback is appreciated. I hope this helps someone :) 

Tl; DR: Data infra is complex; use this template for your portfolio data projects 

Blog: https://www.startdataengineering.com/post/data-engineering-projects-with-free-template/
Code: https://github.com/josephmachado/data_engineering_project_template",422,37,joseph_machado,2022-10-29 12:29:46,https://www.reddit.com/r/dataengineering/comments/ygieh8/data_engineering_projects_with_template_airflow/,False,False,False,False
1553rxb,"Barbenheimer, Data Engineering edition",,415,18,Top-Substance2185,2023-07-20 21:24:56,https://i.redd.it/tw33gopht6db1.png,False,False,False,False
obyzb9,When my prof asks me to “find information on every person whose been pardoned ever for the past 4 presidencies”,,414,23,veeeerain,2021-07-02 00:28:43,https://i.redd.it/z30386912p871.jpg,False,False,False,False
1b1f95l,Expectation from junior engineer ,,415,133,Foot_Straight,2024-02-27 15:56:43,https://i.redd.it/nyexsfsbh5lc1.png,False,False,False,False
zaqmay,If data engineering did Spotify Wrapped,,410,8,Straight_House8628,2022-12-02 16:33:11,https://i.redd.it/uemt99k9ii3a1.png,False,False,False,False
124d6qi,State of Data Engineering 2022,,401,101,SyntheticBlood,2023-03-28 04:49:55,https://i.redd.it/3rc8hxffueqa1.jpg,False,False,False,False
10leclf,Follow up on that Google Drive question...,,398,25,bartosaq,2023-01-26 00:19:57,https://i.redd.it/mkhpod7r6aea1.jpg,False,False,False,False
1c1cbfg,"Common DE pipelines and their tech stacks on AWS, GCP and Azure",,404,59,_areebpasha,2024-04-11 11:06:55,https://i.redd.it/1t2lrh8q1utc1.jpeg,False,False,False,False
uojh8n,Data Scientist: building a fabulous AI out of garbage,,401,23,Ems_gobears,2022-05-13 04:16:53,https://i.redd.it/wu7je3qv56z81.jpg,False,False,False,False
whz7pw,Anyone read this book? It came out in 2022 so it's very modern and up to date.,,397,76,DrRedmondNYC,2022-08-06 21:50:03,https://i.redd.it/juq64l99z5g91.png,False,False,False,False
lio4nh,"How I feel in the coding interview when I get asked about BSTs and I damn well know all I’m going to do is call apis, parse json, and copy to Redshift.",,393,20,pawtherhood89,2021-02-12 23:48:50,https://i.redd.it/8hwzbhn7w4h61.jpg,False,False,False,False
y2bl65,"What’s your process for deploying a data pipeline from a notebook, running it, and managing it in production?",,390,207,jnkwok,2022-10-12 18:32:20,https://i.redd.it/pq04w47z4ft91.jpg,False,False,False,False
sexic6,How I feel today,,392,33,Shamboma,2022-01-28 18:39:55,https://i.redd.it/usrjoe144he81.png,False,False,False,False
qr5z5v,"Ladies and gentlemen, I have good news and I wouldn't have been able to do it without this wholesome and helpful community",,386,49,noNSFWcontent,2021-11-10 22:28:13,https://i.redd.it/3tbjs8rsguy71.jpg,False,False,False,False
otwwfe,This nice illustration or visualization of the Data Pipeline by semantix (https://semantix.com.br/data-platform/). Hope it may some insights for new DE friends.,,384,25,sriny4c,2021-07-29 12:53:02,https://i.redd.it/isgxui7ue5e71.gif,False,False,False,False
v4hhz7,Just getting into Apache Airflow...this is the first thing that came to mind,,385,35,ThyssenKurup,2022-06-04 04:52:52,https://i.redd.it/natxbqa7cj391.jpg,False,False,False,False
1767l40,Just do a quick 30min to 1hr take home test. 🤡 🤡,This is UMortgage interview assessment. Reads to me like free work more than skills assessment.,376,105,Dull_Biscotti7205,2023-10-12 13:55:58,https://i.redd.it/fvlmf8qz1stb1.jpg,False,False,False,False
10kl6lg,Finally got a job,"I did it! After 8 months of working as a budtender for minimum wage post-graduation, more than 400 job applications, and 12 interviews with different companies I finally landed a role as a data engineer. I still couldn't believe it till my first day, which was yesterday. Just got my laptop, fob, and ID card, still feels so unreal. Learned a lot from this sub and I'm forever grateful for you guys.",377,99,1000gratitudepunches,2023-01-25 00:31:50,https://www.reddit.com/r/dataengineering/comments/10kl6lg/finally_got_a_job/,False,False,False,False
rr7paf,I'm Leaving FAANG After Only 4 Months,"I apologize for the clickbaity title, but I wanted to make a post that hopefully provides some insight for anyone looking to become a DE in a FAANG-like company. I know for many people that's the dream, and for good reason. Meta was a fantastic company to work for; it just wasn't for me. I've attempted to explain why below.

## It's Just Metrics
I'm a person that really enjoys working with data early in its lifecycle, closer to the collection, processing, and storage phases. However, DEs at Meta (and from what I've heard all FAANG-like companies) are involved much later in that lifecycle, in the analysis and visualization stages. In my opinion, DEs at FAANG are actually Analytics Engineers, and a lot of the work you'll do will involve building dashboards, tweaking metrics, and maintaining pipelines that have already been built. Because the company's data infra is so mature, there's not a lot of pioneering work to be done, so if you're looking to _build_ something, you might have better luck at a smaller company.

## It's All Tables
A lot of the data at Meta is generated in-house, by the products that they've developed. This means that any data generated or collected is made available through the logs, which are then parsed and stored in tables. There are no APIs to connect to, CSVs to ingest, or tools that need to be connected so they can share data. It's just tables. The pipelines that parse the logs have, for the most part, already been built, and thus your job as a DE is to work with the tables that are created every night. I found this incredibly boring because I get more joy/satisfaction out of working with really dirty, raw data. That's where I feel I can add value. But data at Meta is already pretty clean just due to the nature of how it's generated and collected. If your joy/satisfaction comes from helping Data Scientists make the most of the data that's available, then FAANG is definitely for you. But if you get your satisfaction from making unusable data usable, then this likely isn't what you're looking for.

## It's the Wrong Kind of Scale
I think one of the appeals to working as a DE in FAANG is that there is just so much data! The idea of working with petabytes of data brings thoughts of how to work at such a large scale, and it all sounds really exciting. That was certainly the case for me. The problem, though, is that this has all pretty much been solved in FAANG, and it's being solved by SWEs, not DEs. Distributed computing, hyper-efficient query engines, load balancing, etc are all implemented by SWEs, and so ""working at scale"" means implementing basic common sense in your SQL queries so that you're not going over the 5GB memory limit on any given node. I much prefer ""breadth"" over ""depth"" when it comes to scale. I'd much rather work with a large variety of data types, solving a large variety of problems. FAANG doesn't provide this. At least not in my experience.

## I Can't Feel the Impact
A lot of the work you do as a Data Engineer is related to metrics and dashboards with the goal of helping the Data Scientists use the data more effectively. For me, this resulted in all of my impact being along the lines of ""I put a number on a dashboard to facilitate tracking of the metric"". This doesn't resonate with me. It doesn't motivate me. I can certainly understand how some people would enjoy that, and it's definitely important work. It's just not what gets me out of bed in the morning, and as a result I was struggling to stay focused or get tasks done. 

In the end, Meta (and I imagine all of FAANG) was a great company to work at, with a lot of really important and interesting work being done. But for me, as a Data Engineer, it just wasn't my thing. I wanted to put this all out there for those who might be considering pursuing a role in FAANG so that they can make a more informed decision. I think it's also helpful to provide some contrast to all of the hype around FAANG and acknowledge that it's not for everyone and that's okay. 

## tl;dr
I thought being a DE in FAANG would be the ultimate data experience, but it was far too analytical for my taste, and I wasn't able to feel the impact I was making. So I left.",379,122,therealtibblesnbits,2021-12-29 13:02:08,https://www.reddit.com/r/dataengineering/comments/rr7paf/im_leaving_faang_after_only_4_months/,False,False,False,False
1aggfae,"Got a flight this weekend, which do I read first?",I’m an Analytics Engineer who is experienced doing SQL ETL’s. Looking to grow my skillset. I plan to read both but is there a better one to start with?,376,141,cheanerman,2024-02-01 17:28:14,https://i.redd.it/9a5y3tgyd0gc1.jpeg,False,False,False,False
yyh6l9,What are your favourite GitHub repos that shows how data engineering should be done?,"Looking to level up my skills and want to know what repos out there follow good data engineering practice.

What accounts/repos stood out to you?

Which repos do you find yourself peeking at from time to time?

Which ones taught you something that you didn't already know?",377,40,theoriginalmantooth,2022-11-18 10:50:18,https://www.reddit.com/r/dataengineering/comments/yyh6l9/what_are_your_favourite_github_repos_that_shows/,False,False,False,False
owkdly,53+ years of experience in data engineering…,,375,63,renok_archnmy,2021-08-02 18:16:35,https://i.redd.it/nnx7qc6tkze71.jpg,False,False,False,False
1ajyazo,Is there a DE equivalent to this?,Thought about posting in r/DataAnalysis but figured it fit here more as this is the exact reason I am trying so hard to leave my DA role and get into DE.,374,33,PoloParachutes,2024-02-06 01:56:18,https://i.redd.it/7oa8cu09gvgc1.jpeg,False,False,False,False
zdj8y1,Data engineering with ChatGPT,,371,51,bravehamster,2022-12-05 20:51:31,https://i.redd.it/jjbqnmnq554a1.png,False,False,False,False
1b7ojk4,An actual post in my company Slack today ,Mentally preparing myself for the eventual request to untangle this mess ,368,67,OneSixteenthRobot,2024-03-06 02:44:28,https://i.redd.it/nepsf40anmmc1.png,False,False,False,False
yfuknq,It's not always Old Man Jenkins...,,369,19,Top-Substance2185,2022-10-28 17:15:39,https://i.redd.it/zus7bi9wxkw91.jpg,False,False,False,False
19bg4jf,I’m releasing a free data engineering boot camp in March,"Meeting 2 days per week for an hour each. 

Right now I’m thinking: 

- one week of SQL
- one week of Python (focusing on REST APIs too) 
- one week of Snowflake 
- one week of orchestration with Airflow
- one week of data quality 
- one week of communication and soft skills 

What other topics should be covered and/or removed? I want to keep it time boxed to 6 weeks. 

What other things should I consider when launching this? 

If you make a free account at dataexpert.io/signup you can get access once the boot camp launches. 

Thanks for your feedback in advance!",358,183,eczachly,2024-01-20 16:55:10,https://www.reddit.com/r/dataengineering/comments/19bg4jf/im_releasing_a_free_data_engineering_boot_camp_in/,False,False,False,False
14zh0hc,"It's not a glamorous life, but we all know who really drives the bus",,359,52,itty-bitty-birdy-tb,2023-07-14 13:50:59,https://i.redd.it/s3fg4dwiqxbb1.png,False,False,False,False
prlh97,"Big Data Pipelines on AWS, Azure & GCP (associated blog in comments)",,349,19,Kickass_Wizard,2021-09-20 02:06:44,https://i.redd.it/zrj5sut5gko71.png,False,False,False,False
1c0uf21,And so it begins. Databricks just couldn't help themselves. Get your wallets out.,"As a long-time Databricks user, I've seen this trend toward them being solely focused on making as much money as possible. Unless they plan to change pricing as well, this will double the Job cost of people currently running Standard. Can you imagine waking up and the cost of your data platform doubling overnight?

""But you get so many more features, it's a great price, you can optimize this and that, blah, blah, blah."" Well, maybe I don't want or need Unity Catalog.

I still say it's the beginning of a new era of Corporate Databricks, bow before us and bring your tribute.

**UPDATE:**   
This post got a lot more traction than I expected ... enough for the Databricks people to hunt me down **at work** and chastise me for being naughty. I made a YouTube video about this specific topic to expand more. [https://youtu.be/GgNaLYoFP3E](https://youtu.be/GgNaLYoFP3E)



https://preview.redd.it/vzco1rabfptc1.png?width=1620&format=png&auto=webp&s=0f4d2e84bbf4e9efaafaa609c2d78b11d0170573",339,190,dataengineeringdude,2024-04-10 19:38:20,https://www.reddit.com/r/dataengineering/comments/1c0uf21/and_so_it_begins_databricks_just_couldnt_help/,False,False,False,False
104xlft,Free Data engineering bootcamp - Data Engineering Zoomcamp - starts in 10 days," Do you want to learn Data Engineering? In 2023, we start another iteration of Data Engineering Zoomcamp. It's a free, practical, 10-week long course about the main concepts in Data Engineering.   


Join us to learn about:

* Docker, Terraform and GCP
* Orchestration with Prefect
* Creating a data warehouse with BigQuery
* Analytics engineering and dbt
* Batch processing and Spark
* Stream processing and Kafka

It starts on the 16th of January, 2023.  

Sign up here: [https://github.com/DataTalksClub/data-engineering-zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)",343,88,stolzen,2023-01-06 15:54:24,https://www.reddit.com/r/dataengineering/comments/104xlft/free_data_engineering_bootcamp_data_engineering/,False,False,False,False
y9xa2k,"It is a recession after all, isn't it?",,341,6,MooJerseyCreamery,2022-10-21 15:56:57,https://i.redd.it/i2wm18ghl6v91.png,False,False,False,False
149urpv,"The ""Big Three's"" Data Storage Offerings",,343,23,Kickass_Wizard,2023-06-15 06:24:23,https://i.redd.it/ao5lvu6rk46b1.png,False,False,False,False
y7dxl4,How are you exporting your prod DB tables to your data warehouse?,,335,41,tchungry,2022-10-18 17:54:49,https://v.redd.it/0opyq3brrlu91,False,False,False,False
18ak69g,What opinion about data engineering would you defend like this?,,331,371,OverratedDataScience,2023-12-04 13:27:11,https://i.redd.it/au4t5bq55a4c1.png,False,False,False,False
14midyu,Now in Snowflake: GROUP BY ALL,,330,81,fhoffa,2023-06-29 22:20:32,https://i.imgur.com/Kh5unKB.jpg,False,False,False,False
1au9s4s,New DE advice from a Principal,"So I see a lot of folks here asking how to break into Data Engineering, and I wanted to offer some advice beyond the fundamentals of learning tool X. I've hired and trained dozens of people in this field, and at this point I've got a pretty solid sense of what makes someone successful in it. This is what I'd personally recommend.


1. Focus on SWE fundamentals. The algorithms and algebra you learned in school can feel a little impractical for day-to-day work, but they're the core of the powerful distributed processing engines you work with in DE. Moving data around efficiently requires a strong understanding of hardware behavior and memory management. Orchestration tools like Airflow are just regular applications with servers and API's like anything else. Realistically, you're not going to walk into your first DE job with experience with DE tools, but you can reason through solutions based on what you know about software in general. The rest will come with time and training.


2. Learn battle-tested modeling and architecture patterns and where to apply them. Again, the fundamentals will serve you very well here. Data teams are often tasked with handling data from all over the company, across many contexts and business domains. Trying to keep all of that straight and building bespoke solutions for each one will not only drive you insane, but will end up wasting a ton of time and money reinventing the wheel and reverse-engineering long-forgotten one-offs. Using durable, repeatable patterns is one way to avoid that. Get some books on the subject and start reading.


3. Have a clear Definition of Done for your projects that includes quality controls and ongoing monitoring. Data pipelines are uniquely vulnerable to changes entirely outside of your control, since it's highly unlikely that you are the producer of the input data. Think carefully about how eventual changes in upstream data would affect your workload - where are the fragile points, and how you can build resiliency into them. You don't have to (and realistically can't) account for every scenario upfront, but you can take simple steps to catch issues before they reach the CEO's dashboard.


4. This is a team sport. Empathy for stakeholders and teammates, in particular assuming good intentions and that previous decisions were made for a good reason, is the #1 thing I look for in a candidate outside of reasoning skills. I have disqualified candidates for off-handed comments about colleagues ""not knowing what they're talking about"", or dragging previous work when talking about refactoring a pipeline. Your job as a steward for the data platform is to understand your stakeholders and build something that allows them to safely and effectively interact with it. It's a unique and complex system which they likely don't, and shouldn't have to, have as deep an understanding of as you do. Behave accordingly. 


5. Understand what responsible data stewardship looks like. Data is often one of, if not the most, expensive line item for a company. As a DE you are being trusted with the thing that can make or break a company's success both from a cost and legal liability perspective. In my role I regularly make architecture decisions that will cost or pay someone's salary - while it will probably take you a long time to get to that point, being conscientious of the financial impact/risk of your projects makes the jobs of people who do have to make those decisions (the ones who hire and promote you) much easier. 


6. Beware hype trains and silver bullets. Again, I have disqualified candidates of all levels for falling into this trap. Every tool, language, and framework was built (at least initially) to solve a specific problem, and when you choose to use it you should understand what that problem is. You're absolutely allowed to have a preferred toolbox, but over-indexing on one solution is an indicator that you don't really understand the problem space or the pitfalls of that thing. I've noticed a significant uptick in this problem with the recent popularity of AI; if you're going to use/advocate for it, you'd better be prepared to also speak to the implications and drawbacks.


Honorable mention: this may be controversial but I strongly caution against inflating your work experience in this field. Trust me, they'll know. It's okay and expected that you don't have big data experience when you're starting out - it would be ridiculous for me to expect you to know how to scale a Spark pipeline without access to an enterprise system. Just show enthusiasm for learning and use what you've got to your advantage.


I believe in you! You got this.

Edit: starter book recommendations in this thread https://www.reddit.com/r/dataengineering/s/sDLpyObrAx",324,86,ithinkiboughtadingo,2024-02-19 00:30:46,https://www.reddit.com/r/dataengineering/comments/1au9s4s/new_de_advice_from_a_principal/,False,False,False,False
nh53m5,Is there any interest in a collection of DE interview practice questions (Data Modeling/Architecture/SQL),"I’ve done A LOT of Senior DE interviews at FAANG etc recently. I noticed that there isn’t a lot of real world interview questions online, you sort of have to rely on previous work experience. 

So now I’m building a InterviewCake style website for DE questions. It’s gonna be more subjective than algorithms obviously but I think there’s a lot of value in having questions to mull over.

Would people be interested in something like this?",329,91,TazMazter,2021-05-20 16:44:50,https://www.reddit.com/r/dataengineering/comments/nh53m5/is_there_any_interest_in_a_collection_of_de/,False,False,False,False
mwr0ji,Happened in the making of this meme as well,,324,15,ffs_not_this_again,2021-04-23 08:55:29,https://i.redd.it/hzxl4c7k0wu61.png,False,False,False,False
yx2qsb,How are you monitoring your data pipelines and what are you using to debug production issues?,,322,69,tchungry,2022-11-16 19:03:13,https://i.redd.it/f9dmcrn92d0a1.jpg,False,False,False,False
15f9uab,Fancy dashboards with volatile data pipelines!,,321,12,growth_man,2023-08-01 11:29:09,https://i.redd.it/diy1dfnohhfb1.jpg,False,False,False,False
13f4sbf,I didn’t know you guys were paid THIS well,,305,105,BeneficialTitle9042,2023-05-12 00:09:48,https://i.redd.it/3q8sosk43cza1.jpg,False,False,False,False
xkyzt8,I like caravans more.,,304,9,Petermaniac,2022-09-22 11:39:26,https://i.redd.it/yn9kvrjvcep91.png,False,False,False,False
1abmrzv,"yes, I really said it",,298,75,Awkward-Cupcake6219,2024-01-26 16:45:56,https://i.redd.it/8jacgiphctec1.jpeg,False,False,False,False
uh5juq,worried about the transition from data engineering to big data engineer,"I've been building ETL pipelines, warehousing solutions, doing schema design, and setting up data-streaming for several years now at companies like Amazon and more recently at smaller pre-ipo unicorn companies. My concern is that I'm only 5'11 (5'10.5"" really but whose counting) and my concern is that I may not be qualified to call myself a big data engineer as a result. Does anyone else have this problem? Thank you.",294,52,PacificShoreGuy,2022-05-03 02:00:11,https://www.reddit.com/r/dataengineering/comments/uh5juq/worried_about_the_transition_from_data/,False,False,False,False
s59bpv,Free data engineering course starts tomorrow!,"&#x200B;

https://preview.redd.it/l9vrn2r741c81.jpg?width=800&format=pjpg&auto=webp&s=46c01ba2b0792f43dbd9d11fc0a952ae61b6d567

We at [DataTalks.Club](https://DataTalks.Club) are running a free data engineering course.

&#x200B;

We'll cover:

* GCP, Terraform, Docker, SQL
* Data pipelines orchestration (Airflow)
* Data warehousing (Big Query)
* Analytics engineering (DBT)
* Batch processing (Spark)
* Streaming (Kafka)

More details here: [https://github.com/DataTalksClub/data-engineering-zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)

&#x200B;

See you tomorrow!",295,46,stolzen,2022-01-16 10:44:04,https://www.reddit.com/r/dataengineering/comments/s59bpv/free_data_engineering_course_starts_tomorrow/,False,False,False,False
1c7xdr0,"Data Engineers - No Matter the situation, they will fix it!!",,289,30,de4all,2024-04-19 13:50:21,https://i.redd.it/2f335ko5yfvc1.jpeg,False,False,False,False
14663ur,r/dataengineering will be joining the blackout from June 12-14 to protest the proposed API changes which will end 3rd party apps.,"[See here for the original r/dataengineering thread on this issue.](https://www.reddit.com/r/dataengineering/comments/140xtxu/does_the_de_community_want_to_join_the_reddit/)

# What's going on?

A recent Reddit policy change threatens to kill many beloved third-party mobile apps, making a great many quality-of-life features not seen in the official mobile app **permanently inaccessible** to users.

On May 31, 2023, Reddit announced they were raising the price to make calls to their API from being free to a level that will kill every third party app on Reddit, from [Apollo](https://www.reddit.com/r/apolloapp/comments/13ws4w3/had_a_call_with_reddit_to_discuss_pricing_bad/) to [Reddit is Fun](https://www.reddit.com/r/redditisfun/comments/13wxepd/rif_dev_here_reddits_api_changes_will_likely_kill/) to [Narwhal](https://www.reddit.com/r/getnarwhal/comments/13wv038/reddit_have_quoted_the_apollo_devs_a_ridiculous/jmdqtyt/) to [BaconReader](https://www.reddit.com/r/baconreader/comments/13wveb2/reddit_api_changes_and_baconreader/).

Even if you're not a mobile user and don't use any of those apps, this is a step toward killing other ways of customizing Reddit, such as Reddit Enhancement Suite or the use of the old.reddit.com desktop interface.

This isn't only a problem on the user level: many subreddit moderators depend on tools only available outside the official app to keep their communities on-topic and spam-free.

# What's the plan?

On June 12th, [many subreddits](https://old.reddit.com/r/ModCoord/comments/1401qw5/incomplete_and_growing_list_of_participating/) will be going dark to protest this policy. Some will return after 48 hours: others will go away *permanently* unless the issue is adequately addressed, since many moderators aren't able to put in the work they do with the poor tools available through the official app. This isn't something any of us do lightly: we do what we do because *we love Reddit*, and we truly believe this change will make it impossible to keep doing what we love.

The two-day blackout isn't the *goal*, and it isn't the end. Should things reach the 14th with no sign of Reddit choosing to fix what they've broken, we'll use the community and buzz we've built between then and now as a tool for further action.

What can *you* do?

1. **Complain.** Message the mods of [r/reddit](https://www.reddit.com/r/reddit/).com, who are the admins of the site: message [/u/reddit](https://www.reddit.com/u/reddit/): submit a [support request](https://support.reddithelp.com/hc/en-us/requests/new): comment in relevant threads on [r/reddit](https://www.reddit.com/r/reddit/), such as [this one](https://www.reddit.com/r/reddit/comments/12qwagm/an_update_regarding_reddits_api/), leave a negative review on their official iOS or Android app- and sign your username in support to this post.
2. **Spread the word.** Rabble-rouse on related subreddits. Meme it up, make it spicy. Bitch about it to your cat. Suggest anyone you know who moderates a subreddit join us at our sister sub at [r/ModCoord](https://www.reddit.com/r/ModCoord/) \- but please don't pester mods you *don't* know by simply spamming their modmail.
3. **Boycott** ***and*** **spread the word...to Reddit's competition!** Stay off Reddit entirely on June 12th through the 13th- instead, take to your favorite *non*\-Reddit platform of choice and make some noise in support!
4. **Don't be a jerk.** As upsetting this may be, threats, profanity and vandalism will be worse than useless in getting people on our side. Please make every effort to be as restrained, polite, reasonable and law-abiding as possible.

&#x200B;

*Any communication during the blackout will be made via* [*our official mailing list*](https://dataengineeringcommunity.substack.com/)*. Please sign up if you wish to receive updates.*",296,21,AutoModerator,2023-06-10 17:34:04,https://www.reddit.com/r/dataengineering/comments/14663ur/rdataengineering_will_be_joining_the_blackout/,False,False,False,False
zr2klf,ETL using pandas,,296,207,Salmon-Advantage,2022-12-20 23:01:14,https://i.redd.it/nu453udgd67a1.jpg,False,False,False,False
10fg07o,just got laid off (FAANG),"hi all, its been a pretty awful day. Two months ago my boss transferred out to a new team, my team was integrated in with another. Sure enough this morning i woke up to an email letting me know im no longer a valued member of the team.   


at this point im feeling in the pits. over the last few years ive gone above and beyond for this company, spending untold unpaid evening and weekend hours trying to push our projects forward.   


not sure about the next steps, but i felt like i needed to vent. if anyone out there knows of any DE opportunities, please DM me.",291,84,Foodwithfloyd,2023-01-18 19:18:49,https://www.reddit.com/r/dataengineering/comments/10fg07o/just_got_laid_off_faang/,False,False,False,False
vkfs57,"I created a pipeline extracting Reddit data using Airflow, Docker, Terraform, S3, dbt, Redshift, and Google Data Studio","Dashboard \~ [Link](https://datastudio.google.com/reporting/e927fef6-b605-421c-ae29-89a66e11ea18)

Github Project - [Link](https://github.com/ABZ-Aaron/Reddit-API-Pipeline)

# Overview

Built this a while ago, but refactored recently.

I put it together after going through the DataTalksClub [Zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp). The aim was develop basic skills in a number of tools and to visualise r/DataEngineering data over time.

I'm currently learning DE, so project is FAR from perfect, and tools used are very much overkill, but it was a good learning experience. 

I've written out the README in a way that others can follow along, set it up themselves without too much trouble, and hopefully learn a thing or two.

# Pipeline

1. Extract r/dataengineering data using the Reddit API.
2. Load file into AWS S3.
3. Copy file data to AWS Redshift.
4. Orchestrate the above with Airflow & Docker on a schedule.
5. Run some VERY basic transforms with dbt (not necessary)
6. Visualise with Google Data Studio
7. Setup (and destroy) AWS infra with Terraform

# Notes

Redshift only had a 2 month free trial, so I've destroyed my cluster. The source for my dashboard is now a CSV with some data I downloaded from Redshift before shutting down. I may create an alternate pipeline with more basic & free tools.",288,82,TheDataPanda,2022-06-25 14:05:54,https://www.reddit.com/r/dataengineering/comments/vkfs57/i_created_a_pipeline_extracting_reddit_data_using/,False,False,False,False
14vw6y3,Typical interview with Airflow enjoyer,,284,25,ponkipo,2023-07-10 14:50:32,https://i.redd.it/2231e36jh5bb1.jpg,False,False,False,False
13l9ur0,DBT lays off 15% of their staff,"DBT will be reducing their headcount by 15% of their global team. This reduction will impact every function of the business. 

My team had to migrate away from DBT after their price hike, so this is not surprising.


https://www.getdbt.com/blog/dbt-labs-update-a-message-from-ceo-tristan-handy/",285,151,Educational-Sir78,2023-05-18 20:26:44,https://www.reddit.com/r/dataengineering/comments/13l9ur0/dbt_lays_off_15_of_their_staff/,False,False,False,False
uyc87m,"""You wouldn't understand, it is not SQL!!"" - An advice for upcoming data engineers.","TLDR; Advocate for yourself as an engineer, don't let other people tell you what you know. Show them.

A bit about me before the actual story: I've worked for a mid-size tech company in the HR industry for the past 2 years as a data engineer. Before this, I've held back-end roles and other data engineer positions in companies ranging from seed-stage startups to banks. I've maintained codebases in Java, Python, Scala, and Go alongside some of the smartest people in the data industry. Saying all this not to brag, but just to show you guys I am not just a random guy with no credentials. I am an engineer.

On to the story,

4 months back the CTO hired an Engineer Manager (let's call him Scott) to help improve our workflows and mature the company from an organizational standpoint. The first red flag came when the data engineering team (myself and three other people) were not invited to the biweekly engineering meetings created by Scott. No biggie, could have been just a mistake, nothing to get mad about. When one of my co-workers nicely asked Scott to share the zoom link with us, he said ""Umm. We are not talking about anything related to analytics in those meetings but I'll add you if you insist"". Oh boy.

Two weeks later, I tried to get Scott to meet with us to talk about our data architecture and the different projects we had going on at the moment (Projects that had several SENIOR software engineers in it btw). His response?

""I appreciate the offer but I really should take my first few weeks at the company to get up to speed with my engineering team about THEIR projects. We can discuss data analytics at a later time.""

The audacity of this man!

I quickly responded with an email with the CTO cc'd saying how urgent some of our work was and how we would appreciate a meeting with the leadership team (Scott included) to talk about it. The CTO immediately send google an invite to a meeting with all of us and thanked the data team for driving software talks with the new manager. But apparently, that wasn't enough for Scott to understand he was being dismissive of our highly technical team just because we had the word ""data"" in front of our titles.

Here is the worst part and the reason for the post:

The front-end team has different Github repos for code written in React and Python. There was a problem in one of their visualizations where the data showed to clients was completely different from what we have in our database. Because of this, I created a ticket to try to pair with one of the front-end folks to debug the code behind the reports and for them to share the GitHub repo with me. Come to find out, the ticket was removed from the front-end inbox a few days later!

What was left was a comment from Scott on why he removed the ticket, saying:

""Removed from the team's backlog and moved to a later date. There is no reason to share GitHub credentials with the data team as the dashboard is written in Python, not in Raw SQL""

The audacity, the gall, the gumption, the nerve of this man!

This is where I want to give a bit of advice to future data engineers/SWEs, don't take this kind of talk lightly, advocate your abilities and your expertise at every step of your career. When someone is deliberately dismissing your abilities as an engineer or even your years of experience, stand up and correct them. Otherwise, your role within your organization will become diminished as people won't trust you can get the job done. This happens more often than you think, especially when you encounter managers/senior engineers who are know-it-alls who think their shit doesn't stink.

Now, I was irate. As I said before, I'm an engineer who is passionate about data. Not a data analytics guy who only understands excel and SQL. So I immediately screenshotted Scott's comment, send it to our team and the CTO in a slack channel, and wrote how Scott's actions were excluding us from the engineering team, making our work much harder, and somewhat denigrating our abilities. I ended the message with something like:

""I know he is new to the team, but I don't think we must advocate our resumes to him. Our titles should be sufficient.""

A couple of days later, Scott met with our team for the first time to apologize to us. a little too late if you ask me.",285,58,uncomfortablepanda,2022-05-26 17:01:52,https://www.reddit.com/r/dataengineering/comments/uyc87m/you_wouldnt_understand_it_is_not_sql_an_advice/,False,False,False,False
12ekdv2,Data engineers processing data access requests,,277,24,Bart_Vee,2023-04-07 12:54:01,https://i.redd.it/fvhpekkylgsa1.png,False,False,False,False
tfm2hx,This job at Chewy looks very interesting.,,279,36,Deb_Tradeideas,2022-03-16 16:31:32,https://i.redd.it/2979auv2wrn81.jpg,False,False,False,False
v8k8gs,Me when the DAG run fails,,275,12,NaClEric,2022-06-09 16:08:00,https://i.redd.it/qob8bxq0dm491.png,False,False,False,False
13umeek,"So I watched a few videos about Fabric, and started to cry a little...",[DELETED] ` this message was mass deleted/edited with redact.dev `,277,104,None,2023-05-29 05:58:16,https://www.reddit.com/r/dataengineering/comments/13umeek/so_i_watched_a_few_videos_about_fabric_and/,False,False,False,False
wriowg,"If you know, you know",,271,38,SoonerDay,2022-08-18 12:46:42,https://i.imgur.com/4DF3dym.jpg,False,False,False,False
nto0nd,Data Engineering project for beginners V2,"Hello everyone,

A while ago, I wrote an [article](https://www.reddit.com/r/dataengineering/comments/gq2bmf/data_engineering_project_for_beginners/) designed to help people who are new to data engineering, build an end-to-end data pipeline and learn some of the best practices in data engineering.

Although this article was well-received, it was hard to set up, follow, and used Airflow 1.10. Hence, I  made setup easy, made code more understandable, and upgraded to Airflow 2.

Blog: [https://www.startdataengineering.com/post/data-engineering-project-for-beginners-batch-edition](https://www.startdataengineering.com/post/data-engineering-project-for-beginners-batch-edition)

Repo: [https://github.com/josephmachado/beginner\_de\_project](https://github.com/josephmachado/beginner_de_project)

Appreciate any questions, feedback, comments. Hope this helps someone.",272,32,joseph_machado,2021-06-06 15:18:03,https://www.reddit.com/r/dataengineering/comments/nto0nd/data_engineering_project_for_beginners_v2/,False,False,False,False
164ybot,Pathway from Data Analyst to Data Engineer: Tips & Takeaways,"Long time lurker here looking for some feedback!

I'm delivering an internal talk to other consultants at my company (primarily Data Analytics Consultants) about my personal journey to become a Data Engineer ( Photographer --> DA --> DE ).

I've been compiling a list of tips and takeaways to punctuate my talk and I'm hoping to get some input from the r/dataengineering brain trust.

**Here's what I've got (in no particular order):**

1. **Data Engineering is fundamentally just moving data around**, and reshaping it.
2. **Be curious.** Learn how things work. Try stuff out. Experiment.
3. Become **intimately familiar with data types**, sources, and structures.
4. **Learn a General Purpose Programming Language**. It doesn't really matter which one, it's the fundamentals that are important—everything else is just syntax.
   1. If you don't know which one to pick, start with Python
5. **Get good at SQL**. It's nearly 50 years old and you're probably going to retire before it does.
   1. No matter what systems and tools you use there's a good chance that it probably uses SQL or something pretty similar.
   2. Even more modern data stores, like data lakes, are still queried using SQL
6. **Learn to use the command line (PowerShell, CMD or Bash).** There are so many problems that can be solved much faster in the terminal.
7. **Learn how computers work, at least a little bit.** How do they communicate? How do they process and store information? What does a server do?
8. **Do as many personal projects as you can.** Sign up for a GitHub account and publish them there.
9. **Get really comfortable using APIs and parsing JSON data.** Outside of databases this is probably how you're going to interact with most of your data.
10. **Get really good at your tools, and then get better.** But, also be at least familiar with what else is out there.
11. **Understand the differences between a Database, Data Lake, and Data Warehouse.** What's the difference between OLTP and OLAP?
12. **Learn to use a Cloud Platform.** AWS has a pretty good Free Tier, try it out and learn what the different services do.
13. **Strong business knowledge is extremely valuable** in both Data Engineering and Data Analytics.
14. Understand different business metrics and how they're calculated.
15. **Learn to find the grain (level of detail) of data.** How is it structured? What is the smallest unit? What exactly is a ""row"" in this table?
16. **When it comes to data, everything (almost) is either a JSON, XML, CSV/\*SV, SQLite, or Database.**
17. Even proprietary files with different extensions are probably one of these. Tableau and Alteryx files are just XML files, and many applications store data in .db files (SQLite).
18. Sometimes a file is just a zipped folder of files. Excel for example is just a zipped folder of XML files.

That's what I've got so far, but the talk is next week so I've got some time to make changes.

What did I miss? What should I remove?

Thanks Team! 😘

Edit: fixed some indenting issues. 14 -> 13.1; 15 -> 14; 16 -> 15; 17 -> 15.1; 18 -> 15.2.

Edit 2: nvm, I'm not allowed to have nice things.",268,63,danlsn,2023-08-29 23:36:15,https://www.reddit.com/r/dataengineering/comments/164ybot/pathway_from_data_analyst_to_data_engineer_tips/,False,False,False,False
129w7vp,I got the job!,"I felt the need to let everyone on this subreddit know I got my dream job offer.

You gave me a bollocking for calling OLAP cubes outdated. I'm sorry I pissed all of you off. 

You pointed out I'm applying for the wrong jobs, and the platform engineering roles are sometimes hidden in devops and software engineering adverts.

You advised that an in-person second stage interview is likely to be a whiteboarding session when I didn't know what to expect.

I made it!

Thank You!",266,51,va1kyrja-kara,2023-04-02 19:58:51,https://www.reddit.com/r/dataengineering/comments/129w7vp/i_got_the_job/,False,False,False,False
1b44v59,Why are there so many ETL tools when we have SQL and Python?,"I've been wondering why there are so many ETL tools out there when we already have Python and SQL. What do these tools offer that Python and SQL don't? Would love to hear your thoughts and experiences on this.

And yes, as a junior I’m completely open to the idea I’m wrong about this😂",264,156,dildan101,2024-03-01 20:39:32,https://www.reddit.com/r/dataengineering/comments/1b44v59/why_are_there_so_many_etl_tools_when_we_have_sql/,False,False,False,False
p3kpq9,1 year of must-read articles,"Last week I've featured 1 year of must-read content about data in one post. If you want to understand better trendy concepts: Modern Data Stack, Data Mesh, Analytics Engineering you can start by reading those articles.

I add directly in Reddit the reading list, but if you want to read my opinion on the matter or support this kind of content do not hesitate to subscribe to the weekly newsletter [there](https://www.blef.fr/data-news-must-read-articles/).

## Modern Data Stack

* [The Modern Data Stack: Past, Present, and Future](https://blog.getdbt.com/future-of-the-modern-data-stack/)
* [Why the Future of ETL Is Not ELT, But EL(T)](https://airbyte.io/blog/why-the-future-of-etl-is-not-elt-but-el) \+ Reddit thread about [Is it just me or ELT seems over hyped?](https://www.reddit.com/r/dataengineering/comments/o5jjig/is_it_just_me_or_elt_seems_over_hyped/)
* [Data & Data Engineering — the past, present, and future](https://medium.com/@eczachly/data-data-engineering-the-past-present-and-future-ac3ad5795ddf)
* [Building The Modern Data Team](https://pedram.substack.com/p/modern-data-team)

## Data Mesh

* [What the Heck is a Data Mesh?!](https://cnr.sh/essays/what-the-heck-data-mesh)
* [Building a data mesh to support an ecosystem of data products at Adevinta](https://medium.com/adevinta-tech-blog/building-a-data-mesh-to-support-an-ecosystem-of-data-products-at-adevinta-4c057d06824d)
* [Orginal writeup: Data monolith to mesh](https://martinfowler.com/articles/data-monolith-to-mesh.html)

## Data Engineering

* [Data Engineering Roadmap](https://github.com/datastacktv/data-engineer-roadmap) — DataStack drawn an awesome roadmap to discover all data engineering concepts. A must seen.
* [How Data Engineering Works](https://www.youtube.com/watch?v=qWru-b6m030) — A YouTube video describing and illustrating in 14 minutes how Data Engineering works.
* [Data Engineering Manifesto](https://medium.com/connecting-dots-blog/the-data-engineering-manifesto-37626aaa208f) — I love it. A poster with 9 principles regarding data engineering.
* [Data Engineering in 5400-words](https://twitter.com/chipro/status/1357329955131191298?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1357329955131191298%7Ctwgr%5E%7Ctwcon%5Es1_&ref_url=https%3A%2F%2Fwww.redditmedia.com%2Fmediaembed%2Flclm3s%3Fresponsive%3Dtrueis_nightmode%3Dfalse) — Chip Huyen wrote a huge Google Doc with her lecture note on the basics of data engineering.
* [One Skill Every Data Engineer Needs](https://medium.com/geekculture/the-most-important-skill-for-data-engineers-46fc1faff6ae)
* [We Don't Need Data Scientists, We Need Data Engineers](https://www.mihaileric.com/posts/we-need-data-engineers-not-data-scientists/)
* [Introduction to Databases](https://github.com/oleg-agapov/data-engineering-book/blob/master/book/2-beginner-path/2-1-databases/databases.md)

## Data analytics (and teams)

* [How should our company structure our data team?](https://medium.com/snaptravel/how-should-our-company-structure-our-data-team-e71f6846024d)
* [Analytics is at a crossroads](https://benn.substack.com/p/analytics-is-at-a-crossroads) \+  [Against SQL](https://scattered-thoughts.net/writing/against-sql) \+ [For SQL](https://pedram.substack.com/p/for-sql)
* [What makes a data analyst excellent (part 2)](https://towardsdatascience.com/what-makes-a-data-analyst-excellent-17ee4651c6db)

## Data Lineage, cataloging, observability, etc.

This part is present in the original post but more an opening for later waiting for consolidation in that space.

&#x200B;

That's all for me. I hope you'll like this kind of curation content (I know a lot of this have been already published here but sporadically) and if you think something should be added here I open the discussion 👇",267,20,blef__,2021-08-13 10:28:20,https://www.reddit.com/r/dataengineering/comments/p3kpq9/1_year_of_mustread_articles/,False,False,False,False
xvr7sm,The only insightful venn diagram I've ever made,,268,7,JParkerRogers,2022-10-04 21:26:19,https://i.redd.it/8jjwmi7rwur91.png,False,False,False,False
10lsa27,Don't Fall for the Hype: A Data Professional's Perspective on Familiar Concepts Rebranded as Innovations,"As a data professional with 20 years of experience, I've seen repeated terms in tech over and over again. Today, I discovered ""**Personalized API**"", yet another new term for something that already existed. It's similar to an **Analytics API**, **Semantic Layer**, and other existing technologies.

It's important to remember that just because a term is new, doesn't mean the technology is. Data modeling with **Kimball/Inmon**, Reverse ETL, Data Mesh, and Data Lakes are just a few examples of familiar concepts that are being rebranded as new innovations.

**One Big Table (OBT)**, mainly a big denormalized table, is the same as core and data marts **Dimensional Modeling** by Kimball/Inmon, published initially in February 1996, using **Materialized Views** to persist them.

**Reverse ETL** is just an additional step to the existing data pipeline, or you might call it **Master Data Management (MDM)**, where Business people in typically ""stewardship"" add business data back to the DWH. **Semantic Layers** have been here since the beginning of Business Intelligence tools (called BO Universe); one could say it's also a fancy term for **OLAP Cubes**. Highly praised, although they have existed since the beginning of BI with SSAS (MS), OBIEE (Oracle), and SAP BI/BW (SAP).

**Data Mesh** is another hyped term and another name for microservices, which we fought a couple of years back, breaking out of monolithic **Data Warehouses** or B**usiness Intelligence** applications. And if you want to add another buzzword here, **software-defined assets**, try a similar thing with defining each **Data Product** as an asset. **Data Contracts**, haven't we validated schemas and data types all our life? 😉 Encoding data tests in our applications, sometimes through creating an abstraction in-between with an API, sometimes within the ETL tool as part of **Data Governance**.

**Data Lake** is another term for a **Data Warehouse** on top of distributed files. A **Lakehouse** is a data warehouse based on open standards.

Let's not get caught up in buzzwords and hype. Let's focus on understanding the technology and its capabilities, rather than the name it's given. Have you encountered similar situations in your career, or what terms reminded you of something you learned long ago?",262,99,sspaeti,2023-01-26 13:52:34,https://www.reddit.com/r/dataengineering/comments/10lsa27/dont_fall_for_the_hype_a_data_professionals/,False,False,False,False
10e5fus,Job search for Data Engineering in Stockholm (2yoe),,263,51,aleda145,2023-01-17 06:49:14,https://i.redd.it/ifsbknzovjca1.png,False,False,False,False
12j7r5a,can't wait for an end to end python stack with no JVM,,258,63,gorkemyurt,2023-04-12 03:04:04,https://i.redd.it/a7vapom3ddta1.png,False,False,False,False
v9hwf8,My Job Search as a Mid-Level Data Engineer since March 2022,,257,56,TAno15,2022-06-10 21:20:12,https://i.redd.it/pt0mg1ak1v491.png,False,False,False,False
12meohj,One day we’ll get the respect we deserve 🥲,,257,39,ThatGrayZ,2023-04-14 21:45:47,https://i.redd.it/pqafjpltoyta1.jpg,False,False,False,False
150qcx2,"Is this fear-mongering, or is this actually truthful?",,253,130,Analyst2163,2023-07-15 23:24:51,https://i.redd.it/poz94nkcq7cb1.png,False,False,False,False
18xj97r,Why does nothing ever get used?,"Dashboards, views, tables, pipelines, entire data marts. Why does 90% of the work I do never get used?   

I used to be one of the best BA's in my entire company so I am very good at requirements gathering and understanding what the business is trying to accomplish. Most of the work that I get comes from the CEO/VP level (global corporation not startup so real CEO and real VP 's) so a lot of people seem like they are very invested in solving these problems and my work always gets rave reviews.....but once things go into prod they basically never get touched.  

Six months ago I just.... stopped doing QA.. I have been relying on the ""scream test"", I mark tickets resolved and immediately move to prod and only do QA if someone screams that something is wrong. I have yet to hear back on anything.",255,64,None,2024-01-03 13:20:50,https://www.reddit.com/r/dataengineering/comments/18xj97r/why_does_nothing_ever_get_used/,False,False,False,False
16vhp70,Worst Data Engineering Mistake youve seen?,"I started work at a company that just got databricks and did not understand how it worked.

So, they set everything to run on their private clusters with all purpose compute(3x's the price) with auto terminate turned off because they were ok with things running over the weekend.  Finance made them stop using databricks after two months lol.

Im sure people have fucked up worse.  What is the worst youve experienced?",257,188,Inevitable-Quality15,2023-09-29 17:26:27,https://www.reddit.com/r/dataengineering/comments/16vhp70/worst_data_engineering_mistake_youve_seen/,False,False,False,False
wvor7o,The problem with data industry is hiring roles instead of people,"Data Engineer, Database Architecht, Data Scientist, Solution Architecht, Data Specialist...

Each one of these categories contains a wide variety of skillsets with a lot of overlap. Some companies call anyone who knows SQL a Data Engineer, and some companies call anyone who knows XGBoost a Data Scientist.  On the flip side, I've seen companies that have one Data Engineer and are running around with their heads cut off because the CTO decided they needed a new platform. I've seen companies that have one Data Scientist and when hired, the CTO says ""Ok, you're a scientist. Now do some data!""

I started out as an actuary in 2013, then moved to Data Science in 2018, and now lean heavier on the Data Engineering and Solution Architecting side of things because that's where the demand and money is at.

I've done tons of staff aug for companies and I have noticed a similar pattern: they can't find talent that has a holistic view on data. The data engineers only know and care about data engineering, data scientists only care about their algorithms, etc. There's no collaboration, communication or understanding of the other sides of the shop and no one there to form the bridge. 

I think the problem stems from there being TOO much out there to understand and be competent at. So younger folk go off to the youtubes and watch surface level videos on a technology so they can put ""proficient"" on their resume. Then when they're thrown onto their first project, they have to either figure it out quickly, or embarrass themselves. The ""thrown to the wolves"" strategy is very common in corporate culture. 

My advice to the young folk: take time to understand the theoretical knowledge of why you're doing something rather than just because your boss told you to. Think about what you would have done differently if you were in a leadership position and what technologies you would rely on. If you rely on a specific tool or technology, what are the pros and cons of using it over another technology? 

If I had to suggest a book, it'd be ""Designing Data-Intensive Applications"" by Martin Kleppmann. It's a very dense book, but contains a lot of valuable information. It's important to remember that technologies are just a tool and what's popular and in demand right now, might not be the case in the future. Otherwise, I'd still be in Excel formulating solutions in  VBA instead of realizing I should have created a Python pipeline all together. 

In terms of core technologies to know:

* SQL: Not just SELECT \*, but DDL, DML, CTE, windowing functions etc.
   * Rule of thumb: If you can do it in SQL, do it in SQL
* Python: It's quick enough for most cases and has huge community support (easier to find job placement)
* Spark/Distributed Computing: Distributed computing is going nowhere, but the query engine used will vary. I say Spark because it's the easiest to learn platform for now. PySpark is really intuitive, but the underlying concepts around drivers/excututors/tuning clusters is where the real value comes in. Spark is open source, has a lot of community support, and is in demand right now. The skillsets learned from Spark/distributed computing are transferrable to other platforms like Snowflake, AWS Athena, Dremio, Presto/Trino, Ignite, Impala etc.
* Streaming technology: Also important to distinguish the difference between batch processing and streaming. Some companies will need insane latency requirements and you have to think about the physical devices the data interacts with in order to meet those requirements. Apache Flink, Cassandra/Kafka are useful starting points. Kafka reins supreme right now, but it's a hugely competitive area right now.",256,76,None,2022-08-23 13:24:19,https://www.reddit.com/r/dataengineering/comments/wvor7o/the_problem_with_data_industry_is_hiring_roles/,False,False,False,False
pkdprn,2nd time this week and it's Wednesday,,251,17,Thriven,2021-09-08 16:17:51,https://i.redd.it/szcfz1x81bm71.jpg,False,False,False,False
14yfh6p,"Python library for automating data normalisation, schema creation and loading to db","

Hey Data Engineers!,

For the past 2 years I've been working on a library to automate the most tedious part of my own work - data loading, normalisation, typing, schema creation, retries, ddl generation, self deployment, schema evolution... basically, as you build better and better pipelines you will want more and more.

The value proposition is to automate the tedious work you do, so you can focus on better things.

So dlt is a library where in the easiest form, you shoot response.json() json at a function and it auto manages the typing normalisation and loading.

In its most complex form, you can do almost anything you can want, from memory management, multithreading, extraction DAGs, etc.

The library is in use with early adopters, and we are now working on expanding our feature set to accommodate the larger community.

Feedback is very welcome and so are requests for features or destinations.

The library is open source and will forever be open source. We will not gate any features for the sake of monetisation - instead we will take a more kafka/confluent approach where the eventual paid offering would be supportive not competing. 

Here are our [product principles](https://dlthub.com/product/) and docs page and our [pypi page](https://pypi.org/project/dlt/).



I know lots of you are jaded and fed up with toy technologies - this is not a toy tech, it's purpose made for productivity and sanity.



Edit: Well this blew up! Join our growing slack community on dlthub.com ",248,116,Thinker_Assignment,2023-07-13 08:57:27,https://www.reddit.com/r/dataengineering/comments/14yfh6p/python_library_for_automating_data_normalisation/,False,False,False,False
14abng6,Is data at every company still an absolute mess?,"So I switched from mechanical engineering to IoT data engineering about a year ago. At first I was pretty oblivious to a lot of stuff, but as I've learned I look around in horror.

There's so much duplicate information, bad source data, free-for-all solo project DBs.

Everything is a mess and I can't help but think most other companies are like this. Both companies I've worked for didn't start hiring a serious amount of IT infrastructure until a few years ago. The data is clearly getting better but has a loooong way to go.

And now with ML, Industry 4.0, and cloud being pushed I feel companies will all start running before they walk and everything will be a massive mess.

I thought data jobs were peaking now but in reality I think they're just now going to start growing, thoughts?",245,152,Reddit_Account_C-137,2023-06-15 19:56:19,https://www.reddit.com/r/dataengineering/comments/14abng6/is_data_at_every_company_still_an_absolute_mess/,False,False,False,False
130rfc2,What's your favorite data quality horror story?,"My personal favorite... A man's health insurance bill went up astronomically after moving from the EU to the USA because his height was listed at 1.8ft instead of meters. Needless to say, the insurance company decided someone shaped like a 180lb pancake is a high-risk individual to insure.",246,68,superconductiveKyle,2023-04-27 15:26:04,https://www.reddit.com/r/dataengineering/comments/130rfc2/whats_your_favorite_data_quality_horror_story/,False,False,False,False
114vyvz,Snowflake pushing snowpark really hard,,246,110,letmebefrankwithyou,2023-02-17 19:57:07,https://i.redd.it/enkguffr0tia1.png,False,False,False,False
qsgd02,The ecosystem be like that sometimes,,246,11,mwlon,2021-11-12 17:34:58,https://i.redd.it/0jinjoyi97z71.png,False,False,False,False
nvj4m2,"Now that Snowflake can store and analyze unstructured data, Padme is in for a great surprise",,247,21,fhoffa,2021-06-09 00:41:22,https://i.imgur.com/myxj0qd.png,False,False,False,False
1bp335j,Airflow homies be like...,,242,36,JoeyWeinaFingas,2024-03-27 14:22:09,https://i.redd.it/figwno4pyvqc1.png,False,False,False,False
16frhic,The state of data content on LinkedIn: you can reduce costs by just doing less! Game changing,,240,41,dataxp-community,2023-09-11 10:11:02,https://i.redd.it/cg4yomx0plnb1.jpg,False,False,False,False
11kth8p,Getting tired of “How do I break into DE posts”,"I would like to see more substantial content in this Reddit.  Lately the depth and quality of recent posts have not really added anything to the community.

Are there any thoughts on how to lift and improve it?",239,68,TheCauthon,2023-03-07 08:21:34,https://www.reddit.com/r/dataengineering/comments/11kth8p/getting_tired_of_how_do_i_break_into_de_posts/,False,False,False,False
19f9dba,"Well guys, this is the end",🥹,241,128,marclamberti,2024-01-25 13:30:21,https://i.redd.it/mnp1r3m49lec1.jpeg,False,False,False,False
yez0ad,It's cron all the way down,,237,19,FireflyCaptain,2022-10-27 18:10:00,https://i.redd.it/yaeal5qi2ew91.png,False,False,False,False
12asp78,MLOps is 98% Data Engineering,"After a few years and with the hype gone, it has become apparent that MLOps overlap more with Data Engineering than most people believed.

I wrote my thoughts on the matter and the awesome people of the MLOps community were kind enough to host them on their blog as a guest post. You can find the post here:

[https://mlops.community/mlops-is-mostly-data-engineering/](https://mlops.community/mlops-is-mostly-data-engineering/)",237,55,cpardl,2023-04-03 18:13:24,https://www.reddit.com/r/dataengineering/comments/12asp78/mlops_is_98_data_engineering/,False,False,False,False
1bkebv5,We (Dagster) are throwing a party,"Hi /r/dataengineering,

I'm Pete, the CEO at Dagster Labs. We're launching our new product, Dagster+, in a few weeks, and are having launch parties in SF and NYC to preview the release with the community and enjoy some food, drinks, and good company.

This subreddit has been very supportive of us over the past few years, and we'd love to see you there! They're happening the evening of April 12th, and you can sign up [here](https://share.hsforms.com/10i2u9WzCRyK77-D0MCn89gq55vz).

Spots are limited, so we’ll reach out to confirm attendance as available closer to the date of the parties. Thanks in advance for expressing your interest!",235,39,floydophone,2024-03-21 18:57:57,https://www.reddit.com/r/dataengineering/comments/1bkebv5/we_dagster_are_throwing_a_party/,False,False,False,False
1491swe,A must-read data engineering collection,"I just finished writing up a welcome gift for my newsletter, but I wanted to share at least the list of links here. 

For comments on all the books & articles, don't hesitate to subscribe to [https://www.finishslime.com/](https://www.finishslime.com/).  

*FWIW: I have read all of these, and I did consider all of them very helpful for my data engineering skills! This is not a bogus collection of what others have shared.* 

# Books 

* [Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems](https://www.amazon.de/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321) \- Martin Kleppmann
* [Fundamentals of Data Engineering](https://www.amazon.de/-/en/Joe-Reis/dp/1098108302) \- Reis & Housley
* [Data Science for Business](https://www.amazon.de/-/en/Foster-Provost/dp/1449361323/) \- Provost & Fawcett
* [Big Data](https://www.amazon.de/-/en/Nathan-Marz/dp/1617290343): Principles and best practices of scalable realtime data systems - Nathan Marz
* [Database Reliability Engineering](https://www.amazon.com/Database-Reliability-Engineering-Designing-Operating/dp/1491925949/): Designing and Operating Resilient Database Systems - Campbell Majors
* [Storytelling with data](https://www.amazon.com/Storytelling-Data-Visualization-Business-Professionals/dp/1119002257) \- Nussbaumer Knaflic
* [Data Mesh](https://www.amazon.com/Data-Mesh-Delivering-Data-Driven-Value/dp/1492092398/) \- Zhamak Dehghani

# Articles from last year

* [Stop aggregating away the signal in your data](https://stackoverflow.blog/2022/03/03/stop-aggregating-away-the-signal-in-your-data/%20) — Zan Armstrong 
* [Data Mesh in practice](https://www.starburst.io/info/data-mesh-in-practice-ebook/%20) — Max Schultze & Arif Wider
* [The future of the modern data stack](https://www.montecarlodata.com/the-future-of-the-modern-data-stack/) — Barr Moses
* [Reshaping data engineering](https://preset.io/blog/reshaping-data-engineering/) — Maxime Beauchemin
* [Emerging Architectures for modern data infrastructure](https://a16z.com/2020/10/15/emerging-architectures-for-modern-data-infrastructure/) — Matt Bornstein, Jennifer Li, Martin Casado
* [Dodging the data bottleneck, data mesh at starship](https://www.starship.xyz/medium_blog_posts/dodging-the-data-bottleneckdata-mesh-at-starship/) — Taavi Pungas
* [3 Level data lakes](https://youtu.be/4zLCUPNIV3M) — Paul Singman
* [Miro's journey to data monitoring](https://medium.com/miro-engineering/our-journey-to-data-engineering-monitoring-c14d6ff20351) — Goncalo Costa, Ricardo Souza
* [Photobox data platform](https://medium.com/photobox-technology-product-and-design/photobox-new-data-platform-da5d70296ba0) — Stefan Solimito
* [Talk on Functional Data Engineering](https://www.youtube.com/watch?v=4Spo2QRTz1k&feature=youtu.be&themeRefresh=1) — Maxime Beauchemin

# Overall great articles 

* [The Rise of the Data Engineer](https://medium.com/free-code-camp/the-rise-of-the-data-engineer-91be18f1e603)
* [The Modern Stack of ML Infrastructure](https://outerbounds.com/blog/the-modern-stack-of-ml-infrastructure/)
* [The Downfall of the Data Engineer](https://maximebeauchemin.medium.com/the-downfall-of-the-data-engineer-5bfb701e5d6b)
* [How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh](https://martinfowler.com/articles/data-monolith-to-mesh.html)
* [Functional Data Engineering — a modern paradigm for batch data processing](https://maximebeauchemin.medium.com/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a)
* [Data Mesh Principles and Logical Architecture](https://martinfowler.com/articles/data-mesh-principles.html)
* [The Future Of Business Intelligence Is Open Source](https://preset.io/blog/future-of-business-intelligence/)
* [Tristan Handy on the changing face of the data stack](https://mixpanel.com/blog/tristan-handy-changing-data-stack/)
* [The Future of the Data Engineer](https://preset.io/blog/the-future-of-the-data-engineer/)
* [The Modern Data Stack: Past, Present, and Future](https://www.getdbt.com/blog/future-of-the-modern-data-stack/)
* [The Case for Dataset-Centric Visualization](https://preset.io/blog/dataset-centric-visualization/)
* [Building The Modern Data Team](https://databased.pedramnavid.com/p/modern-data-team)
* [Introducing Entity-Centric Data Modeling for Analytics](https://preset.io/blog/introducing-entity-centric-data-modeling-for-analytics/)
* [We Don't Need Data Scientists, We Need Data Engineers](https://www.mihaileric.com/posts/we-need-data-engineers-not-data-scientists/)
* [How should our company structure our data team?](https://medium.com/super/how-should-our-company-structure-our-data-team-e71f6846024d)
* [What makes a data analyst excellent?](https://towardsdatascience.com/what-makes-a-data-analyst-excellent-17ee4651c6db)
* [Data Strategy: Good Data vs. Bad Data](https://towardsdatascience.com/data-strategy-good-data-vs-bad-data-d40f85d7ba4e)
* [What Companies REALLY Want in an Analytics Engineer](https://medium.com/geekculture/what-companies-really-want-in-an-analytics-engineer-1ac03ff4494a)
* [Stop using so many CTEs](https://hex.tech/blog/stop-using-so-many-ctes/)
* [7 Antifragile Principles for a Successful Data Warehouse](https://blog.picnic.nl/7-antifragile-principles-for-a-successful-data-warehouse-574b655f0bc6)

What about you? Got anything to add? I bet!",231,15,sbalnojan,2023-06-14 07:52:01,https://www.reddit.com/r/dataengineering/comments/1491swe/a_mustread_data_engineering_collection/,False,False,False,False
13wqhby,Databricks and Snowflake: Stop fighting on social,"I've had to unfollow Databricks CEO as it gets old seeing all these Snowflake bashing posts. Bordeline click bait. Snowflake leaders seem to do better, but are a few employees I see getting into it as well. As a data engineer who loves the space and is a fan of both for their own merits (my company uses both Databricks and Snowflake) just calling out this bashing on social is a bad look. Do others agree? Are you getting tired of all this back and forth?",236,220,slayer_zee,2023-05-31 16:11:34,https://www.reddit.com/r/dataengineering/comments/13wqhby/databricks_and_snowflake_stop_fighting_on_social/,False,False,False,False
105o50o,Data pipeline design patterns,"Hello everyone,

I've been building data pipelines for a while now. In contrast to the availability of examples of code design patterns, & data modeling techniques, there are few to none on data flow design patterns. In my experience building pipelines, using the appropriate data flow patterns increases feature delivery speed, decreases toil during pipeline failures, and builds trust with stakeholders.

With that in mind, I wrote an article that goes over the most commonly used data flow design patterns, what they do, when to use them, and, more importantly, when not to use them. It's aimed to give an overview of the typical data flow patterns and guidelines for choosing the appropriate one for your use case.

[https://www.startdataengineering.com/post/design-patterns/](https://www.startdataengineering.com/post/design-patterns/)

I'd love to hear about any patterns that I have missed. Any feedback is appreciated. I hope this helps someone :)",235,31,joseph_machado,2023-01-07 12:40:02,https://www.reddit.com/r/dataengineering/comments/105o50o/data_pipeline_design_patterns/,False,False,False,False
1bix81r,F1 team Williams used Excel as their database to track the car components (hundreds of thousands of different components),,233,53,Tape56,2024-03-19 22:21:09,https://www.the-race.com/formula-1/shocking-details-behind-painful-williams-f1-revolution/,False,False,False,False
17mi1iz,Be aware of the AI snake salesman,"I got a demo of a new product a team is gonna implement in my company that is going to ‘use AI to help users find data and reports’. 

It had nothing to do with AI. You had to manually tag every report with answers that report could answer. Then if someone’s question they typed in matched a tag it would put it at the top of the search results. 

I asked what was the AI component of this? They said because the bot responds to the question. 

It’s a good reminder to try and educate business users about AI and how it isn’t currently going to solve all their issues.",232,46,anon_data_person,2023-11-03 00:15:36,https://www.reddit.com/r/dataengineering/comments/17mi1iz/be_aware_of_the_ai_snake_salesman/,False,False,False,False
15wunwt,Me around Customer Success,,235,15,little_dog_lover,2023-08-21 02:32:01,https://i.redd.it/vo8kt5akkdjb1.jpg,False,False,False,False
160nyxw,Just got certified! - Databricks Certified Data Engineer Professional,"Hi all!

Just successfully completed the Databricks Data Engineering Professional certification. Admittedly, the Professional certification was pretty difficult, primarily due to the lack of resources available online and the extensive range of concepts (including Apache Spark™, Delta Lake, MLflow, Databricks CLI, and more) you are expected to know.

Here are the resources I used:

1. Databricks Advanced Data Engineering Course (Free): Used my customer account. It's open for anyone to sign up, and they even offer a 2-week trial. You can download the .dbc files, upload them to the community edition workspace, and get hands-on experience.
2. Udemy: [Databricks Certified Data Engineer Professional Course](https://www.udemy.com/course/databricks-certified-data-engineer-professional) \- Currently, this is the only course available. While some topics like MLFlow and certain CLI concepts are missing, making it slightly outdated, it's an excellent for a decent foundation.
3. Practice Tests:  
\- [Practice Exams for Databricks Data Engineer Professional](https://www.udemy.com/course/practice-exams-databricks-data-engineer-professional-k/) \- This is a decent resource to pinpoint weak areas. However, it lacks questions on MLFlow and CLI, and it's from the same author as the above course.  
- [Databricks Data Engineer Professional Practice Exams](https://www.udemy.com/course/databricks-data-engineer-professional-practice-exams-i) \- An invaluable resource, that I relied on heavily throughout prep. The questions are in-depth and cover all topics. Ensure you go through both the questions and answers meticulously.

4. YouTube Resources:  
\- [Advanced Analytics](https://www.youtube.com/@AdvancingAnalytics): A fantastic channel for deep-diving into a plethora of concepts.

- [Stephanie Rivera](https://www.youtube.com/@stephanieamrivera): My go-to for Databricks training videos.

 

I had a lot of trouble gathering resources to use. Hope this helps!

&#x200B;",232,37,Background_Debate_94,2023-08-25 03:44:08,https://www.reddit.com/r/dataengineering/comments/160nyxw/just_got_certified_databricks_certified_data/,False,False,False,False
xyxpku,"Built and automated a complete end-to-end ELT pipeline using AWS, Airflow, dbt, Terraform, Metabase and more as a beginner project!","GitHub repository: [https://github.com/ris-tlp/audiophile-e2e-pipeline](https://github.com/ris-tlp/audiophile-e2e-pipeline)

Pipeline that extracts data from [Crinacle's](https://crinacle.com/) Headphone and InEarMonitor rankings and prepares data for a Metabase Dashboard. While the dataset isn't incredibly complex or large, the project's main motivation was to get used to the different tools and processes that a DE might use.

## Architecture

https://preview.redd.it/4nl5gasv4ms91.jpg?width=1858&format=pjpg&auto=webp&s=f0766ad2d58e19689474acc5a51ef35c9388284d

Infrastructure provisioning through [Terraform](https://www.terraform.io/), containerized through [Docker](https://www.docker.com/) and orchestrated through [Airflow](https://airflow.apache.org/). Created dashboard through [Metabase](https://www.metabase.com/).

DAG Tasks:

1. Scrape data from [Crinacle's](https://crinacle.com/) website to generate bronze data.
2. Load bronze data to [AWS S3](https://aws.amazon.com/s3/).
3. Initial data parsing and validation through [Pydantic](https://github.com/pydantic/pydantic) to generate silver data.
4. Load silver data to [AWS S3](https://aws.amazon.com/s3/).
5. Load silver data to [AWS Redshift](https://aws.amazon.com/redshift/).
6. Load silver data to [AWS RDS](https://aws.amazon.com/rds/) for future projects.
7. and 8. Transform and test data through [dbt](https://docs.getdbt.com/) in the warehouse.

## Dashboard

The dashboard was created on a local Metabase docker container, I haven't hosted it anywhere so I only have a screenshot to share, sorry!

&#x200B;

https://preview.redd.it/9a5zv15y4ms91.jpg?width=2839&format=pjpg&auto=webp&s=8df8ba42d8ab602fb72dc9ffcc2102e6a517e0c5

## Takeaways and improvements

1. I realize how little I know about advance SQL and execution plans. I'll definitely be diving deeper into the topic and taking on some courses to strengthen my foundations there.
2. Instead of running the scraper and validation tasks locally, they could be deployed as a Lambda function so as to not overload the airflow server itself.

Any and all feedback is absolutely welcome! I'm fresh out of university and trying to hone my skills for the DE profession as I'd like to integrate it with my passion of astronomy and hopefully enter the data-driven astronomy in space telescopes area as a data engineer! Please feel free to provide any feedback!",232,70,infiniteAggression-,2022-10-08 16:58:28,https://www.reddit.com/r/dataengineering/comments/xyxpku/built_and_automated_a_complete_endtoend_elt/,False,False,False,False
17vxmth,Microsoft data products - merry-go-round of mediocrity,"Hey r/dataengineering,

For anyone that says this is my fault for specializing in Microsoft stack - you're absolutely, 100% correct. I blame only myself. 

The incessant cycle of ""progress"". I'm reaching my wit's end with how we're handling tech debt. It seems like every other year, there's a new 'bright new day' in the Microsoft analytics stack, and it's driving me nuts.

First off, let's address the myth of avoiding tech debt. Spoiler alert: it's a fairy tale. Every couple of years, MS flips the script, and suddenly, what was cutting-edge is now old news. The execs, bless their hearts, eat up all the marketing spiel and suddenly, last year's innovation is this year's digital paperweight.

It's a merry-go-round of mediocrity So, what do we do? We slap a new 'notebook' GUI over Spark clusters and pat ourselves on the back for 'innovation.' It's a cycle as predictable as it is frustrating. Microsoft partners? Under constant pressure to sell whatever's been rebranded this week, with awards handed out for sales volume, not product quality. 

We've all heard the mantras: ""ADF is the way,"" ""Databricks is the way,"" ""Synapse is the way,"" ""Fabric is the way."" It's just a parade of platforms, each hailed as the messiah of data engineering, but they're not, they're very naughty boys, only to be replaced by the next shiny thing in a year or two.

I (and anyone working with Azure/MS tech) need to get some self-respect and leave the execs, wordcels and 'platnum's to it.",227,119,biowl,2023-11-15 16:40:11,https://www.reddit.com/r/dataengineering/comments/17vxmth/microsoft_data_products_merrygoround_of_mediocrity/,False,False,False,False
sm5bd0,Seems like dbt's the solution to everything,,233,67,finobu,2022-02-06 19:34:33,https://i.redd.it/lvjp5xvul9g81.jpg,False,False,False,False
nntbv1,STAY FAR AWAY FROM UDACITY's DATA ENGINEERING COURSE,"I picked up this course because it was ""on sale""... this is legit the worst course of all time and a complete waste of time and money.. The material is decent, but there are plenty of gaps in instructions, missing data sets and outdated code... For the price, this is just unacceptable and I can't believe I wasted my money. Please stay far far away!",229,57,bjj17,2021-05-29 18:25:24,https://www.reddit.com/r/dataengineering/comments/nntbv1/stay_far_away_from_udacitys_data_engineering/,False,False,False,False
wklueu,So today I learned Data Swamps exist. Anyone ever have to deal with one in a production environment?,,226,64,DrRedmondNYC,2022-08-10 02:33:50,https://i.redd.it/n86id1wgssg91.png,False,False,False,False
153o48v,Fact,,225,10,Sailja_Jain,2023-07-19 07:37:16,https://i.redd.it/glkf1eivkvcb1.jpg,False,False,False,False
1ce0ohq,Why do companies use Snowflake if it is that expensive as people say ? ,Same as title,229,144,Normal-Inspector7866,2024-04-27 00:04:14,https://www.reddit.com/r/dataengineering/comments/1ce0ohq/why_do_companies_use_snowflake_if_it_is_that/,False,False,False,False
1549emd,Is it normal for data engineers to be lacking basic technical skills?,"I've been at my new company for about 4 months.  I have 2 years of CRUD backend experience and I was hired to replace a senior DE (but not as a senior myself) on a data warehouse team.  This engineer managed a few python applications and Spark + API ingestion processes for the DE team.  

I am hired and first tasked to put these codebases in github, setup CI/CD processes, and help upskill the team in development of this side of our data stack.  It turns out the previous dev just did all of his development on production directly with no testing processes or documentation.  Okay, no big deal.  I'm able to get the code into our remote repos, build CI/CD pipeline with Jenkins (with the help of an adjacent devops team), and overall get the codebase updated to a more mature standing.  I've also worked with the devops team to build out docker images for each of the applications we manage so that we can have proper development environments. Now we have visibility, proper practices in place, and it's starting to look like actual engineering.

Now comes the part where everything starts crashing down.  Since we have a more organized development practices, our new manager starts assigning tasks within these platforms to other engineers.  I come to find out that the senior engineer I replaced was the only data engineer who had touched these processes within the last year.  I also learn that none of the other DE's (including 4 senior DE's) have any experience with programming outside of SQL.  

Here's a list of some of the issues I've run into:  
Engineer wants me to give him prod access so he can do his development there instead of locally.

Senior engineers don't know how to navigate a CLI.

Engineers have no idea how to use git, and I am there personal git encyclopedia.

Engineers breaking stuff with a git GUI, requiring me to fix it.

Engineers pushing back on git usage entirely.

Senior engineer with 12 years at the company does not know what a for-loop is.

Complaints about me requiring unit testing and some form of documentation that the code works before pushing to production.

Some engineers simply cannot comprehend how Docker works, and want my help to configure their windows laptop into a development environment (I am not helping you stand up a Postgres instance directly on your Windows OS).

I am at my wits end.  I've essentially been designated as a mentor for the side of the DE house that I work in.  That's fine, but I was not hired as a senior, and it is really demotivating mentoring the people who I thought should be mentoring me.  I really do want to see the team succeed, but there has been so much pushback on following best-practices and learning new skills.  Is this common in the DE field?

&#x200B;",227,155,Techthrowaway2222888,2023-07-19 22:42:45,https://www.reddit.com/r/dataengineering/comments/1549emd/is_it_normal_for_data_engineers_to_be_lacking/,False,False,False,False
z6s0pe,Airflow DAG with 150 tasks dynamically generated from a single module file,,225,100,FactMuncher,2022-11-28 09:24:26,https://i.redd.it/2sy72i7obp2a1.jpg,False,False,False,False
lclm3s,Lecture Notes on Data Engineering Basics - Stanford CS 329S,,227,21,neuromantik8086,2021-02-04 17:58:14,https://twitter.com/chipro/status/1357329955131191298,False,False,False,False
1b6ghh6,"Accepted an offer, 2 weeks later got dream offer from another company","So I accepted an offer with a decent comp at a bank. Role is remote I started and got my work laptop mailed and have been going through on boarding. 

Now I've just gotten an offer from another company which I thought ghosted me and I'm in a bit of a dilemma. The offer is 60% more than my current comp. I'm not even questioning it tbh I am definitely going to accept, I know my current company can't match and of course they won't I literally just started. 

Whats my best course of action? Just tell them about the job? Bullshit something else (like medical issue) and say I can't work anymore?

Edit: while the job is remote they did fly me out for my first week so I can meet the core team so that does add another insult when I leave. ",224,63,bigYman,2024-03-04 17:19:17,https://www.reddit.com/r/dataengineering/comments/1b6ghh6/accepted_an_offer_2_weeks_later_got_dream_offer/,False,False,False,False
12v9d3v,Is it normal to not remember Pandas commands and need to constantly Google them?,"I use Pandas pretty much daily and except from the usual head(), keys(), dtypes etc, I always have to Google things like groupby to remember the syntax. I know how to use them all but does this syndrome disappear as you get more experienced or does everyone Google these things too? SQL commands I remember a lot as it's plain English but Pandas, no.",226,98,miridian19,2023-04-22 15:28:41,https://www.reddit.com/r/dataengineering/comments/12v9d3v/is_it_normal_to_not_remember_pandas_commands_and/,False,False,False,False
1amizw6,Data lovers!,,223,11,growth_man,2024-02-09 07:49:43,https://i.redd.it/oqurvykylihc1.jpeg,False,False,False,False
t5tp9p,Every Freaking Time,,226,14,None,2022-03-03 15:10:47,https://i.redd.it/49jwurusp6l81.jpg,False,False,False,False
12u2542,Step-by-step tutorial: Building a Kimball dimensional model with dbt,"Hey everyone! I am thrilled to announce that as part of the [dbt technical writing mentorship program](https://www.getdbt.com/blog/technical-writing-mentorship-program/), I have just published a brand new developer blog article for all my fellow data enthusiasts out there! In this tutorial, I provide a step-by-step guide on how to build a Kimball dimensional model with dbt. 

* Blog article: [https://docs.getdbt.com/blog/kimball-dimensional-model](https://docs.getdbt.com/blog/kimball-dimensional-model) 
* Repository: [https://github.com/Data-Engineer-Camp/dbt-dimensional-modelling](https://github.com/Data-Engineer-Camp/dbt-dimensional-modelling) 

I had trouble finding clear explanations on this topic myself, which is why I decided to write one and share my knowledge with the community. Check out my latest article and let me know what you think!",220,18,j__neo,2023-04-21 13:41:59,https://www.reddit.com/r/dataengineering/comments/12u2542/stepbystep_tutorial_building_a_kimball/,False,False,False,False
11nqc61,Tencent Data Engineer: Why We Went from ClickHouse to Apache Doris?,"This article is co-written by me and my colleague Kai Dai. We are both data platform engineers at Tencent Music (NYSE: TME), a music streaming service provider with a whopping 800 million monthly active users. To drop the number here is not to brag but to give a hint of the sea of data that my poor coworkers and I have to deal with everyday.

# What We Use ClickHouse For?

The music library of Tencent Music contains data of all forms and types: recorded music, live music, audios, videos, etc. As data platform engineers, our job is to distill information from the data, based on which our teammates can make better decisions to support our users and musical partners.

Specifically, we do all-round analysis of the songs, lyrics, melodies, albums, and artists, turn all this information into data assets, and pass them to our internal data users for inventory counting, user profiling, metrics analysis, and group targeting.

https://preview.redd.it/y36uy7do4xma1.png?width=1280&format=png&auto=webp&s=5690569dac32b9206e14f187d31d9de0fc4ccdf0

We stored and processed most of our data in Tencent Data Warehouse (TDW), an offline data platform where we put the data into various tag and metric systems and then created flat tables centering each object (songs, artists, etc.).

Then we imported the flat tables into ClickHouse for analysis and Elasticsearch for data searching and group targeting.

After that, our data analysts used the data under the tags and metrics they needed to form datasets for different usage scenarios, during which they could create their own tags and metrics.

The data processing pipeline looked like this:

https://preview.redd.it/18em4jjr4xma1.png?width=1280&format=png&auto=webp&s=00bcc93010957518038f892c185d7bd7803b7d94

# Why ClickHouse is Not a Good Fit

When working with the above pipeline, we encountered a few difficulties:

1. **Partial Update**: Partial update of columns was not supported. Therefore, any latency from any one of the data sources could delay the creation of flat tables, and thus undermine data timeliness.
2. **High storage cost**: Data under different tags and metrics was updated at different frequencies. As much as ClickHouse excelled in dealing with flat tables, it was a huge waste of storage resources to just pour all data into a flat table and partition it by day, not to mention the maintenance cost coming with it.
3. **High maintenance cost**: Architecturally speaking, ClickHouse was characterized by the strong coupling of storage nodes and compute nodes. Its components were heavily interdependent, adding to the risks of cluster instability. Plus, for federated queries across ClickHouse and Elasticsearch, we had to take care of a huge amount of connection issues. That was just tedious.

# Transition to Apache Doris

[Apache Doris](https://github.com/apache/doris), a real-time analytical database, boasts a few features that are exactly what we needed in solving our problems:

1. **Partial update**: Doris supports a wide variety of data models, among which the Aggregate Model supports real-time partial update of columns. Building on this, we can directly ingest raw data into Doris and create flat tables there. The ingestion goes like this: Firstly, we use Spark to load data into Kafka; then, any incremental data will be updated to Doris and Elasticsearch via Flink. Meanwhile, Flink will pre-aggregate the data so as to release burden on Doris and Elasticsearch.
2. **Storage cost**: Doris supports multi-table join queries and federated queries across Hive, Iceberg, Hudi, MySQL, and Elasticsearch. This allows us to split the large flat tables into smaller ones and partition them by update frequency. The benefits of doing so include a relief of storage burden and an increase of query throughput.
3. **Maintenance cost**: Doris is of simple architecture and is compatible with MySQL protocol. Deploying Doris only involves two processes (FE and BE) with no dependency on other systems, making it easy to operate and maintain. Also, Doris supports querying external ES data tables. It can easily interface with the metadata in ES and automatically map the table schema from ES so we can conduct queries on Elasticsearch data via Doris without grappling with complex connections.

What’s more, Doris supports multiple data ingestion methods, including batch import from remote storage such as HDFS and S3, data reads from MySQL binlog and Kafka, and real-time data synchronization or batch import from MySQL, Oracle, and PostgreSQL. It ensures service availability and data reliability through a consistency protocol and is capable of auto debugging. This is great news for our operators and maintainers.

Statistically speaking, these features have cut our storage cost by 42% and development cost by 40%.

During our usage of Doris, we have received lots of support from the open source Apache Doris community and timely help from the SelectDB team, which is now running a commercial version of Apache Doris.

https://preview.redd.it/4epkzulg5xma1.png?width=1280&format=png&auto=webp&s=2247711ec449b0af555a1132033a3a7528f9519c

# Further Improvement to Serve Our NeedsIntroduce a Semantic Layer

Speaking of the datasets, on the bright side, our data analysts are given the liberty of redefining and combining the tags and metrics at their convenience. But on the dark side, high heterogeneity of the tag and metric systems leads to more difficulty in their usage and management.

Our solution is to introduce a semantic layer in our data processing pipeline. The semantic layer is where all the technical terms are translated into more comprehensible concepts for our internal data users. In other words, we are turning the tags and metrics into first-class citizens for data definement and management.

https://preview.redd.it/7yxzag2k5xma1.png?width=1280&format=png&auto=webp&s=f345a13e729a35cafcb0e7d432e87b8b163f82df

**Why would this help?**

For data analysts, all tags and metrics will be created and shared at the semantic layer so there will be less confusion and higher efficiency.

For data users, they no longer need to create their own datasets or figure out which one is applicable for each scenario but can simply conduct queries on their specified tagset and metricset.

# Upgrade the Semantic Layer

Explicitly defining the tags and metrics at the semantic layer was not enough. In order to build a standardized data processing system, our next goal was to ensure consistent definition of tags and metrics throughout the whole data processing pipeline.

For this sake, we made the semantic layer the heart of our data management system:

https://preview.redd.it/yitj349p5xma1.png?width=1280&format=png&auto=webp&s=f8077b99a71f0718becb2dd7497aa719e5cc90a3

**How does it work?**

All computing logics in TDW will be defined at the semantic layer in the form of a single tag or metric.

The semantic layer receives logic queries from the application side, selects an engine accordingly, and generates SQL. Then it sends the SQL command to TDW for execution. Meanwhile, it might also send configuration and data ingestion tasks to Doris and decide which metrics and tags should be accelerated.

In this way, we have made the tags and metrics more manageable. A fly in the ointment is that since each tag and metric is individually defined, we are struggling with automating the generation of a valid SQL statement for the queries. If you have any idea about this, you are more than welcome to talk to us.

# Give Full Play to Apache Doris

As you can see, Apache Doris has played a pivotal role in our solution. Optimizing the usage of Doris can largely improve our overall data processing efficiency. So in this part, we are going to share with you what we do with Doris to accelerate data ingestion and queries and reduce costs.

**What We Want?**

https://preview.redd.it/1s4n2nls5xma1.png?width=1280&format=png&auto=webp&s=0be5c21e347b2fec7eabb475bdab86444c2efb8d

Currently, we have 800+ tags and 1300+ metrics derived from the 80+ source tables in TDW.

When importing data from TDW to Doris, we hope to achieve:

* **Real-time availability:** In addition to the traditional T+1 offline data ingestion, we require real-time tagging.
* **Partial update**: Each source table generates data through its own ETL task at various paces and involves only part of the tags and metrics, so we require the support for partial update of columns.
* **High performance**: We need a response time of only a few seconds in group targeting, analysis and reporting scenarios.
* **Low costs**: We hope to reduce costs as much as possible.

**What We Do?**

1. **Generate Flat Tables in Flink Instead of TDW**

https://preview.redd.it/of8zcyyu5xma1.png?width=1280&format=png&auto=webp&s=1bcc31f236b2373385a2e2954445cd13568987d3

Generating flat tables in TDW has a few downsides:

* **High storage cost**: TDW has to maintain an extra flat table apart from the discrete 80+ source tables. That’s huge redundancy.
* **Low real-timeliness**: Any delay in the source tables will be augmented and retard the whole data link.
* **High development cost**: To achieve real-timeliness would require extra development efforts and resources.

On the contrary, generating flat tables in Doris is much easier and less expensive. The process is as follows:

* Use Spark to import new data into Kafka in an offline manner.
* Use Flink to consume Kafka data.
* Create a flat table via the primary key ID.
* Import the flat table into Doris.

As is shown below, Flink has aggregated the five lines of data, of which “ID”=1, into one line in Doris, reducing the data writing pressure on Doris.

https://preview.redd.it/qk6mr24x5xma1.png?width=1280&format=png&auto=webp&s=56fd72f14962af9bae20d74e06e947d84efd0658

This can largely reduce storage costs since TDW no long has to maintain two copies of data and KafKa only needs to store the new data pending for ingestion. What’s more, we can add whatever ETL logic we want into Flink and reuse lots of development logic for offline and real-time data ingestion.

**2. Name the Columns Smartly**

As we mentioned, the Aggregate Model of Doris allows partial update of columns. Here we provide a simple introduction to other data models in Doris for your reference:

**Unique Model**: This is applicable for scenarios requiring primary key uniqueness. It only keeps the latest data of the same primary key ID. (As far as we know, the Apache Doris community is planning to include partial update of columns in the Unique Model, too.)

**Duplicate Model**: This model stores all original data exactly as it is without any pre-aggregation or deduplication.

After determining the data model, we had to think about how to name the columns. Using the tags or metrics as column names was not a choice because:

I. Our internal data users might need to rename the metrics or tags, but Doris 1.1.3 does not support modification of column names.

II. Tags might be taken online and offline frequently. If that involves the adding and dropping of columns, it will be not only time-consuming but also detrimental to query performance.

Instead, we do the following:

* **For flexible renaming of tags and metrics**, we use MySQL tables to store the metadata (name, globally unique ID, status, etc.). Any change to the names will only happen in the metadata but will not affect the table schema in Doris. For example, if a `song_name` is given an ID of 4, it will be stored with the column name of a4 in Doris. Then if the `song_name` is involved in a query, it will be converted to a4 in SQL.
* **For the onlining and offlining of tags**, we sort out the tags based on how frequently they are being used. The least used ones will be given an offline mark in their metadata. No new data will be put under the offline tags but the existing data under those tags will still be available.
* **For real-time availability of newly added tags and metrics**, we prebuild a few ID columns in Doris tables based on the mapping of name IDs. These reserved ID columns will be allocated to the newly added tags and metrics. Thus, we can avoid table schema change and the consequent overheads. Our experience shows that only 10 minutes after the tags and metrics are added, the data under them can be available.

Noteworthily, the recently released Doris 1.2.0 supports Light Schema Change, which means that to add or remove columns, you only need to modify the metadata in FE. Also, you can rename the columns in data tables as long as you have enabled Light Schema Change for the tables. This is a big trouble saver for us.

**3. Optimize Date Writing**

Here are a few practices that have reduced our daily offline data ingestion time by 75% and our CUMU compaction score from 600+ to 100.

* Flink pre-aggregation: as is mentioned above.
* Auto-sizing of writing batch: To reduce Flink resource usage, we enable the data in one Kafka Topic to be written into various Doris tables and realize the automatic alteration of batch size based on the data amount.
* Optimization of Doris data writing: fine-tune the the sizes of tablets and buckets as well as the compaction parameters for each scenario:

`max_XXXX_compaction_thread`  
`max_cumulative_compaction_num_singleton_deltas`

* Optimization of the BE commit logic: conduct regular caching of BE lists, commit them to the BE nodes batch by batch, and use finer load balancing granularity.

https://preview.redd.it/q5dqk7b76xma1.png?width=1280&format=png&auto=webp&s=f8e491d403cb52cb92a0a99802ecccb5633def23

**4. Use Dori-on-ES in Queries**

About 60% of our data queries involve group targeting. Group targeting is to find our target data by using a set of tags as filters. It poses a few requirements for our data processing architecture:

* Group targeting related to APP users can involve very complicated logic. That means the system must support hundreds of tags as filters simultaneously.
* Most group targeting scenarios only require the latest tag data. However, metric queries need to support historical data.
* Data users might need to perform further aggregated analysis of metric data after group targeting.
* Data users might also need to perform detailed queries on tags and metrics after group targeting.

After consideration, we decided to adopt Doris-on-ES. Doris is where we store the metric data for each scenario as a partition table, while Elasticsearch stores all tag data. The Doris-on-ES solution combines the distributed query planning capability of Doris and the full-text search capability of Elasticsearch. The query pattern is as follows:

`SELECT tag, agg(metric)`   
 `FROM Doris`   
 `WHERE id in (select id from Es where tagFilter)`  
 `GROUP BY tag`

As is shown, the ID data located in Elasticsearch will be used in the sub-query in Doris for metric analysis.

In practice, we find that the query response time is related to the size of the target group. If the target group contains over one million objects, the query will take up to 60 seconds. If it is even larger, a timeout error might occur.

After investigation, we identified our two biggest time wasters:

I. When Doris BE pulls data from Elasticsearch (1024 lines at a time by default), for a target group of over one million objects, the network I/O overhead can be huge.

II. After the data pulling, Doris BE needs to conduct Join operations with local metric tables via SHUFFLE/BROADCAST, which can cost a lot.

https://preview.redd.it/yhfjj9uh6xma1.png?width=1280&format=png&auto=webp&s=10d6f5fadd5c6818600fb3a39191bbd75292981d

Thus, we make the following optimizations:

* Add a query session variable `es_optimize` that specifies whether to enable optimization.
* In data writing into ES, add a BK column to store the bucket number after the primary key ID is hashed. The algorithm is the same as the bucketing algorithm in Doris (CRC32).
* Use Doris BE to generate a Bucket Join execution plan, dispatch the bucket number to BE ScanNode and push it down to ES.
* Use ES to compress the queried data; turn multiple data fetch into one and reduce network I/O overhead.
* Make sure that Doris BE only pulls the data of buckets related to the local metric tables and conducts local Join operations directly to avoid data shuffling between Doris BEs.

https://preview.redd.it/or4nmhsk6xma1.png?width=1280&format=png&auto=webp&s=d615401e35e42dd29feff840483a825865dc2f02

As a result, we reduce the query response time for large group targeting from 60 seconds to a surprising 3.7 seconds.

Community information shows that Doris is going to support inverted indexing since version 2.0.0, which is soon to be released. With this new version, we will be able to conduct full-text search on text types, equivalence or range filtering of texts, numbers, and datetime, and conveniently combine AND, OR, NOT logic in filtering since the inverted indexing supports array types. This new feature of Doris is expected to deliver 3\~5 times better performance than Elasticsearch on the same task.

**5. Refine the Management of Data**

Doris’ capability of cold and hot data separation provides the foundation of our cost reduction strategies in data processing.

* Based on the TTL mechanism of Doris, we only store data of the current year in Doris and put the historical data before that in TDW for lower storage cost.
* We vary the numbers of copies for different data partitions. For example, we set three copies for data of the recent three months, which is used frequently, one copy for data older than six months, and two copies for data in between.
* Doris supports turning hot data into cold data so we only store data of the past seven days in SSD and transfer data older than that to HDD for less expensive storage.

# Conclusion

Thank you for scrolling all the way down here and finishing this long read. We’ve shared our cheers and tears, lessons learned, and a few practices that might be of some value to you during our transition from ClickHouse to Doris. We really appreciate the help from the Apache Doris community and the SelectDB team, but we might still be chasing them around for a while since we attempt to realize auto-identification of cold and hot data, pre-computation of frequently used tags/metrics, simplification of code logic using Materialized Views, and so on and so forth.",221,27,ApacheDoris,2023-03-10 14:14:15,https://www.reddit.com/r/dataengineering/comments/11nqc61/tencent_data_engineer_why_we_went_from_clickhouse/,False,False,False,False
xzw46d,How I got a Data Engineering position in less than two months of applying,"I am seeing quite a bit of post on here with people asking for advice on their resumes or expressing their difficulties in landing a Data Engineering position or even getting an interview at all. 

I want to share some tips that helped me as someone who is also fairly new to the Data Engineering world. My previous work experience ranged from doing basic SQL/Excel stuff for the first two years of my career and then advancing into more of a Database Developer position for another two years. Once of the things that was discouraging me to most is that I had a pretty bad 3 year gap in employment for anything tech related, I was laid off in 2019 and decided to finish a degree I was working on in Data Science which seemed to be the hottest job title back in 2018-2019. Not saying it isn't hot anymore but it seems like Data Engineering is an even hotter and in demand title at the moment. I got caught up with collecting unemployment , working odd jobs off the books, door dashing etc for a few years but this past summer I decided it was time to get back in the tech world before it was too late.

So in July I made it my mission to get a job by the end of the summer so I could start when my two kids went back to school.

First off the only platform I used to apply for jobs was LinkedIN. It seemed like the most professional and provided the most information about the types of jobs I was applying for. I never used LinkedIn Premium before but because there was a 1 month free trial I went and enabled that too which then started to give you better insights on which jobs you would be a top candidate for.

Here are the main tips I have for navigating the application process on LinkedIN :

1) Get your easy apply set up so you can one click apply or click your way through the simple questions they have on the job postings 

2) Skill Assessments : This is a big one. Anytime I applied for a job on LinkedIN it would offer for me to take a skill assessment test where you would receive a badge if you scored over a certain percentage. Many of them also came with videos and mini courses to take to help you pass them. Any skill assessment I felt I had a shot at passing I would take, and even if you fail the first time it lets you retry it twice I believe and you get an idea of how the questions are. So I took every single one I could (SQL, Excel, Python , Azure, PowerBI, R, a few more I can't remember off the top of my head. The only ones I didn't take were the ones I had no chance of passing because I had no prior experience with such as Java Development, JavaScript libraries I've never used, C++. But because all the jobs I was applying for were data related I didn't encounter those to much.

I know for a fact this helped considerably because when I would receive responses from the recruiters it would show a copy of what my application looked like on their end and it would say so and so has 3/3 of the required skills. So they know you aren't just making up stuff on your resume you are showing them that you know at least the basics.

3) Become familiar with a cloud platform. This was the biggest change I have seen since 2019 is the massive shift towards cloud based platforms, software as a service and all that good stuff. I went with Azure just because my previous experience was mostly in Microsoft SQL Server. Azure offers a 1 month free trial and with it you get $200 credit. I signed up for this and began using a bunch of different tools related to data engineering, mostly Azure Data Factory but I also spinned up an Azure SQL Server Instance, an Azure Cosmos Database which is free for a year and a Linux VM. Made some basic pipelines in Data Factory to ETL data from one source to another and learned quite a bit about using the command line interfaces PowerShell and Bash. When people asked me about this stuff in interviews I was able to answer basic questions about it and it showed them I was interested in learning new things on my own.

4) Check the number of applicants for the LinkedIN Jobs. Alot of people only spam the easy apply button, which of course I did too. But for many of the jobs that don't offer this option and require a full application on their HR platform I noticed the number of applicants were much lower. The job I currently have now was one of the ones I couldn't easy apply for and I saw it only had a 20 or so applicants as opposed to the hundreds you would see for the easy apply ones. Don't skip over these jobs just because the application process will take a few more minutes especially if it's one you really want.

5) Apply for any job you feel you can do. The job says 5 years of experience required but you only have 3 ? Apply anyway. Shoot as high as you can. Those are the ideal candidate requirements they are looking for but they know they aren't always going to find someone with that much experience. Don't get discouraged if you don't meet the minimum requirements. This is a numbers game and you want to apply for as many jobs as you possibly can especially since things have gone remote for a large part. 

6) During the interviews if you don't know something just be honest with them. Don't try to BS your way around it because this will only make you look worse if they find out you are being dishonest. Showing that you are willing to learn new things looks alot better than getting caught in a lie. 

7) Ask for criticism on your resume. This sub reddit seems to be a great source for that as well as the SQL one, Data Science, or just programming subs in general. Ask someone to do a mock interview too if you can.

8) Most important one - Don't give up! There are so many jobs available out there. Don't get discouraged. Keep learning new things. Learn from your previous mistakes. You will get one eventually. 

I started applying for jobs in mid July and I got so many offers for interviews that I had to start rejecting them. Out of the 10 interviews I did , 8 of them made it to the second round, 5 to the third. The one I ended up getting involved 5 separate interviews actually. And the day I started that job I got an offer for one of the other ones I had applied for that actually paid slightly more but it was a contract/hourly job for 9 months and it didn't seem as interesting as the one I already started so I had to turn it down. A few of the other ones I made it to the third round either went with a different candidate or just ghosted me which seems very common these days. Don't take it to you personally if this happens to you some people just don't have the common courtesy to get back to you.

Anyway I hope this is helpful or encouraging to anyone who reads it and if you have any questions for me please feel free to ask.",222,53,DrRedmondNYC,2022-10-09 20:57:21,https://www.reddit.com/r/dataengineering/comments/xzw46d/how_i_got_a_data_engineering_position_in_less/,False,False,False,False
t3mxlh,Apache Airflow for Beginners Tutorial Series,"Hey there,

I have been using Airflow for a couple of years in my work. I think it is a great tool for data pipeline or ETL management. Therefore, I have created this tutorial series to help folks like you want to learn Apache Airflow. So far, there are 12 episodes uploaded, and more will come.

If you are interested, you can watch the whole playlist on [YouTube](https://www.youtube.com/watch?v=z7xyNOF8tak&list=PLwFJcsJ61oujAqYpMp1kdUBcPG0sE0QMT). If you think it is helpful, consider subscribing to my [youtube channel](https://www.youtube.com/c/coder2j) and star my [GitHub repository](https://github.com/coder2j/airflow-docker). Comment what topics you want to see or discuss about Airflow in the next episode.

Latest episode: [Airflow Hooks S3 PostgreSQL](https://www.youtube.com/watch?v=rcG4WNwi900&list=PLwFJcsJ61oujAqYpMp1kdUBcPG0sE0QMT&index=13)

Updated Tutorial Episode 16.05.2022

1. [Introduction and Local Installation](https://youtu.be/z7xyNOF8tak)
2. [Get Airflow running in Docker](https://youtu.be/J6azvFhndLg)
3. [Airflow Core Concepts in 5 mins](https://youtu.be/mtJHMdoi_Gg)
4. [Airflow Task Lifecycle and Basic Architecture](https://youtu.be/UFsCvWjQT4w)
5. [Airflow DAG with BashOperator](https://youtu.be/CLkzXrjrFKg)
6. [Airflow DAG with PythonOperator and XComs](https://youtu.be/IumQX-mm20Y)
7. [Airflow TaskFlow API](https://youtu.be/9y0mqWsok_4)
8. [Airflow Catchup and Backfill](https://youtu.be/OXOiUeHOQ-0)
9. [Schedule Airflow DAG with Cron Expression](https://youtu.be/tpuovQFUByk)
10. [Airflow Connection and PostgresOperator](https://youtu.be/S1eapG6gjLU)
11. [Add Python Dependencies via Airflow Docker Image Extending and Customizing](https://youtu.be/0UepvC9X4HY)
12. [AWS S3 Key Sensor Operator](https://youtu.be/vuxrhipJMCk)
13. [Airflow Hooks S3 PostgreSQL](https://www.youtube.com/watch?v=rcG4WNwi900&list=PLwFJcsJ61oujAqYpMp1kdUBcPG0sE0QMT&index=13)",223,42,Coder2j,2022-02-28 18:57:27,https://www.reddit.com/r/dataengineering/comments/t3mxlh/apache_airflow_for_beginners_tutorial_series/,False,False,False,False
ow7od0,Any interest in DE interview questions & experience material ?,"In the past 2 months I have given around 10 DE interviews(With startups, small and large corporates) , which gave me a fair idea of what one can expect in a DE interview.

I have saved most of the questions, which I can share in a blog, if of course there is a demand for the same.

Let me know if it would help the members on this subreddit.

Suggestion : use !remindme 7 days to be reminded in a week. I'll compile the data by then and will share it here.

Here you go :
 https://www.linkedin.com/posts/niteshx2_bigdata-dataengineer-interview-activity-6834361837778198528-AxF2",220,217,GreekYogurtt,2021-08-02 04:47:44,https://www.reddit.com/r/dataengineering/comments/ow7od0/any_interest_in_de_interview_questions_experience/,False,False,False,False
18ja9hq,How I interview data engineers,"Hi everybody,

This is a bit of a self-promotion, and I don't usually do that (I have never done it here), but I figured many of you may find it helpful.

For context, I am a Head of data (& analytics) engineering at a Fintech company and have interviewed hundreds of candidates.

What I have outlined in my blog post would, obviously, not apply to every interview you may have, but I believe there are many things people don't usually discuss.

Please go wild with any questions you may have.

https://open.substack.com/pub/datagibberish/p/how-i-interview-data-engineers?r=odlo3&utm_campaign=post&utm_medium=web&showWelcome=true",222,81,ivanovyordan,2023-12-15 21:05:34,https://www.reddit.com/r/dataengineering/comments/18ja9hq/how_i_interview_data_engineers/,False,False,False,False
sexcgm,"I've had 7 interviews this week alone and more due next week, here is what I learned","Hi all,

Often see 'how do I get into the field' posts, and whilst they're no doubt useful to some, I seldom see interview advice, learnings etc. Perhaps they just don't appear in my feed, but thought it might be useful to talk about my experiences in broad terms. 

Worth mentioning that I'm a senior DE, GCP certified among a few other certs useful in this space and I'm going for other senior roles which use a broader tech stack and can help me develop. 


Learning 1: 

no one knows what they are looking for.

Why do I say this? Well, it seems as though each company has its own definition of what a data engineer does. It could be that in some companies a DE role involves only analytical engineering, whilst in others its pipeline management only and in others its a hybrid dev ops, pipeline and analytics engineer. 

I consider myself to have most of the relevant skills in this space but  the conversations I've had with hiring managers (often SEM/ HOD level) have been so widely varied, that it's worth familiarising yourself with the concepts of dev ops/ infra management/ analytics. 

One company stated in their job spec that they were happy for someone to have an understanding of Kafka and would be trained on the job, whilst in fact wanted a streaming expert. So whilst I had already recognised this an area to develop for myself, I would say that you should be more than familiar with streaming concepts (types of windows, exactly once vs at most once etc etc) if streaming is in the job spec. 

Learning 2: 

Have some code ready to discuss with your interviewer. 

My recruiter got in touch with a position last week and followed it up a few times. I've had other recruiters do this amazing thing called prep and have had them run through a list of things we'd be doing in the interview. I asked this particular recruiter about this and they replied but just the day before, and told me that they wanted me to go through some code with the hiring manager. Lucky I had something I could share, but I would suggest you have a personal project ready just in case. 

Learning 3: 

People are using AWS more than any other cloud. 

Not a problem, just an observation. 

Learning 4: 

Some hiring managers are just there to feel good about their 25 years experience and shit all over you. 

It's worth being ballsy with these people and start asking them technical questions in return. They may have a solid understanding of architecture but they won't know it all. Just because you don't know the answer to their question doesn't invalidate you. I had to ask a hiring manager WHY the problem they were asking was even designed that way. Of course you're there to evidence your skills but make sure you challenge, even if its just for you. 


Learning 5: 

Most of the directors I spoke to are fucking clueless. 

Learning 6: 

Make sure you brush up on your basics. 

I've not interviewed for a while and my mind went blank when I was asked about functional programming. It's one of those things one might read over in a document or whatever but commit that shit to memory. Other basic questions I identified as the 'basics' was 

How do you define structured / unstructured data?

What makes a database relational? 

What is OOP? 

What is setverless? 

What is distributed processing? - this came in various forms. 

What is insertfiletype? When would you use this file type? 

How do you describe denormalised data? 


These are questions that at first, I found profoundly tricky answering because I didn't have any nice quick answer for. But after a quick Google for some consolidation and revision , I was able to better summarise. I could certainly tell you about parquet, but is it useful to know that it was created by Apache? Probably not. 


Learning 6: 

ALWAYS make sure the company you're going to will support your personal development beyond the scope of your role. 

If they aren't prepared to do this, they are too corporate and bureaucratic and often will never flex for you. As a data engineer, you're in demand and can call the shots. 

And finally, out of the 7 I've been on I think 5 went well. I've been invited back for 2 second rounds already. 4 of these interviews were today alone andi think all went really well, so we shall see. I'm not in any rush to leave the company im at, I just wanted to see if I could fly.




Edit: thanks all for the engagement. Just wanted to mention I'm in the UK. I'm sure much of my experience can be transferred to any location but just thought I'd mention as the processes might vary.",216,45,None,2022-01-28 18:32:42,https://www.reddit.com/r/dataengineering/comments/sexcgm/ive_had_7_interviews_this_week_alone_and_more_due/,False,False,False,False
12s61tg,Forreal though,,219,54,BoiElroy,2023-04-19 18:45:38,https://i.redd.it/485lt7l8hxua1.jpg,False,False,False,False
1041369,Seems like astronomer quietly laid off 20%,,215,58,mistanervous,2023-01-05 15:03:32,https://i.redd.it/8cdbcg7t6aaa1.jpg,False,False,False,False
u9o08x,For the love of god please use consistent formatting and descriptive aliases in your SQL,"People need to be able to read your queries and understand them. This includes your future self, a.k.a. the poor sap who will be stuck trying to decipher your caffeine addled hieroglyphics at 2am when a critical pipeline breaks.

This isn't beat poetry. Use a standardized naming scheme and formatting style, and be descriptive. I can't believe how many people I've encountered in this field who don't follow this advice.",216,101,LuthienByNight,2022-04-22 20:32:35,https://www.reddit.com/r/dataengineering/comments/u9o08x/for_the_love_of_god_please_use_consistent/,False,False,False,False
zqqsqx,2022 data buzzwords translated to their actual meaning,"ELT: “shift your cost center to your warehouse”  


Modern Data Stack - “shift your cost center to your warehouse”  


Zero ETL:  “shift your cost center to your warehouse \*now with more lock in!\*”  


Credits:  “shift your costs to….variable”  


No code: “shift to needing two tools for the same job”  


Low code: “shift to coding normally”  


Batch:  “Business model for NYSE:SNOW”  


Real-time: “somewhere between nano seconds and hours”  


Data quality: “the thing we keep talking about and would like to get to someday”  


Streaming SQL: “Vendor-specific mashups of various strategies for bolting notions of time variance into a language not designed for it”  


Schemaless: “there is a schema, but we don’t know what it is”  


Bonus alternative ELT definition: ""we changed our schema and broke the data pipeline, but we can make the analysts deal with it""  


What others are we missing?  


Great thread of comments on this prompt as well: [https://www.linkedin.com/feed/update/urn:li:activity:7009593010644557825/](https://www.linkedin.com/feed/update/urn:li:activity:7009593010644557825/)",212,35,MooJerseyCreamery,2022-12-20 15:06:55,https://www.reddit.com/r/dataengineering/comments/zqqsqx/2022_data_buzzwords_translated_to_their_actual/,False,False,False,False
ilr6er,Modern Data Engineer Roadmap 2020,"Hey everyone — In the last couple of weeks I've put a lot of effort into creating a high quality, comprehensive roadmap for data engineers. Hope you'll find it useful.

Here is the Github repo with the roadmap: [https://github.com/datastacktv/data-engineer-roadmap](https://github.com/datastacktv/data-engineer-roadmap)

Let me know what you think!",214,63,alexandraabbas,2020-09-03 10:52:50,https://www.reddit.com/r/dataengineering/comments/ilr6er/modern_data_engineer_roadmap_2020/,False,False,False,False
u1gkua,Building a Data Engineering Project in 20 Minutes,"I created a [fully open-source project](https://www.sspaeti.com/blog/data-engineering-project-in-twenty-minutes/) with tons of tools where you'd learn web-scraping with real-estates, uploading them to S3, Spark and Delta Lake, adding Data Science with Jupyter, and ingesting into Druid, visualising with Superset and managing everything with Dagster.

I want to build another one for my personal finance with tools such as Airbyte, dbt, and DuckDB. Is there any other recommendation you'd include in such a project? Or just any open-source tools you'd want to include? I was thinking of adding a metrics layer with [MetricFlow](https://transform.co/metricflow/) as well. Any recommendations or favourites are most welcome.",213,22,sspaeti,2022-04-11 20:22:31,https://www.reddit.com/r/dataengineering/comments/u1gkua/building_a_data_engineering_project_in_20_minutes/,False,False,False,False
13hebz5,DE's when a new job uses a different cloud platform,,214,22,notGaruda1,2023-05-14 14:55:49,https://i.redd.it/szesjdra9tza1.png,False,False,False,False
y2mdxi,Thanks to everyone here that contributes,"I would say I am still in the early years of being a data engineer and there is so much to learn. But lurking in this subreddit has helped me learn new tools, techniques, best practices, and so much more. So thanks to everyone that contributes in this subreddit. To those that give helpful advice to someone that may have asked the same question 100x. This subreddit has been like a free mentoring program at times!",213,12,darthsatoshious,2022-10-13 02:02:27,https://www.reddit.com/r/dataengineering/comments/y2mdxi/thanks_to_everyone_here_that_contributes/,False,False,False,False
199fg7g,Did I get bamboozled into a data engineering job?,"I'm coming up on 1.5 YoE at my job where my title is ""data analyst"". This is my first real job and I got it out of college. Up until today, I assumed that I was a data analyst doing data analysty things and building a career in data analytics. However, since finding out that data engineering is a separate thing, I've started to suspect that I may actually be working in an entry-level data engineering role.

The job description asked for mastery of Tableau and proficiency in Python. Since starting, I've used Python for scripting a fair amount, but have used Tableau EDA a grand total of zero times. They trained me up in Alteryx, an ETL tool, and now my work mainly consists of Alteryx, SQL, and Python.

90% of my work is building automated data pipelines for other teams; they come to us with some process that they're doing manually in Excel and we make it automatic for them. We follow an Agile framework, gather requirements, build and test, deploy and support. Our typical end product is an app that another team uses, not a dashboard.

Am I actually a trainee data engineer? ",209,58,WarCrimeWizard,2024-01-18 02:43:48,https://www.reddit.com/r/dataengineering/comments/199fg7g/did_i_get_bamboozled_into_a_data_engineering_job/,False,False,False,False
svxsep,5 Beginner Data Engineering Exercises - all ready to go!,,207,25,None,2022-02-19 01:17:38,https://github.com/danielbeach/data-engineering-practice,False,False,False,False
1cb2ym3,Bombed a technical,"I bombed a SQL screening. I have 8 YoE. I have done something in SQL every day for the past 8 years and I failed a LC easy.  

It was a super simple join two tables, do some aggregations, get the top 3 and order by. I actually completed the question by doing a COUNT(), SUM() and AVG() and then ordering by AVG() DESC LIMIT 3 but the interviewer was nudging me towards a rank dense and thats when things fell apart. I got frazzled and couldn't think of how to do a window calculation ordering by an aggregation.  

Afterwards I logged into LC and did like 20 window calc problems and scored in the top 10% for each of them on the first try.",206,96,bjogc42069,2024-04-23 12:09:27,https://www.reddit.com/r/dataengineering/comments/1cb2ym3/bombed_a_technical/,False,False,False,False
mfrpw9,How Data Engineering Works,,209,21,None,2021-03-29 15:13:29,https://youtube.com/watch?v=qWru-b6m030&feature=share,False,False,False,False
14ws2ht,PARTITION BY whatever,,208,19,itty-bitty-birdy-tb,2023-07-11 13:53:51,https://i.redd.it/2h35rk2tccbb1.jpg,False,False,False,False
zcwydz,New to Data engineering and wanted to learn more about it and hoping this is the right way to go about it. Done with steps 1 and 2 for this and just wanted to know is GoogleBigQuery or Snowflake free ? and could anyone just explain step 5. Didnt understand what to do there. Thanks,,207,37,JackOfFarts69,2022-12-05 04:34:12,https://i.redd.it/ruvkboq6c04a1.png,False,False,False,False
om3wl5,"Data engineering project, with a live dashboard","Hello fellow Redditors, 

  I've been interviewing engineers for a while. When someone has a side project listed on their resume I think it's pretty cool and try to read through it. But reading through the repo is not always easy and is time-consuming. This is especially true for data pipeline projects, which are not always visual (like a website). 

  With this issue in mind, I wrote an article that shows how to host a dashboard that gets populated with near real-time data. This also covers the basics of project structure, automated formatting, testing, and having a README file to make your code professional.

  The dashboard can be linked to your resume and LinkedIn profile.  I believe this approach can help showcase your expertise to a hiring manager. 

  [https://www.startdataengineering.com/post/data-engineering-project-to-impress-hiring-managers/](https://www.startdataengineering.com/post/data-engineering-project-to-impress-hiring-managers/)

  Hope this helps someone. Any feedback is appreciated.",208,23,joseph_machado,2021-07-17 13:06:45,https://www.reddit.com/r/dataengineering/comments/om3wl5/data_engineering_project_with_a_live_dashboard/,False,False,False,False
xsb7rm,Data Engineering Zoomcamp - free data engineering course comes back!,"We're launching another iteration of Data Engineering Zoomcamp in January 2023

&#x200B;

Join us too and learn about:

* Docker
* Orchestration
* Data lakes
* Data warehousing
* Analytics engineering
* Batch processing 
* Streaming

&#x200B;

More information here: [https://github.com/DataTalksClub/data-engineering-zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)",203,12,stolzen,2022-09-30 20:10:13,https://www.reddit.com/r/dataengineering/comments/xsb7rm/data_engineering_zoomcamp_free_data_engineering/,False,False,False,False
nn3x58,Finding the perfect identifier for entity matching,,204,8,kuwala-io,2021-05-28 18:11:54,https://i.redd.it/e7wh1cbajw171.gif,False,False,False,False
vbd44r,The API I created as an intern surprisingly made it to production!,"I really wanted to share this as I'm so happy with the work I did. 

&#x200B;

A little background, I was a summer intern a few years back at a large and well known company in the US. I was supposed to create an API that services downstream (internal) clients that were currently connecting to the Data Lake directly. We all knew the data lake was migrating to Redshift, but didn't know when it would be live, so they had me creating the API for the current PostgreSQL DB anyways despite my mild protest. In the middle of the internship, the actual migration to Redshift started so they let me start creating a middle layer with AWS. 

&#x200B;

I honestly had no idea what I was doing, but my manager forced me to fully research top to bottom which tools I should use (as they didn't know the answer themselves) and had me justify in writing the why and how behind each tool I selected before I started creating. I ended up using AWS Lambda and some other layers, including third party IDP, to service the 12 or so clients with highly varied requirements. I basically made it so they could interact with Redshift as if there was no middle layer, but incorporated other tools that allowed for easier security management and operational management from a data lake owner perspective. After all the research and back-and-forth to get proper permissions I was finally able to create and successfully test my API on Postman.

&#x200B;

I didn't think what I created was was actually going to be used as a contractor guy who was supposed to be my mentor sandbagged me towards the end and said he knew a better way to do it without so many layers, to which I was all ears on his better idea but he withheld the specifics. I felt like all my work was for naught and chalked it up as a learning experience and moved on. I just met my old boss in person the other day and he told me that they were actually using my architecture in production! Albeit, with some minor changes. It was so awesome to hear that I got it right in the end, it is a much needed confidence boost.",204,30,fish_the_fred,2022-06-13 14:11:10,https://www.reddit.com/r/dataengineering/comments/vbd44r/the_api_i_created_as_an_intern_surprisingly_made/,False,False,False,False
1bvpz99,Impact of DQ on AI,,200,9,de4all,2024-04-04 15:24:32,https://i.redd.it/0kwphnbadhsc1.jpeg,False,False,False,False
18567aq,Me as an ETL engineer watching people build data tables with no regard to what goes in them.,,201,50,claytonjr,2023-11-27 15:58:17,https://imgur.com/9ZJkPvV,False,False,False,False
1531jz7,the devs chose mongo again smh,,199,37,itty-bitty-birdy-tb,2023-07-18 15:28:44,https://i.redd.it/ux9wsli3sqcb1.png,False,False,False,False
1c8thmh,Nobody appreciates when things work ; The curse of the Data Engineer,"Mini rant on that all too familiar feeling we all have. Nobody appreciates when things are running well uninterrupted. They just expect them to run no matter how many problems we've foresaw and dealt with ahead of time to ensure they didn't affect production. Anyways thats probably part of the gig we all chose, so heres a screenshot of the perfect day (that happens 95% of the time) that nobody besides us appreciates 

https://preview.redd.it/vdo4zhtcqnvc1.png?width=2914&format=png&auto=webp&s=47078f90cd0056fe8682941536ee983ddb76e7c9",202,46,CingKan,2024-04-20 16:03:16,https://www.reddit.com/r/dataengineering/comments/1c8thmh/nobody_appreciates_when_things_work_the_curse_of/,False,False,False,False
1beltlg,What is the hardest you have ever seen someone work manually?,"I once worked with a team who was in charge of some sales dashboards. Their process to update them was to have someone individually open the PDF's of every new invoice for the week, enter the dollar figures into an excel sheet, and then update the workbook  datasource with the new static excel file.

I work for a global market leader, we are lapping the #2 company behind us 5 times over. I would estimate that 5-10% of our headcount is allocated to jobs like these.",200,71,bjogc42069,2024-03-14 13:59:19,https://www.reddit.com/r/dataengineering/comments/1beltlg/what_is_the_hardest_you_have_ever_seen_someone/,False,False,False,False
140xtxu,Does the DE community want to join the Reddit protest?,"Don't Let Reddit Kill 3rd Party Apps!

#What's going on?

A recent Reddit policy change threatens to kill many beloved third-party mobile apps, making a great many quality-of-life features not seen in the official mobile app **permanently inaccessible** to users.

On May 31, 2023, Reddit announced they were raising the price to make calls to their API from being free to a level that will kill every third party app on Reddit, from [Apollo](https://www.reddit.com/r/apolloapp/comments/13ws4w3/had_a_call_with_reddit_to_discuss_pricing_bad/) to [Reddit is Fun](https://www.reddit.com/r/redditisfun/comments/13wxepd/rif_dev_here_reddits_api_changes_will_likely_kill/) to [Narwhal](https://www.reddit.com/r/getnarwhal/comments/13wv038/reddit_have_quoted_the_apollo_devs_a_ridiculous/jmdqtyt/) to [BaconReader](https://www.reddit.com/r/baconreader/comments/13wveb2/reddit_api_changes_and_baconreader/).

Even if you're not a mobile user and don't use any of those apps, this is a step toward killing other ways of customizing Reddit, such as Reddit Enhancement Suite or the use of the old.reddit.com desktop interface [](/ajwtf ""and everything that goes with it!"").

This isn't only a problem on the user level: many subreddit moderators depend on tools only available outside the official app to keep their communities on-topic and spam-free.

#What's the plan? 

On June 12th, [many subreddits](https://old.reddit.com/r/ModCoord/comments/1401qw5/incomplete_and_growing_list_of_participating/) will be going dark to protest this policy. Some will return after 48 hours: others will go away *permanently* unless the issue is adequately addressed, since many moderators aren't able to put in the work they do with the poor tools available through the official app. This isn't something any of us do lightly: we do what we do because *we love Reddit*, and we truly believe this change will make it impossible to keep doing what we love.

The two-day blackout isn't the *goal*, and it isn't the end. Should things reach the 14th with no sign of Reddit choosing to fix what they've broken, we'll use the community and buzz we've built between then and now as a tool for further action.

What can *you* do?

1. **Complain.** Message the mods of /r/reddit.com, who are the admins of the site: message /u/reddit: submit a [support request](https://support.reddithelp.com/hc/en-us/requests/new): comment in relevant threads on /r/reddit, such as [this one](https://www.reddit.com/r/reddit/comments/12qwagm/an_update_regarding_reddits_api/.), leave a negative review on their official iOS or Android app- and sign your username in support to this post.

2. **Spread the word.** Rabble-rouse on related subreddits. Meme it up, make it spicy. Bitch about it to your cat. Suggest anyone you know who moderates a subreddit join us at our sister sub at /r/ModCoord.

3. **Boycott *and* spread the word...to Reddit's competition!** Stay off Reddit entirely on June 12th through the 13th- instead, take to your favorite *non*-Reddit platform of choice and make some noise in support!

4. **Don't be a jerk.** As upsetting this may be, threats, profanity and vandalism will be worse than useless in getting people on our side. Please make every effort to be as restrained, polite, reasonable and law-abiding as possible.

**Further reading**

https://www.reddit.com/r/Save3rdPartyApps/comments/13yh0jf/dont_let_reddit_kill_3rd_party_apps/

https://www.reddit.com/r/apolloapp/comments/13ws4w3/had_a_call_with_reddit_to_discuss_pricing_bad/

https://old.reddit.com/r/ModCoord/comments/1401qw5/incomplete_and_growing_list_of_participating/

https://www.reddit.com/r/SubredditDrama/comments/1404hwj/mods_of_rblind_reveal_that_removing_3rd_party/

https://www.reddit.com/r/redditdev/comments/13wsiks/api_update_enterprise_level_tier_for_large_scale/jmolrhn/?context=3

edit: [Open Letter regarding API pricing](https://www.reddit.com/r/ModCoord/comments/13xh1e7/an_open_letter_on_the_state_of_affairs_regarding/)

[View Poll](https://www.reddit.com/poll/140xtxu)",200,51,theporterhaus,2023-06-05 01:33:53,https://www.reddit.com/r/dataengineering/comments/140xtxu/does_the_de_community_want_to_join_the_reddit/,False,False,False,False
11zudt1,If I have to run this data pipeline one more time I'm going to lose my mind,That is all. Thank you,196,63,RandyMoss93,2023-03-23 19:21:05,https://www.reddit.com/r/dataengineering/comments/11zudt1/if_i_have_to_run_this_data_pipeline_one_more_time/,False,False,False,False
yfx3m1,How I landed a $287k offer for entry-level Data Engineer at FAANG+,"A lot of posts and advice seem to focus on getting into DE after college, or transitioning into DE from an outside field. I want to tell my story as an inspiration that; there are other related roles and industries that are easier to break into; as I did. Analyst, Business Intelligence Engineer, and Analytics Engineer are all noble roles that make for easy pivots into Data Engineering with a great deal of overlap in skill set with Data Engineering. 

Here’s my career progression, to show you how I made it from $40k to $287k in 7 years:

Salary: $40k

Title: Account Executive

Out of college, first job in software sales. The company I worked for and product I sold was in the Business Intelligence space, so I wanted to learn everything about it. I quickly became the most technical salesperson in my org, and tried to transition to a more technical solutions engineer role, but was blocked by management. So I left after 18 months. 

Salary: $80k

Title: Account Executive

I worked selling a competing product to the former product, again learning everything I could. I began to learn SQL. After 14 months, I was laid off as the whole division went under. 

Salary: $100k

Title: Sr. Analyst

I went to work in the healthcare space. Because of how technical I had become in my first sales job, I easily qualified for a tool-specific specialist at healthcare company. I was by far the most advanced on my team in this tool. My SQL was my biggest weakness, and I learned fast and studied countless hours outside of work. I left after 3 years due to lack of opportunity for advancement and political messes. 

Salary: $150k+$15k bonus 

Title: Technical Lead

I worked as a contractor for a FAANG company which is a great brand for my resume, but I was just a lowly contractor. I led complex projects writing SQL and a tiny bit of python. I studied python a lot. After 2 years, I tried to convert to FTE and it became obvious that it wouldn’t happen. For the last 5 months, I spent about 5 hours a day studying algorithms and data structures and/or interviewing. After 2.5 years, I resigned. 

Salary: $175k+$25k bonus+$87k stock+$40k sign on bonus

Title: Data Engineer

I received probably 5 offers between about 30 interviews. I got 1 FAANG offer ($225k, L5 BIE) 1 Top N offer ($287k DE) and 1 fortune 100 offer ($250k Sr DE). My advice here is that; I knew algorithms and data structures would be my weakness, so I focused there. With solid SQL, Algos, and behavioral, the rest I would need to figure out as I went. I began making lists of of topics that I was weak in as I went through more and more interviews, and I would get a few inches deep into each topic and/or make study sheets and flash cards as I went. A large element of this was luck. I focused on concepts, not tools. Normal forms, for example, are conceptual and tool-agnostic. There was also a massive luck element to this game. Some companies went for hard algos questions; I failed those interviews. Some interviewers wanted to talk about optimizing a pig job; I failed that interview. Was pig worth learning because one company asked about it? No. I stayed the course of conceptual topics. 

Despite my title as a Technical Lead as a contractor at FAANG, I interviewed for a Sr. role. Because the scope of some of the projects I worked on didn’t impact multiple technical teams, I was down-leveled to entry-level Data Engineer. I declined the offer, and planned to accept another offer. A week later, the recruiter contacted me again and told me things changed on their end and they really wanted me. While they weren’t able to up-level me, they were able to negotiate the salary to the upper-end of the range for entry-level Data Engineer.",199,100,Flat_Shower,2022-10-28 18:41:38,https://www.reddit.com/r/dataengineering/comments/yfx3m1/how_i_landed_a_287k_offer_for_entrylevel_data/,False,False,False,False
16uu03a,Tools that seemed cool at first but you've grown to loathe?,I've grown to hate Alteryx.  It might be fine as a self service / desktop tool but anything enterprise/at scale is a nightmare.  It is a pain to deploy.  It is a pain to orchestrate.  The macro system is a nightmare to use.  Most of the time it is slow as well.  Plus it is extremely expensive to top it all off.,196,265,endless_sea_of_stars,2023-09-28 22:34:28,https://www.reddit.com/r/dataengineering/comments/16uu03a/tools_that_seemed_cool_at_first_but_youve_grown/,False,False,False,False
14rt3ur,Is cloud a big scam?,[DELETED] ` this message was mass deleted/edited with redact.dev `,200,132,None,2023-07-06 00:56:26,https://www.reddit.com/r/dataengineering/comments/14rt3ur/is_cloud_a_big_scam/,False,False,False,False
138cvct,Welcome to JOIN hell,,197,50,tiltaltti,2023-05-05 06:22:35,https://i.redd.it/ho2lxe3nhyxa1.jpg,False,False,False,False
ydg0g9,What do you do when your data pipeline depends on someone else’s pipeline and that upstream pipeline fails?,,198,15,tchungry,2022-10-25 21:16:39,https://v.redd.it/rbl58o15q0w91,False,False,False,False
14ckwyq,Stack Overflow Will Charge AI Giants for Training Data,,198,51,wagfrydue,2023-06-18 13:47:00,https://www.wired.com/story/stack-overflow-will-charge-ai-giants-for-training-data/,False,False,False,False
wsimt4,TIL,,195,11,whb2030,2022-08-19 16:41:47,https://twitter.com/teej_m/status/1489491830580264960?s=20&t=abQFGg2ZXKybyfjYJ3TVdg,False,False,False,False
10ybytx,Thanking my plumber every time,"So after building a near real time pipeline with super complex parsing and data quality in record speed your dashboard gal/guy sends an email to C-level showing her/his awesome dashboard and just cc you in the email.



Nobody would know 

— time spent to find optimal parallelism


— time spent for solving weird dq issues


— time spent to make the pipeline dynamically handle schema change


— time spent for threading it all together in orchestrator


After all these years still hurts on how thankless this job is.

Only lesson : Thank your plumber next time you meet him.",197,40,Smart-Weird,2023-02-10 01:03:04,https://www.reddit.com/r/dataengineering/comments/10ybytx/thanking_my_plumber_every_time/,False,False,False,False
yntyev,Skills to Learn for $200k - $300k Position?,"I'm currently a data warehouse engineer making $140k/year.  What skills should I master next to increase my salary to $200k and eventually to $300k?  What are some positions with higher salaries that I could transition to?  Any advice would be greatly appreciated!

Below are a few skills that I have already mastered:

* SQL Server
* SSIS
* SSRS
* Tableau
* Excel
* VBA
* Visual Basic.Net
* C#

I'm interested in data engineering, GCP/Azure/AWS, AI/machine learning, automation, big data, etc. and have a strong programming background.  I love learning new technologies, and there's so much that I want to learn, but want to focus on the most useful skills first that a $200k - $300k position would most likely require.",194,106,DesignedIt,2022-11-06 16:00:07,https://www.reddit.com/r/dataengineering/comments/yntyev/skills_to_learn_for_200k_300k_position/,False,False,False,False
yd28wn,U.K. gov consider this a decent package for a Lead DE…,,195,88,tawaiii,2022-10-25 11:15:59,https://i.redd.it/q5oli442rxv91.jpg,False,False,False,False
vjwrdv,"I've been a DE at FAANG for over 4 years, gone from L4 to L6, AMA","Y'all enjoyed this when ezachly did it, so thought  I'd give it a go.

I'm a different kind of DE to him I think too, so it's good to have a bit of variety.

Edit: Thanks for the AMA, feel free to ask more questions, I'll reply async. Sorry I disappeared, time zone difference is a bit tricky. Good luck with all your data engineering endeavours.",194,110,nesh34,2022-06-24 19:47:48,https://www.reddit.com/r/dataengineering/comments/vjwrdv/ive_been_a_de_at_faang_for_over_4_years_gone_from/,False,False,False,False
17iyl70,Great on-prem open source source modern data stack,Went to a dbt meetup a few days ago and this guy from the Barcelona Supercomputing Lab presented his fully open source on-prem modern data stack. Pretty cool imo. They were working with health data from hundreds of hospitals all across EU so they couldn’t use the cloud.,192,55,rudboi12,2023-10-29 08:36:02,https://i.redd.it/bmpkp2jes3xb1.jpg,False,False,False,False
xe5x3t,Controversial and blunt guide for why you can't get a DE job.,"Alright lads, we need to have a chat.


**Introduction**

There's a serious amount of posts in here from people who can't get a DE job.  These posts are along the lines of ""I can't get a DE job no matter how hard I try but I want one how do I get one there must be a secret thanks"".  

So, as a grateful member of this community, I intend on giving back in the form of creating the thread to end all threads.  For some it will be the advice and kick up the arse they needed to hear in order to get their head together. For others, it will crush their already crushed dreams (obligatory Sun Tzu: ""Victorious warriors win first and then go to war, while defeated warriors go to war first and then seek to win""). 

**So...why are *you* giving *me* advice? Seriously, who are you?**

I was you once upon a time (probably/maybe).  I lost my job during the great pando of 2020 and had never written a line of code before or worked in tech or IT before.  I began teaching myself how to code and 6 months later I got my first DE job (My title was and is still now Data Engineer.  Surprisingly, I also do Data Engineer things).  Whilst we're probably not equal in terms of working experience (I had a completely different career before this), we would have definitely been equal in terms of our objectives and what we had - no experience and wanting to break into the DE field.  

On that note, of course this is all from my perspective of ""I can do it, so you can too"", so there's going to be some pretty harsh advice because of two reasons.  One, some of you need a good ol' talking to and two, I'm obviously shit at communicating.

So, let's get going.


**Your CV/Resume sucks.**

This has to be one of the most common problems on here.  You have people experienced and inexperienced all with the same problem - your CV/resume sucks.  It isn't that you suck or your skills are bad, it's just that you have no idea how to write a CV.  And that's alright, it just needs to be said.

Why does your CV suck? Because it's either all over the place, lacks consistency, you're blatantly overselling yourself and people are seeing it a mile off, or is just a plain boring read.  

A CV should tell a story about why you are doing what you are doing.  It should be easy to read, logical to follow (I never ever want to be looking at three different places in your CV.  Should be top down), allow the reader to make their own assumptions about your skill level based on your work, and the only questions left to ask should be covered with ""We can just ask them that in an interview"".  Skills should be what you are comfortable talking about and have done a little of but want to work in.  If you include every single skill you know it makes you look desperate to please to search algorithm and makes you look like you have no idea what you actually want to do.  

That's the general format covered.  Why else does it suck? If you're experienced, you haven't talked a single bit about what you did was important.  Your CV needs to read closer to a list of accomplishments and stuff you're proud of rather than a list of duties.  Not only is it more fun to write about, but it gives the person reading an idea of how you communicate, what you value, and how you think.  Value is the key word here.  Engineering and programming really is just a means to an end and the actual goal is being able to identify and create valuable things for somebody else.


**Not experienced? Applying with projects? Your projects suck.**

So, we've gone over why your CV is probably bad if you are experienced.  So, let's talk about if you have zero experience and are looking at breaking into the industry.

There is an extremely high probability your projects are absolute garbage.  It's incredibly easy to make garbage projects - you can copy them from the internet free of charge.  It's why they are garbage - you don't learn anything from it.  Being extra blunt - there have been a lot shite projects here because almost all of them are copypasta.  Generic binary classification model for some health problem, using the Spotify API to map your most played songs, making a weather app.  They are all boring and over done so the first thing people will think see that in your portfolio is ""These are boring and over done"".  Bonus punishment points if your portfolio is nothing but copypasta.

What are ""good"" projects? Honestly, any data pipeline (get data in, transform, store) is a great start for anybody starting out.  Storing in a database of sorts is even better.  With good logging and unit tests is really getting up there.  Including CI/CD on top of that and you are truly onto a winner.  If it's in the cloud, you're basically a DE at this point so local is fine.  But you'll 100% need to know at least what cloud services do and how they work i.e. the concept of serverless architecture.  The difficult part here is they have to be personal and you will have to have used your own ideas to make it.  If you get asked about your pipeline design decisions during the interview, it should be super easy because you made every single decision.  If it's somebody else's work, you are very likely to fail.

Asking anybody for ""good projects"" is my number one hated question if you are trying to get into somewhere from nothing because if you can't come up with your own ideas, then the job itself is going to be very very hard.  Coming up with ways to solve problems is literally the game here so if you can't invent your own problems with no constraints and unlimited freedom on how you solve it, solving actual problems with actual requirements is going to be a pain in the tits.


**Your mindset sucks.**

So, we've covered application problems.  

One of most annoying things to read is people saying they can't get jobs with no experience or that job ads ask for loads of skills they don't have.  I agree this is common, however, what I disagree with this mindset of giving up at the first hurdle.  If you think you can do the job, then just apply anyway.  The worst thing they can do is say no.  If people keep saying no, then your problem is likely to be your CV sucks.  Go fix it and try again.

Don't let not knowing every single technology on the list stop you.  If you know your lakes from your warehouses, if you have written a few of your own basic pipelines and have a solid Github with DE related projects, what you need is time, patience, and perserverance.  Rest assured if you are tenacious enough, you will become lucky enough.  Applying for jobs isn't by it's nature easy.  It's draining, it's hard work, and, like some things just do in life, needs energy put into it to be successful.  

Be brave.  Small barriers are exactly that - small.  Whatever you don't know, you can learn on the job and you 100% can if you trust yourself.  I know.  It sounds like bullshit but I openly invite you all to try a little bravery and a little bravado when applying.

**Conclusion.**

The real goal of this is to emphasise that job hunting as well as interviewing are themselves skills. You can be the greatest engineer in your area but if there's no introspection on what you can do better, then you will of course be doomed to being stuck in whatever role you have now.  You could be exactly what a company is looking for but you have to be brave and willing to take risks when opportunities arise.",191,112,MikeDoesEverything,2022-09-14 16:10:09,https://www.reddit.com/r/dataengineering/comments/xe5x3t/controversial_and_blunt_guide_for_why_you_cant/,False,False,False,False
11lvm8z,"I got a data engineering horror story, what is yours?","I don't know about you, but I have plenty of data engineering horror stories to share. I'd love to hear the one that still gives you shivers.

  
**Here's my highlight:**  


* I'm at a 500 people medium sized company, 300-400$m revenue. But we're growing strong, at 10-20% each year. Ingest data via python, dbt for transformations, storage in PostgreSQL and Tableau as BI tool on top of it.
* We've been pushing the adoption of Tableau, and the company is eating it up, they love it. They are all over the dashboards.
* What we're particular proud of is our **""north star metric**"" dashboard, showing our new key metric, based on a recent business pivot.
* In our most important customer segment, it looks like it's starting to grow **exponentially**! Everyone is excited!

Suddenly an important manager calls me up

""*hey, something is wrong with the north star. The dashboard looked completely different yesterday! Our exponential growth is gone! Surely there is something wrong, please fix it by this evening. Tomorrow is the board meeting and I'm presenting the exponential growth.""*

  
Took us some time to understand this one... Apparently, ALL data changed, the complete metric in this customer segment broke in, not just for today, but also for yesterday, the day before, and so on...

  
After some research, we realized a huge problem: The biggest customer in that segment left a few months ago, and filed a ""deletion request"". The upstream team responsible for this followed through, and basically ""detached the relevant data from the customer account"". 

  
So there we were. We didn't even know about this process, and had no chance to recover. The manager was left without his exponential growth. 

  
*Aftermath: So what we did from then on is to turn on snapshotting of important data sources (using dbt). After being really unhappy, the manager was still convinced of the underlying exponential growth which shouldn't be reliant on one big customer, but the situation felt terrible. And I'm quite happy that only data from basically one customer went down the drain.*

\----

  
How about you? Do you have a horror story to share?",190,88,sbalnojan,2023-03-08 12:52:53,https://www.reddit.com/r/dataengineering/comments/11lvm8z/i_got_a_data_engineering_horror_story_what_is/,False,False,False,False
141vfwl,I feel like I won the job lottery and it has sent my imposter syndrome into the stratosphere,"A couple years ago I put my career on pause and moved back home to be closer to an ailing parent. I was a data analyst in my previous role, where I mostly worked in Excel, Power BI, a bit of SQL and Python. During this pause I beefed up my SQL, Python and stats knowledge and built some portfolio projects in Streamlit to showcase my interests and abilities. I also learned Git along the way as well.

About a year ago I started job hunting where I was mostly looking for analyst roles. As I progressed in the job hunt I became increasingly interested in analytics engineering but felt like my skills weren't quite there yet. At the time I was thinking the best path would be getting another DA role and using it to catapult into an engineering role down the line. Fast forward a year, countless job applications, numerous interviews later, I finally had the most incredible breakthrough.

Last week someone in my network connected me to the owner of a software development company that builds AI tools for customers. We exchanged a couple texts, hoped on a call, got talking about my journey in the world of data and how I ultimately want to get into analytics engineering. He then asked me if I was comfortable with window functions in SQL (I am) and if I know basic data structures (I do). At that point he asked me if I could send him a copy of my CV and if I could meet with him and some of his teammates for a dinner later in the week. I go to the dinner, meet the teams senior software engineers, we hit it off over some tacos, and they essentially create a role for me out of thin air! It was all very serendipitous and I still have no idea how this happened. They even told me that they weren't in the market for hiring but liked my story/journey/tenacity so much that they wanted me to come and work for them. The owner even told me as we were shaking hands and saying goodbye that he doesn't care about my background and that ""it is all about investing in the right people"".

Yes, they are a legitimate company and have several large clients that you have heard of. I received their job offer today and I nearly threw up after reading it. The title is Data Automation Developer, so most of my work is going to be in automation testing and they have also thrown in some ELT tasks as well. It's a fully remote role and one that I never imagined I would have in my career. While I don't feel like I am being setup for failure, and I do really like the team, I can't help but feel immense imposter syndrome when I look at the job posting that they created and I see all sorts of things I have no experience in. Is this even real life?? I understand the idea of hiring on someones potential but this is all incredibly daunting. Has anyone had a similar experience to this?",188,58,yror007,2023-06-05 23:08:45,https://www.reddit.com/r/dataengineering/comments/141vfwl/i_feel_like_i_won_the_job_lottery_and_it_has_sent/,False,False,False,False
uu9j14,Created my First Data Engineering Project a Surf Report,"# Surfline Dashboard

Inspired by this post: [https://www.reddit.com/r/dataengineering/comments/so6bpo/first\_data\_pipeline\_looking\_to\_gain\_insight\_on/](https://www.reddit.com/r/dataengineering/comments/so6bpo/first_data_pipeline_looking_to_gain_insight_on/)

&#x200B;

I just wanted to get practice with using AWS, Airflow and docker. I currently work as a data analyst at a fintech company but I don't get much exposure to data engineering and mostly live in sql, dbt and looker. I am an avid surfer and I often like to journal about my sessions. I usually try to write down the conditions (wind, swell etc...) but I sometimes forget to journal the day of and don't have access to the past data. Surfline obviously cares about forecasting waves and not providing historical information. In any case seemed to be a good enough reason for a project.

Repo Here:

[https://github.com/andrem8/surf\_dash](https://github.com/andrem8/surf_dash)

&#x200B;

# Architecture

# 

https://preview.redd.it/ckjp60xdhp091.png?width=9792&format=png&auto=webp&s=f7cae5b8fb1167bebef44753dadb8d73a4d5c2dc

# Overview

The pipeline collects data from the surfline API and exports a csv file to S3. Then the most recent file in S3 is downloaded to be ingested into the Postgres datawarehouse. A temp table is created and then the unique rows are inserted into the data tables. Airflow is used for orchestration and hosted locally with docker-compose and mysql. Postgres is also running locally in a docker container. The data dashboard is run locally with ploty.

# ETL

https://preview.redd.it/vgfsrk8ihp091.png?width=1895&format=png&auto=webp&s=841cf9002ac643a17536ebf92dde63714a1e3989

# Data Warehouse - Postgres

# 

https://preview.redd.it/dhy74eykhp091.png?width=1039&format=png&auto=webp&s=cbed0caf7e89198aa0d68a96a254fc846ba13212

# Data Dashboard

&#x200B;

https://preview.redd.it/896bizwnhp091.png?width=627&format=png&auto=webp&s=86cadc6e6ab8fa594e49e0a3fca56df2d9c72740

# Learning Resources

Airflow Basics:

&#x200B;

\[Airflow DAG: Coding your first DAG for Beginners\]([https://www.youtube.com/watch?v=IH1-0hwFZRQ](https://www.youtube.com/watch?v=IH1-0hwFZRQ))

&#x200B;

\[Running Airflow 2.0 with Docker in 5 mins\]([https://www.youtube.com/watch?v=aTaytcxy2Ck](https://www.youtube.com/watch?v=aTaytcxy2Ck))

&#x200B;

S3 Basics:

&#x200B;

\[Setting Up Airflow Tasks To Connect Postgres And S3\]([https://www.youtube.com/watch?v=30VDVVSNLcc](https://www.youtube.com/watch?v=30VDVVSNLcc))

&#x200B;

\[How to Upload files to AWS S3 using Python and Boto3\]([https://www.youtube.com/watch?v=G68oSgFotZA](https://www.youtube.com/watch?v=G68oSgFotZA))

&#x200B;

\[Download files from S3\]([https://www.stackvidhya.com/download-files-from-s3-using-boto3/](https://www.stackvidhya.com/download-files-from-s3-using-boto3/))

&#x200B;

Docker Basics:

&#x200B;

\[Docker Tutorial for Beginners\]([https://www.youtube.com/watch?v=3c-iBn73dDE](https://www.youtube.com/watch?v=3c-iBn73dDE))

&#x200B;

\[Docker and PostgreSQL\]([https://www.youtube.com/watch?v=aHbE3pTyG-Q](https://www.youtube.com/watch?v=aHbE3pTyG-Q))

&#x200B;

\[Build your first pipeline DAG | Apache airflow for beginners\]([https://www.youtube.com/watch?v=28UI\_Usxbqo](https://www.youtube.com/watch?v=28UI_Usxbqo))

&#x200B;

\[Run Airflow 2.0 via Docker | Minimal Setup | Apache airflow for beginners\]([https://www.youtube.com/watch?v=TkvX1L\_\_g3s&t=389s](https://www.youtube.com/watch?v=TkvX1L__g3s&t=389s))

&#x200B;

\[Docker Network Bridge\]([https://docs.docker.com/network/bridge/](https://docs.docker.com/network/bridge/))

&#x200B;

\[Docker Curriculum\]([https://docker-curriculum.com/](https://docker-curriculum.com/))

&#x200B;

\[Docker Compose - Airflow\]([https://medium.com/@rajat.mca.du.2015/airflow-and-mysql-with-docker-containers-80ed9c2bd340](https://medium.com/@rajat.mca.du.2015/airflow-and-mysql-with-docker-containers-80ed9c2bd340))

&#x200B;

Plotly:

&#x200B;

\[Introduction to Plotly\]([https://www.youtube.com/watch?v=hSPmj7mK6ng](https://www.youtube.com/watch?v=hSPmj7mK6ng))",187,21,surf_ocean_beach,2022-05-20 22:34:05,https://www.reddit.com/r/dataengineering/comments/uu9j14/created_my_first_data_engineering_project_a_surf/,False,False,False,False
rdftxf,Hold the line fellow data engineers,"I’ve been interviewing around and so far every place I’ve interviewed with hasn’t been able to secure a data engineer because they require in office work after the pandemic.  

All I wanted to say is keep it up guys. If you’re looking for a new job that is remote work only, keep putting pressure on these companies!",187,45,importpandaaspd,2021-12-10 19:01:00,https://www.reddit.com/r/dataengineering/comments/rdftxf/hold_the_line_fellow_data_engineers/,False,False,False,False
13qg3t1,Why can I not understand what DataBricks is? Can someone explain slowly?!,"I have experience as a BI Developer / Analytics Engineer using dbt/airflow/SQL/Snowflake/BQ/python etc... I think I have all the concepts to understand it, but nothing online is explaining to me exactly what it is, can someone try and explain it to me in a way which I will understand?",183,110,wallyflops,2023-05-24 08:56:40,https://www.reddit.com/r/dataengineering/comments/13qg3t1/why_can_i_not_understand_what_databricks_is_can/,False,False,False,False
vzlnh7,"I made a pipeline that integrates London bike journeys with weather data using Google Cloud, Airflow, Spark, BigQuery and Data Studio","Like [another recent post](https://www.reddit.com/r/dataengineering/comments/vkfs57/i_created_a_pipeline_extracting_reddit_data_using/), I developed this pipeline after going through the [DataTalksClub Data Engineering course](https://github.com/DataTalksClub/data-engineering-zoomcamp). I am working in a data-intensive STEM field currently, but was interested in learning more about cloud technologies and data engineering. 

The pipeline digests two separate datasets: one that records bike journeys that take place using London's public cycle hire scheme, and another that contains daily weather variables on a 1km x 1km grid across the entirety of the UK. The pipeline integrates these two datasets into a single BigQuery database. Using the pipeline, you can investigate the 10 million journeys that take place each year, including the time, location and weather for both the start and end of each journey.

The repository has a detailed [README](https://github.com/jackgisby/tfl-bikes-data-pipeline/blob/main/README.md) and additional documentation both within the Python scripts and in the [docs/](https://github.com/jackgisby/tfl-bikes-data-pipeline/tree/main/docs) directory.

The GitHub repository: [https://github.com/jackgisby/tfl-bikes-data-pipeline](https://github.com/jackgisby/tfl-bikes-data-pipeline)

&#x200B;

**Key pipeline stages**

1. Use Docker/Airflow to ingest weekly cycling data to Google Cloud Storage
2. Use Docker/Airflow to ingest monthly weather to Google Cloud Storage
3. Send a Spark job to a Google Cloud Dataproc cluster to transform the data and load it to a BigQuery database
4. Use Data Studio to create dashboards

[Overview of the technologies used and the main pipeline stages](https://preview.redd.it/x8b8gmwqfpb91.png?width=1497&format=png&auto=webp&s=9eeb04e7dfcd8d25a73b97c3fcf0a6e81a549e6f)

&#x200B;

**BigQuery Database**

I tried to design the BigQuery database like a star schema, although my journeys ""fact table"" doesn't actually have any key measures. The difficult part was creating the weather ""dimension"" table, which includes recordings each day in a 1km x 1km grid across the UK. I joined it to the journeys/locations tables by finding the closest grid point to each cycle hub.

[Schema for the final BigQuery database](https://preview.redd.it/rtnvexdqfpb91.png?width=689&format=png&auto=webp&s=8bac5877b9378b56bba588734bb48dbf2bfc611c)

&#x200B;

**Dashboards**

I made a couple of dashboards, the first visualises the main dataset (the cycle journey data), for instance in the example below.

[Dashboard filtered for the four most popular destinations from 2018-2021](https://preview.redd.it/ak8a0q2yfpb91.png?width=2845&format=png&auto=webp&s=a8dda8a1f4878a6689ed5f845719f371cd491e83)

And another to show how the cycle data can be integrated with the weather data.

[A dashboard comparing the number of journeys taking place to the daily temperature in 2018 and 2019. The data is for journeys starting at \\""Hop Exchange, The Borough\\"" in London](https://preview.redd.it/r0cborybgpb91.png?width=1834&format=png&auto=webp&s=3ca83e7ddb611fbdc99328ad0e437f142de88a52)

**Data sources**

* Transport for London Cycling Data: [https://cycling.data.tfl.gov.uk/](https://cycling.data.tfl.gov.uk/)
* Weather Data: [https://catalogue.ceda.ac.uk/uuid/4dc8450d889a491ebb20e724debe2dfb](https://catalogue.ceda.ac.uk/uuid/4dc8450d889a491ebb20e724debe2dfb)

&#x200B;

The pipeline has a number of **limitations**, including:

* The pipeline is probably too complex for the size of the data, but I was interested in learning Airflow/Spark and cloud concepts
* I do some data transformations before uploading the weather data to Google Cloud Storage. I believe it would be better to separate the Airflow process from this computation
* It might be worth using Google's Cloud Composer to host Airflow rather than running it locally or on a virtual machine
* The Spark script is overly complex, it would be better to split this up into multiple scripts
* There is a lack of automated testing, validation of input data and logging
* In reality, the weather aspect of the pipeline is probably a bit overkill. The weather at the start and end of each journey is unlikely to be too different. Instead of collecting weather variables for each cycle hub, I could have achieved a similar effect by including a single variable for London as a whole. 

I stopped developing the pipeline as I have other work to do and my Google Cloud trial is coming to an end. But, I'm interested in hearing in any advice/criticisms about the project.",186,27,tmp_username_,2022-07-15 10:46:57,https://www.reddit.com/r/dataengineering/comments/vzlnh7/i_made_a_pipeline_that_integrates_london_bike/,False,False,False,False
vhcn7n,The State of Data Engineering 2022,,182,64,woltan_4,2022-06-21 12:52:08,https://lakefs.io/the-state-of-data-engineering-2022/#,False,False,False,False
18wnsqj,Data Testing Cheat Sheet: 12 Essential Rules," 

1. **Source vs Target Data Reconciliation:** Ensure correct loading of customer data from source to target. Verify row count, data match, and correct filtering.
2. **ETL Transformation Test:** Validate the accuracy of data transformation in the ETL process. Examples include matching transaction quantities and amounts.
3. **Source Data Validation:** Validate the validity of data in the source file. Check for conditions like NULL names and correct date formats.
4. **Business Validation Rule:** Validate data against business rules independently of ETL processes. Example: Audit Net Amount - Gross Amount - (Commissions + taxes + fees).
5. **Business Reconciliation Rule:** Ensure consistency and reconciliation between two business areas. Example: Check for shipments without corresponding orders.
6. **Referential Integrity Reconciliation:** Audit the reconciliation between factual and reference data. Example: Monitor referential integrity within or between databases.
7. **Data Migration Reconciliation:** Reconcile data between old and new systems during migration. Verify twice: after initialization and post-triggering the same process.
8. **Physical Schema Reconciliation:** Ensure the physical schema consistency between systems. Useful during releases to sync QA & production environments.
9. **Cross Source Data Reconciliation:** Audit if data between different source systems is within accepted tolerance. Example: Check if ratings for the same product align within tolerance.
10. **BI Report Validation:** Validate correctness of data on BI dashboards based on rules. Example: Ensure sales amount is not zero on the sales BI report.
11. **BI Report Reconciliation:** Reconcile data between BI reports and databases or files. Example: Compare total products by category between report and source database.
12. **BI Report Cross-Environment Reconciliation:** Audit if BI reports in different environments match. Example: Compare BI reports in UAT and production environments.

[Data Testing Cheat Sheet](https://preview.redd.it/mknzrwbvn0ac1.png?width=1887&format=png&auto=webp&s=f9023dd75ddde8a5f4355eec6e0d0be08c836e48)",184,10,icedqengineer,2024-01-02 11:59:27,https://www.reddit.com/r/dataengineering/comments/18wnsqj/data_testing_cheat_sheet_12_essential_rules/,False,False,False,False
13jgov7,Secret To Optimizing SQL Queries - Understand The SQL Execution Order,,182,27,mjgcfb,2023-05-16 20:45:01,https://www.youtube.com/watch?v=BHwzDmr6d7s,False,False,False,False
t4kz8u,Wtf is a datalake?,"When I first heard the term used a few years ago I thought it was some advanced highly integrated composition of AI/ML techniques, data, and datastores.    Now, as I decided to learn a little bit about them a datalake seems to be nothing more than a giant bucket for you to toss your ""similiar"" data into.

By the latter, that would mean my  designated storage of 16TBs,  which consists of nothing but horse porn, is a data Lake.

So can someone please explain what a data Lake is, what it does, and how it's used?

EDIT:  Question answered.  Y'all rock, so glad I found this sub.",182,62,SexPartyStewie,2022-03-01 23:01:10,https://www.reddit.com/r/dataengineering/comments/t4kz8u/wtf_is_a_datalake/,False,False,False,False
o64f6n,"DataBricks is providing top notch learning material for Data Engineers and Data Scientists Worth 2000USD(1.45 Lakh Rs) for Free. Verified June 22, 2021.","Update 26-07-2021:

DataBricks is providing top notch learning material for Data Engineers and Data Scientists Worth 2000USD(1.45 Lakh Rs) for Free.

The courses and trainings are working again.

Please go to the below post for further instructions:

https://www.reddit.com/r/Stream2Learn/comments/ortlng/databricks_is_providing_topnotch_learning/?utm_source=share&utm_medium=web2x&context=3

Update 29-06-2021:
---------------------------------------------------------------------------------
The coupon has expired and i will keep you guys posted on this same thread.

Meanwhile as the coupon has expired , as a consolation i am planning to upload a set of Udemy courses with free certificates on a daily basis which are available for a limited time. So kindly make use of the opportunity ASAP.

The courses might vary from various tech stacks!

The post is below here:

https://www.youtube.com/watch?v=J8Oau6zn8f4

<b>The above link would be updated on a daily basis with around 10 to 15 courses per day which are fully free and have lifetime access in Udemy.</b>

---------------------------------------------------------------------------------

Databricks Code:

[https://www.youtube.com/watch?v=iVy9rGZmDoU](https://www.youtube.com/watch?v=iVy9rGZmDoU)

Check the above Video.

DataBricks is providing top notch learning material for Data Engineers and Data Scientists Worth 2000USD(1.45 Lakh Rs) for Free.  

https://preview.redd.it/xh0fvmxtnx671.png?width=2747&format=png&auto=webp&s=1a7c4cea1a82c5ba7e173fd4d47554822a7a9954



I am personally enrolled into data engineering path as I am into Bigdata domain.  


The learning material is top notch as it is prepared by the original creators of Apache Spark.  


Here are the steps to get enrolled for free:  
Check the Video.  

Apply Coupon ""DB_PE"" and click on checkout  

Create your account and get 2000USD course for free  
Enjoy Learning!!  


Share as much as you can so that others can also get a chance to learn too .  


I have also pinned the steps in comments too if anyone is not able to click on link can check the comment section  

Code is working again.

Please do subscribe to my channel . And a like and a comment there would be great as well.

 https://www.youtube.com/channel/UCjO8Jq2sdpuI134axhMp0Fg",182,71,ashwinsakthi,2021-06-23 03:19:33,https://www.reddit.com/r/dataengineering/comments/o64f6n/databricks_is_providing_top_notch_learning/,False,False,False,False
19c2ftl,Some Data Scientists write bad Python code and are stubborn in code reviews,"My first job title in tech was Data Scientist, now I'm officially a Data Engineer, but working somewhere in Data Science/Engineering, MLOps and as a Python Dev.

I'm not claiming to be a good programmer with two and a half years of professional experience, but I think some of our Data Scientists write bad Python code.  


Here I explain why:

* Using generic execptions instead of thinking about what error they really want to catch
* They try to encapsulate all functions as static methods in classes, even though it's okay to use free standing functions sometimes
* They don't use enums (or don't know what enums are used for)
* Sometimes they use bad method names -> they think `da_file2tbl_file()` is better than `convert_data_asset_to_mltalble()` (What do you think is better?)
* Overengineering: Use of design patterns with 70 lines of code, although one simple free-standing function with 10 lines would have sufficed (-> but I respect the fact that an effort is made here to learn and try out new things)
* Use of global variables, although this could easily have been solved with an instance variable or a parameter extension in the method header
* Too many useless and redundant comments like:  
`# Creating dataframe`  
`df = pd.DataFrame(...)`
* Use of magic strings/numbers instead of constants
* etc ...

What are your experiences with Data Scientists or Data Engineers using Python?

I don't despise anyone who makes such mistakes, but what's bad is that some Data Scientists are stubborn and say in code reviews: ""But I want to encapsulate all functions as static methods in a class or ""I think my 70-line design pattern is better than your 10-code-line function"" or ""I'd rather use global variables. I don't want to rewrite the code now."" I find that very annoying. Some people have too big an ego. But code reviews aren't about being the smartest in the room, they're about learning from each other and making the product better.  


Last year I started learning more programming languages. Kotlin and Rust.  I'm working on a personal project in Kotlin to rebuild our machine learning infrastructure and I'm still at tutorial level with Rust.  Both languages are amazing so far and both have already helped me to be a better (Python) programmer. What is your experience? Do you also think that learning more (statically typed) languages makes you a better developer? ",183,130,noisescience,2024-01-21 12:31:19,https://www.reddit.com/r/dataengineering/comments/19c2ftl/some_data_scientists_write_bad_python_code_and/,False,False,False,False
13znm1j,What is the Leetcode equivalent for Data Engineering?,"Actively interviewing so I need some prep material for Data wrangling questions if there is a single source out there.

I'm looking for a source around questions like:

\- Given a source data (JSON, CSV), derive insights to answer questions

\- Clean up a given dataset to answer questions etc.

\- Python dictionary / Json API response manipulation.

&#x200B;

Thank you.",184,46,None,2023-06-03 19:56:52,https://www.reddit.com/r/dataengineering/comments/13znm1j/what_is_the_leetcode_equivalent_for_data/,False,False,False,False
13oaw8m,Cloud Comparison by simonholdorf,,181,33,Kickass_Wizard,2023-05-22 00:14:01,https://i.redd.it/k8myhl1bz91b1.jpg,False,False,False,False
ye10az,"Name that Title.. I’ll go first, Analytics Project Manager","I’m joking, kinda, not really depending on the company lol",182,49,ItsBrittaniaBitch,2022-10-26 15:33:32,https://i.redd.it/9m2exwyw56w91.jpg,False,False,False,False
u2uyty,PSA: Don't apply for Data Engineering positions at FAANG,"Of course , this depends entirely on what you want to do.

If you enjoy less of the software engineering side of Data Engineering, you enjoy building dashboards and using GUI tools.. then a DE position at FAANG is perfect.

However, if you want to do more software engineering, programming, computer science, etc. Then what you want to do is apply for positions such as **Software Engineer - Data**

Here's an example of a [job positing from Amazon](https://www.amazon.jobs/en/jobs/1823062/software-engineer-recommender-systems-big-data-distributed-computing-machine-learning)

Yes, this does mean you will have to know computer science, software engineering, data structures and algorithms. It also means you will have to know how to do leetcode medium / hards (both SQL and traditional ones based on heaps, graphs, stacks, queues, etc). It is also beneficial to have some fullstack / backend software engineering experience for these positions, because there is a huge carry over. You would also want good System Design knowledge / software architecture knowledge.

Just wanted to post this because I see a lot of people complaining that their new DE job isn't what they hoped for. Now you know what to look for!",182,65,None,2022-04-13 17:00:12,https://www.reddit.com/r/dataengineering/comments/u2uyty/psa_dont_apply_for_data_engineering_positions_at/,False,False,False,False
1aqq9vc,What the Hell is a Data Lake?,"I’m writing this post after, admittedly, never having used a “data lake” (I think) but have thought I knew what it was… on a few different occasions.

I know it’s a place to store data, but what data structure does it use? Presumably, it is a directory structure and storage is nonmutable… block storage, like S3. is that it?

Then there’s MongoDB Atlas which says it’s a Data Lake and I’m like wtf… I’ve used Mongo before to record some JSON documents without a schema, but that’s not what I know a Data Lake to be. Unless “Atlas” is something else…

So now I’m wondering, is a Data Lake less about the underlying structure and properties, and more about what goes in (raw data) and what derived data comes out (effectively ELT). Is this general idea, being used at large scale, what constitutes a “Data Lake?”

Help me out here guy please.",178,120,DuckDatum,2024-02-14 15:59:10,https://www.reddit.com/r/dataengineering/comments/1aqq9vc/what_the_hell_is_a_data_lake/,False,False,False,False
1aofpbr,What we learned after running Airflow on Kubernetes for 2 years,,180,14,UpvoteBeast,2024-02-11 19:07:49,https://api.daily.dev/r/HAWJyvDVy,False,False,False,False
146rj9m,Does anyone else hate Pandas?,"I’ve been in data for ~8 years - from DBA, Analyst, Business Intelligence, to Consultant. Through all this I finally found what I *actually* enjoy doing and it’s DE work.

With that said - I absolutely hate Pandas. It’s almost like the developers of Pandas said “Hey. You know how everyone knows SQL? Let’s make a program that uses completely different syntax. I’m sure users will love it”

Spark on the other hand did it right.

Curious for opinions from other experienced DEs - what do you think about Pandas?

*Thanks everyone who suggested Polars - definitely going to look into that",182,195,datingyourmom,2023-06-11 11:26:53,https://www.reddit.com/r/dataengineering/comments/146rj9m/does_anyone_else_hate_pandas/,False,False,False,False
p3b3xd,Just got hired as a DE in FAANG,"It's obviously nice to brag to my friends, but none of them are in this space, so it doesn't feel as good to share this with them. I thought you all would appreciate it more. This is the proudest I've been, and I can't wait to dive in! Give me any tips/advice for onboarding if you've got them!

ETA: I wrote a blog post about my path leading to here and the interview process itself, which can be found [here](https://tibblesnbits.com/posts/de-interview-faang).",180,66,therealtibblesnbits,2021-08-12 23:03:24,https://www.reddit.com/r/dataengineering/comments/p3b3xd/just_got_hired_as_a_de_in_faang/,False,False,False,False
mqseze,"Educational project I built: ETL Pipeline with Airflow, Spark, s3 and MongoDB.","While I was learning about Data Engineering and tools like Airflow and Spark, I made this educational project to help me understand things better and to keep everything organized:

https://github.com/renatootescu/ETL-pipeline

Maybe it will help some of you who, like me, want to learn and eventually work in the DE domain.

What do you think could be some other things I could/should learn?",181,36,derzemel,2021-04-14 15:02:30,https://www.reddit.com/r/dataengineering/comments/mqseze/educational_project_i_built_etl_pipeline_with/,False,False,False,False
18j0ygk,"""We have so many challenging projects!"" ",,178,4,Marawishka,2023-12-15 14:10:32,https://v.redd.it/qgx75hdxug6c1,False,False,False,False
15xoran,I am a 10 YOE (SSIS/low-code) DE preparing to transition into tier 1 tech companies. Here's my study plan in case it helps someone else.,"Everything is listed in order of importance. I'm breaking my prep down into:

1. **DS & Algorithms**
   1. Python Data Structures (Dicts, Lists, Sets, Tuples)
   2. CS Data Structures (Hash, Strings, Trees, Graphs, ArrayLists, Linked Lists, Heaps)
   3. Algorithms (BFS, DFS, Binary Search, Sorting)
   4. Concepts (\*Big O\*, Recursion, DP, Memory)
   5. Book: Cracking the coding interview - use (a) Technical *Approach* and (b) Chapter Explanations ; avoid problem sets
   6. Sites: Leetcode (no more than medium python for each major concept) ; get premium and take advantage of ""Learn"" cards for Recursion and DP.
   7. Sites: Technical Handbook - tells you what you're being evaluated on --- its not just about getting the right answer!
2. **System Design**
   1. Analytics Platforms -
      1. Research the companies you are interested in and understand why they use the technologies they do. Biggest misconception about DE System Design is that it is like SWE System Design -- it is not.
      2. Focus is on: tapping into Operational Data Stores (ODS), using Extract Transform Load (ETL) for batch or streaming processes, storing data with proper partitioning and tools, using data for Reports/Dashboards or serving it up to ML models with APIs.
   2. The *Approach* \-
      1. [Youtube Video by Mikhail Smarshchok](https://www.youtube.com/watch?v=bUHFg8CZFws) By far the best video I have seen on approach. For content, see above.
      2. Book: Alex Xu System Design Interview
      3. Site: Grokking the System Design Interview
   3. SWE Fundamentals - Doesn't hurt to know foundational System Design concepts. They are all related and approach resources will cover what you need to know.
   4. API Design - Site: Grokking the API Design Interview (I haven't personally started yet)
3. **Product Sense (for meta this is # 2 priority)**
   1. What is product sense? To understand and troubleshoot your product means you need to measure the right metrics. Your daily active users (DAU) has tanked dramatically, how do you find out what's the issue? What metrics do you capture and look for? How do you use them to improve your product?
   2. Site: Youtube Channel - Emma Ding - *Approach* and concepts
   3. Resources: Meta Data Engineer Guide (by meta engineers)
4. **Data Modeling**
   1. Book: The data warehouse toolkit (this is the only book on the subject I have ever read, rest I've googled problems when I ran into them for work)
   2. SWE interview snippets - when people dive into ""design uber"" or ""design twitter"", they often set up the data model. SWE system design interviews are worth browsing for this concept
5. **ML Concepts**
   1. Supervised, Unsupervised, Deep Learning, Model Eval -- There's many resources out there, I paid $2000 for MIT Great Learning Course and they have a nice modular learning platform.
   2. Model Ops / Deployment: Book - Machine Learning Design Patterns
   3. Approach: Book - Machine Learning System Design Interview
6. **Cloud (AWS is the most commonly used)**
   1. Learn about common DE tools used for ETL
   2. Learn about common ML tools
   3. Get a cert if you want

&#x200B;

\*Approach resources will help you with developing a methodology for answering certain types of questions. You could understand a DS and probably coded it in college, but you may not be able to use it in an interview which is time-constrained and high-pressure without a good approach.

\*Books - z library

This study guide is my second attempt at trying after passing meta and roblox loops, but ultimately getting down-leveled with no offer. This guide is for senior DE positions; if you are entry-level, you may focus less on System Design and cover high-level ML and cloud concepts.

&#x200B;

Current TC: $240K (Cash, Bonus) No equity -- HCOL",177,77,Raydox328,2023-08-22 00:01:04,https://www.reddit.com/r/dataengineering/comments/15xoran/i_am_a_10_yoe_ssislowcode_de_preparing_to/,False,False,False,False
10h56jr,I started a new DE job in Dec and I suck at git.,"That's it. I was an sql dev and a data analyst before that. The closest thing I have to version control was renaming each new file with a date. I am feeling like a lamb in fire. My new coworkers are nice about but I do not want to keep asking them for help.

Edit: thank you all so much",180,93,WhyDoIHaveAnAccount9,2023-01-20 19:02:11,https://www.reddit.com/r/dataengineering/comments/10h56jr/i_started_a_new_de_job_in_dec_and_i_suck_at_git/,False,False,False,False
rdw3b3,Data Engineering Jargon,"I wrote a list of 50 - I'll share ten at a time.

1- 10 is below.

11-20 is [here](https://www.reddit.com/r/dataengineering/comments/rem26j/data_engineering_jargon_part_2/)

21-30 is [here](https://www.reddit.com/r/dataengineering/comments/rfbuu8/data_engineering_jargon_part_3/)

31-40 is [here](https://www.reddit.com/r/dataengineering/comments/rg5vr0/data_engineering_jargon_part_4/)

**1. Data Dump**

A file or a table containing a significant amount of data to be analysed or transferred.

*A table containing the ""data dump"" of all customer addresses.*

**2. Data Pipelines**

A data processing method akin to a pipeline, which starts with data ingestion then processing then completion.

*A pipeline where customer address data is ingested from source A and then aggregated according to their cities and this new information is loaded into destination B.*

**3. DBA**

Database Administrator is an admin role that understands the particular database technology and how to get the best out of it. This includes improving performance, backups and recovery.

*Performance tuning the database to respond better to particular complex data queries.*

**4. Data Warehouse**

A method of organising data to make it easy to analyse and report to make business decisions

*Oracle data warehouse. Organising customer data in a data warehouse to be able to report the number of newly acquired customers.*

**5. Data Mart**

A subset of a data warehouse, created for a very specific business use case.

*Finance data mart storing all the relevant financial information required by the Accounting team to process their month-end cycles.*

**6. ODS**

Operational data store generally stores limited and current information to help simple queries. Unable to handle historical or complex data queries.

*An ODS for daily stock fluctuations in a warehouse help the warehouse manager decide what to prioritise in the next order delivery.*

**7. EDW**

The same as a data warehouse except it includes all the data within an organisation. This means that the entire enterprise can rely on this warehouse for their business decisions.

*Organising sales, customer, marketing and finance data in an enterprise data warehouse to be able to create several key management reports.*

**8. RDBMS**

Relational database management system. All of the above examples are RDBMS, meaning they store data in a structured format using rows and columns.

*A Microsoft SQL server database.*

**9. In-memory DB**

Traditional databases have been used for complex calculations and queries. They store information on the actual disk in the computer. In-memory DB stores all the information on their memory (RAM), this allows for rapid calculations without read and write a function to a normal disk.

*A drill-down functionality of a live dashboard.*

**10. Data Lake**

A repository for all kinds of structured and unstructured data. Mainly based on Hadoop storage technology. Called a lake as it is flexible enough to store anything from raw data to unstructured email files.

*Hadoop Data Lake. Storing logs of all customers called into the inbound call centre including call duration.*

11-20 is [here](https://www.reddit.com/r/dataengineering/comments/rem26j/data_engineering_jargon_part_2/)

21-30 is [here](https://www.reddit.com/r/dataengineering/comments/rfbuu8/data_engineering_jargon_part_3/)

31-40 is [here](https://www.reddit.com/r/dataengineering/comments/rg5vr0/data_engineering_jargon_part_4/)",180,18,Data_Cog,2021-12-11 09:19:43,https://www.reddit.com/r/dataengineering/comments/rdw3b3/data_engineering_jargon/,False,False,False,False
kt5329,Microsoft has a ton of free courses for data engineering. Here is one with 10 hours of content for DataBricks,,178,17,None,2021-01-08 15:49:14,https://docs.microsoft.com/en-us/learn/paths/data-engineer-azure-databricks/,False,False,False,False
ituxbu,Data Engineering from the Ground Up - Part 1 - Baby's First Data Pipeline,,178,26,None,2020-09-16 12:57:24,https://peterdannemann.com/babys-first-data-pipeline/,False,False,False,False
197t8fz,"Apache Iceberg: SQL and ACID semantics in the front, scalable object storage in the back",,177,26,bitsondatadev,2024-01-16 03:52:52,https://i.redd.it/731s003m5qcc1.jpeg,False,False,False,False
18hlsqb,Can someone explain the job of data engineer like I'm a baboon?,"At my previous job, I was a data analyst, but I've come across a lot of people who tell me that I should go into data engineering. Problem is, every time I ask someone who is in that field, they honestly cannot tell me what it is that they do. Like, I have never met a single data engineer in any company I've worked for who has given me a simple and reasonable explanation for what they do.



At my previous job, I designed ETL queries using SQL and Python, wrote APIs for interfacing with different database softwares for example I created an API in Python to automatically connect to Google BigQuery, retrieve data for the last 30 days, and then move it into other data sources 


I also performed audits on data, so where there were gaps in data or areas where they said that data was incorrect, I would go hunting and find gaps in the data that didn't make sense, for example, why is there missing data between these two linked tables? Is there a specific date that there's missing data?


Finally, I created new links between data across different sources, for example from snowflake to BigQuery, even a little bit of access",178,81,None,2023-12-13 17:16:40,https://www.reddit.com/r/dataengineering/comments/18hlsqb/can_someone_explain_the_job_of_data_engineer_like/,False,False,False,False
13ster7,Reddit Sentiment Analysis Real-Time* Data Pipeline,"Hello everyone!

I wanted to share with you a side project that I started working on recently just in my free time taking inspiration from other similar projects. I am almost finished with the basic objectives I planned but there is always room for improvement. I am somewhat new to both Kubernetes and Terraform, hence looking for some feedback on what I can further work on. The project is developed entirely on a local Minikube cluster and I have included the system specifications and local setup in the README.

  
Github link: [https://github.com/nama1arpit/reddit-streaming-pipeline](https://github.com/nama1arpit/reddit-streaming-pipeline)

&#x200B;

The Reddit Sentiment Analysis Data Pipeline is designed to collect live comments from Reddit using the Reddit API, pass them through Kafka message broker, process them using Apache Spark, store the processed data in Cassandra, and visualize/compare sentiment scores of various subreddits in Grafana. The pipeline leverages containerization and utilizes a Kubernetes cluster for deployment, with infrastructure management handled by Terraform.

Here's the brief workflow:

* A containerized Python application to collect real-time reddit comments from certain subreddits and ingest them into the Kafka broker
* Zookeeper and Kafka pods act as a message broker for providing the comments to other applications.
* A Spark container running job to consume raw comments data from the kafka topic, process it and pour it into the data sink, i.e. Cassandra tables.
* A Cassandra database is used to store and persist the data generated by the Spark job.
* Grafana establishes a connection with the Cassandra database. It queries the aggregated data from Cassandra and presents it visually to users  through a dashboard. Grafana dashboard sample link: [https://raw.githubusercontent.com/nama1arpit/reddit-streaming-pipeline/main/images/grafana\_dashboard.png](https://raw.githubusercontent.com/nama1arpit/reddit-streaming-pipeline/main/images/grafana_dashboard.png)

I am relatively new to almost all the technologies used here, especially Kafka, Kubernetes and Terraform, and I've gained a lot of knowledge while working on this side project. I have noted some important improvements that I would like to make in the README. Please feel free to point out if there are any cool visualisations I can do with such data. I'm eager to hear any feedback you may have regarding the project!

PS: I'm also looking for more interesting projects and opportunities to work on. Feel free to DM me

Edit: I added this post right before my 18 hour flight. After landing, I was surprised by the attention it got. Thank you for all the kind words and stars.",177,34,Minimum-Nebula,2023-05-27 00:38:15,https://www.reddit.com/r/dataengineering/comments/13ster7/reddit_sentiment_analysis_realtime_data_pipeline/,False,False,False,False
vj10xz,How do you guys ace your SQL skills?,"I am asking about mastering them. Like queries with varying levels of complexity. Some of the Technical Analysts I've worked with have written most mind-blowing Scripts with ease. I encounter the databases daily and want to acquire that levels of proficiency. I am familiar with SQL but I want to take it to the next level. Would you guys suggest me the best places to start exploring and also the strategies that worked for you to enhance your SQL skillsets. 

Thanks in advance!!!",175,49,SentientHero,2022-06-23 16:34:08,https://www.reddit.com/r/dataengineering/comments/vj10xz/how_do_you_guys_ace_your_sql_skills/,False,False,False,False
so6bpo,First Data Pipeline - Looking to gain insight on Rust Cheaters,"Hello Everyone,

I posted to this subreddit about a roadmap I created to learn data engineering topics. The community was great at giving advice. [Original Roadmap Post](https://www.reddit.com/r/dataengineering/comments/qpua91/data_engineering_road_map_for_a_computer_science/)

I have now completed my first data pipeline, data warehouse, and dashboard. The purpose of this project is to collect data about Rust cheaters. Ultimately, leading to insights about cheaters. I found some interesting insights. Read below!

&#x200B;

# Architecture

&#x200B;

https://preview.redd.it/vy557o1ttqg81.jpg?width=4096&format=pjpg&auto=webp&s=444b5f5e8653a585192731daf3e7d0b6848efd27

&#x200B;

# Overview

The pipeline collects tweets from a Twitter account(rusthackreport) that posts banned Rust player Steam profiles in real-time. The profile URLs are then extracted from the tweet data and stored in a temp s3 bucket. Ongoing, the steam profile URLs are used to extract the steam profile data via the Steam Web API. Lastly, the data is transformed and staged to be inserted into the fact and dim tables.

&#x200B;

# ETL Flow - Hourly

&#x200B;

https://preview.redd.it/ccmvqm9yuqg81.png?width=2493&format=png&auto=webp&s=faafbee1f525a1ae99198148883681b43c07c883

# Data Warehouse - Postgres

&#x200B;

https://preview.redd.it/ip4xj553vqg81.png?width=1796&format=png&auto=webp&s=1dc4f2d7aaaf8123f2576d12560c89cb08555834

# Data Dashboard

&#x200B;

[Dashboard Data Studio\(Updates Hourly\): https:\/\/datastudio.google.com\/u\/0\/reporting\/85aa118b-9def-48e4-8c88-b3db1e34e3ff\/page\/Ic8kC](https://i.redd.it/1fvsprz4vqg81.gif)

&#x200B;

# Data Insights

* The US has the most accounts banned for cheating with Russia trailing behind.
* Most cheaters have a level 1 steam account.
* The top 3 cheater names

1. 123
2. NeOn
3. xd

* The most common profile picture is the default steam profile picture.
* The majority of cheaters get banned between 0 and 10 hours.
* The top 3 games that cheaters own

1. **Counter-Strike: Global Offensive**
2. **PUBG: BATTLEGROUNDS**
3. **Apex Legends.**

* Top 3 Steam Groups

1. [Rustoria](https://steamcommunity.com/groups/rustoria)
2. [Andysolam](https://steamcommunity.com/groups/andysolam)
3. [Payday](https://steamcommunity.com/games/218620/memberslist/)

* Cheaters use [Archi's SC Farm](https://steamcommunity.com/groups/archiasf) to boost their accounts. It's a cheater's attempt to make their account look more legitimate to normal players.
* Profile Visibility - A lot of people believe if a profile is private it's a cheater. More cheaters have public profiles than private profiles.

1. Friends of Friends - 2,565
2. Private - 824
3. Friends Only - 133

You can look further at the [data studio link](https://datastudio.google.com/reporting/85aa118b-9def-48e4-8c88-b3db1e34e3ff).

&#x200B;

# Project Github

[https://github.com/jacob1421/RustCheatersDataPipeline](https://github.com/jacob1421/RustCheatersDataPipeline)

&#x200B;

&#x200B;

# Acknowledgment

# I want to thank Emily(mod#1073). She is a mod in the discord server for this subreddit! She was very helpful and went above and beyond when helping me with my data warehouse architecture. Thank you, Emily!

&#x200B;

Lastly, I would appreciate any constructive criticism. What technologies should I target next? Now that I have a project under my belt I will start applying.

[Help me by reviewing my resume?](https://www.reddit.com/r/dataengineering/comments/sotfp3/review_my_resume_please)",176,35,jacob1421,2022-02-09 06:04:40,https://www.reddit.com/r/dataengineering/comments/so6bpo/first_data_pipeline_looking_to_gain_insight_on/,False,False,False,False
12da1uw,I messed up today…,"Found out a query that I wrote was causing an issue with duplication. The duplication compounded therefore causing one of the tables to grow exponentially larger and larger each time. It’s also on a scheduled run every hour. Problem ended up costing almost $30k….

Anyone got any stories of when they fucked up?",175,86,burningburnerbern,2023-04-06 05:19:54,https://www.reddit.com/r/dataengineering/comments/12da1uw/i_messed_up_today/,False,False,False,False
wl2xqx,My attempt to explain the Cloud to my daughters,"I hope it's okay to share a passion project here.

I have been thinking a lot about Kafka and The Cloud. There's something magical about a place of unlimited resources, shared environments, and boundless cloud rivers, and I thought it was the perfect recipe for a fun children's book. So, I decided to create a Winnie-the-pooh inspired book on the subject: [https://a.walktothe.cloud/](https://a.walktothe.cloud/)

I thought it might be interesting to this community so wanted to share. I'm also curious if there have been any subjects you've wanted to translate for beginners or non-tech folks? I think it's an interesting and fun space, and wonder if others enjoy doing this, as well.",174,18,mitchum_,2022-08-10 17:11:45,https://www.reddit.com/r/dataengineering/comments/wl2xqx/my_attempt_to_explain_the_cloud_to_my_daughters/,False,False,False,False
zebb3o,The most shared resources from r/dataengineering,"Hi, fellow data engineers!

I've built a tool to find the best resources shared on r/dataengineering as well as other subreddits.Here is the link: [https://www.gembase.ai/search?q=data+engineering](https://www.gembase.ai/search?q=data+engineering)

**Architecture**

I gathered all the archive data from Reddit.

* Then extracted URLs with Go on a big EC2 machine.
* The screenshots and titles were scraped using Python + Playwright hosted on \~1000 ECS tasks.
* The recommendations were offline computed with R + Tidyverse.

I hope you will enjoy it. Feedback and questions are really appreciated.",174,22,flpezet,2022-12-06 16:38:45,https://www.reddit.com/r/dataengineering/comments/zebb3o/the_most_shared_resources_from_rdataengineering/,False,False,False,False
1b67xnz,Giving up data engineering,"Hi,

I've been a data engineer for a few years now and I just dont think I have what it takes anymore.

The discipline requires immense concentration, and the amount that needs to be learned constantly has left me burned out. There's no end to it.

I understand that every job has an element of constant learning, but I think it's the combination of the lack of acknowledgement of my work (a classic occurrence in data engineering I know), and the fact that despite the amount I've worked and learned, I still only earn slightly more than average (London wages/life are a scam). I have a lot of friends who work classic jobs (think estate agent, operations assistant, administration manager who earn just as much as I do, but the work and the skill involved is much less)

To cut a long story short, I'm looking for some encouragement or reasons to stay in the field if you could offer some. I was thinking of transitioning into a business analyst role or to become some kind of project manager, because my mental health is taking a big hit.

Thank you for reading.",173,82,Two_5536,2024-03-04 10:42:03,https://www.reddit.com/r/dataengineering/comments/1b67xnz/giving_up_data_engineering/,False,False,False,False
qw6htq,"After exactly one month and three days of starting my Azure Data Engineer preparation, I have good news!",,172,36,noNSFWcontent,2021-11-17 19:08:20,https://i.imgur.com/CsLFR9q.jpg,False,False,False,False
165hs7c,Influencers in data are doing no justice to the industry,"A few of them aside, most are writing stuff just to fill the gaps. Nothing meaningful, just piece after piece of barely important content.

The reason they somewhat ""succeed"":

1. Most of the data world isn't in the large companies or bleeding edge startups. They find these insightful because their world moves at a much slower pace. When you peel apart the content, anyone with slight experience will tell you there isn't much in there, but without that insight, you get sucked into this.
2. Somewhat related to 1. but most of the newbies in Data Engineering don't have great role models or people to follow. It's bound to be this way because the industry is only recently become popular and some people have taken advantage of that to position themselves as leaders.

Data Twitter, on the other hand, is much more cliquey. Guarding and almost gatekeeping their world. They don't even like the LinkedIn data influencers and sometimes even hate on people in other parts of Data Twitter too.

All of this just hurts the industry more. If you have stuff to share, just write, don't do this nonsense, and collectively pull down everyone else.  I hope the people putting in the real work to share content and not fluff get more of the limelight than these people.

Most of this isn’t new. A previous post highlighted this as well. https://www.reddit.com/r/dataengineering/comments/161zmp3/follow_up_on_my_previous_post_who_are_some_of_the/",172,124,None,2023-08-30 15:26:14,https://www.reddit.com/r/dataengineering/comments/165hs7c/influencers_in_data_are_doing_no_justice_to_the/,False,False,False,False
ymjubx,Cool ML Engineering diagram.,,172,13,DrRedmondNYC,2022-11-05 04:15:21,https://i.redd.it/77lc2zkhn3y91.png,False,False,False,False
kf3kr1,Apache Airflow 2.0 Released,,173,19,kaxil_naik,2020-12-17 18:53:22,https://i.redd.it/utnjqhdjns561.png,False,False,False,False
jhfg6p,"Long time Opensource users? Start contributing to Open Source projects, if you know Python (knowing Airflow is a plus), I can help you contribute your first commit to Airflow :) -- I am Apache Airflow Committer, PMC Member and Release Manager.",,170,26,kaxil_naik,2020-10-24 19:52:49,https://i.redd.it/qqedso6xk3v51.jpg,False,False,False,False
1bjcybi,Can We Stop Using Marketing Terms to Define Data Warehouses?,"This comes up a lot in random posts. Snowflake is a data warehouse. BigQuery is a data warehouse. PostgreSQL, MySQL, and SQL Server are not. We have let companies like Snowflake, Oracle, etc. redefine data warehouse from it's data-centric meaning, to a platform-centric one. 

A data warehouse is a collection of disparate sources modeled to provide efficient querying. Just about any DB system can be part a data warehouse solution, but the platform itself is not the data warehouse. Snowflake is a great solution for larger use cases where it saves significant engineering resources. For some tiny DW with rows in the low millions, it is probably going to be very expensive compared to other platforms. 

I know this sounds pedantic, but as data engineers, we should be precise with our terms. Doing anything else leads to confusion and misunderstandings. In the end, we should perform analysis and choose the best tool for the job. It very well might be one of the advertised ""data warehouses"". It may be Postgres. It may be something else. It's our job to find the right solution with hard data, not marketing hype.",171,53,leogodin217,2024-03-20 13:05:02,https://www.reddit.com/r/dataengineering/comments/1bjcybi/can_we_stop_using_marketing_terms_to_define_data/,False,False,False,False
1131jqq,"Finnhub streaming data pipeline using Spark, Kafka, Kubernetes and more - Github repo & more info in the comments",,168,64,LewWariat,2023-02-15 16:10:25,https://i.redd.it/44en1od0kdia1.png,False,False,False,False
wq4ims,New open-source notebook,"We're currently working on a new open-source notebook to shape the future of building data pipelines.

We would love for you to test out our current version in a collaborative effort to create better workflows for data scientists (and other data and machine learning professionals).

Repo: [https://github.com/mage-ai/mage-ai](https://github.com/mage-ai/mage-ai)  
More about Mage: [https://mage.ai](https://mage.ai/)  
Join our slack community: [https://mage.ai/chat](https://mage.ai/chat)",170,12,tchungry,2022-08-16 20:21:34,https://www.reddit.com/r/dataengineering/comments/wq4ims/new_opensource_notebook/,False,False,False,False
ojutua,"MySQL Deadlocks, Basically",,171,0,rdwn6610,2021-07-14 02:26:20,https://i.redd.it/e8i5s5fy93b71.jpg,False,False,False,False
mdpz25,"I wrote a tutorial on PySpark basics, how to use it in Google Colab, and some fine-tuning tips","Hello!

A few months back, I wrote a [PySpark tutorial](https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/) hoping it would be beneficial for folks looking for a quick ramp-up to using it. I got some positive feedback and so thought it would be a good idea to share it here so that more people can refer to it. I use it as a cheat sheet when I forget something, but the main objective of the tutorial is to:

* Gain a proper understanding of the most common PySpark functions available.
* A short introduction to Google Colab.
* Get some insight into tuning PySpark jobs.
* Answer a few common questions beginners usually have.

I used Google Colab as it is free, easy to set up, share, and convenient to use with other Google services like Google Drive. Please let me know if you have any comments about it or find it nifty! You can find the tutorial here:

[https://jacobceles.github.io/knowledge\_repo/colab\_and\_pyspark/](https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/)",170,20,jacobceles,2021-03-26 14:33:33,https://www.reddit.com/r/dataengineering/comments/mdpz25/i_wrote_a_tutorial_on_pyspark_basics_how_to_use/,False,False,False,False
t5dfo0,Anyone noticing a trend where Data Engineer jobs are advertised but the role is really BI or Analyst work?,"I’ve seen a few jobs now that either in the ad or on the phone call are very obviously not about building pipelines or infrastructure but straight up building reports in tableau? I’m talking 90% of your time is spent in tableau.

I’m assuming these places were struggling to get people for these roles which is why they’ve jazzed it up a bit. But wondering if anyone else has experienced this?",166,77,None,2022-03-02 23:36:06,https://www.reddit.com/r/dataengineering/comments/t5dfo0/anyone_noticing_a_trend_where_data_engineer_jobs/,False,False,False,False
uhifqj,Why does everyone want to work at FAANG?,"Can anyone tell me why this is such a big thing?   
Why is this the ultimate goal?   
Is it just the money or the prestige of working at Facebook, Amazon, Apple, Netflix or Google? 

I worked at a 350.000+ employees German company for over 10 years. What I found is that in large companies you are actually bound by a lot of processes and structures. You are a cog in the big machine.   
For instance I heard from engineers at Facebook that the tasks are very narrow and many are just glorified ETL developers.

Wouldn't it be a lot better to aim for a job with great opportunities, where you and your small team can actually move something? A job where you are responsible end to end and learn and use many skills?

I don't get it.",169,131,the_dataengineer,2022-05-03 15:19:31,https://www.reddit.com/r/dataengineering/comments/uhifqj/why_does_everyone_want_to_work_at_faang/,False,False,False,False
uxhmh0,"If you could only recommend one book to enhance your knowledge of data engineering, what would it be?","As the title states. I’ve heard about “designing data intensive applications” quite a bit, but are there any better comprehensive books?",170,63,None,2022-05-25 13:40:01,https://www.reddit.com/r/dataengineering/comments/uxhmh0/if_you_could_only_recommend_one_book_to_enhance/,False,False,False,False
16zm47c,What data engineering tools are popular right now?,"Hi All,

Just wondering what data engineering tool(ETL, warehouse, what have you) is most widely used these days. Seems every week i get distracted and try to learn some new tool, and i really want to narrow it down so i can be more focused. 

Seems that SQL is the only constant, but i know there's more to that. tia 

&#x200B;

&#x200B;",163,106,albertcuy,2023-10-04 12:37:26,https://www.reddit.com/r/dataengineering/comments/16zm47c/what_data_engineering_tools_are_popular_right_now/,False,False,False,False
1432zk2,How to become a good Data Engineer?,"I'm currently in my first job with 2 years of experience. I feel lost and I'm not as confident as I probably should be in data engineering.

What things should I be doing over the next few years to become more experienced and valuable as a Data Engineer?

- What is data engineering really about? Which parts of data engineering are the most important?
- Should I get experience with as many tools as possible, or focus on the most popular tools?
- Are side/personal projects important or helpful? What projects could I do for data engineering?

Any info would be great. There are so many things to learn that I feel paralyzed when I try to pick one.",167,61,KP_DaBoi99,2023-06-07 04:15:24,https://www.reddit.com/r/dataengineering/comments/1432zk2/how_to_become_a_good_data_engineer/,False,False,False,False
jcaflo,Awesome data engineering learning path,,163,37,SnirD,2020-10-16 14:07:14,https://awesomedataengineering.com/,False,False,False,False
13gln9a,My top 14 tips for Snowflake Data Engineers. What would you add?,"If you're working on **Snowflake** as a Data Engineer, this article might be interesting:

[Snowflake Top Tips for Data Engineers - Loading and Transforming Data](https://www.analytics.today/blog/top-14-snowflake-data-engineering-best-practices)

**Quote:**  

>*If the only tool you have is a hammer - you tend to see every problem as a nail.*  Abraham Maslow.

**In Summary**

1. **Follow the standard ingestion pattern:**  This involves the multi-stage process of landing the data files in cloud storage and loading them to a landing table before transforming the data.  Breaking the overall process into predefined steps makes it easier to orchestrate and test.  
2. **Retain history of raw data:**  Unless your data is sourced from a raw data lake, it makes sense to keep the raw data history which should ideally be stored using the [VARIANT](https://docs.snowflake.com/en/sql-reference/data-types-semistructured.html#variant) data type to benefit from automatic schema evolution.  This means you can truncate and re-process data if bugs are found in the transformation pipeline and provide an excellent raw data source for Data Scientists.  While you may not yet have any machine learning requirements yet, it's almost certain you will, if not now, then in the coming years.  Remember that Snowflake data storage is remarkably cheap, unlike on-premises solutions. 
3. **Use multiple data models:**   On-premises data storage was so expensive it was not feasible to store multiple copies of data, each using a different data model to match the need.  However, using Snowflake, it makes sense to store raw data history in either structured or variant format, cleaned and conformed data in [3rd Normal Form](https://dwbi1.wordpress.com/2011/03/28/storing-history-on-3rd-normal-form/) or using a [Data Vault](https://www.analytics.today/blog/when-should-i-use-data-vault) model. Finally, data is ready for consumption in a [Kimball Dimensional Data model](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/).  Each data model has unique benefits, and storing the results of intermediate steps has huge architectural benefits, not least the ability to reload and reprocess the data in case of mistakes.
4. **Use the right tool:**  As the quote above implies, if you only know one tool, you'll use it inappropriately some times.  The decision about which tool to use should be based upon a range of factors, including, the existing skill set in the team, whether you need rapid near real-time delivery, and whether you're doing a once-off data load or a regular repeating process.  Be aware that Snowflake can natively handle a range of file formats, including Avro, Parquet, ORC, JSON and CSV. There is extensive guidance on [loading data into Snowflake](https://docs.snowflake.com/en/user-guide-data-load.html#loading-data-into-snowflake) on the online documentation.
5. **Use COPY or SNOWPIPE to load data:**  Around 80% of data loaded into a data warehouse is either ingested using a regular batch process or, increasingly, immediately after the data files arrive.  By far, the fastest, most cost-efficient way to load data is using COPY and SNOWPIPE, so avoid the temptation to use other methods (for example, queries against external tables) for regular data loads.  Effectively, this is another example of *using the right tool*.  
6. **Avoid JDBC or ODBC for regular large data loads:**  Another *right tool* recommendation.  While a JDBC or ODBC interface may be fine to load a few megabytes of data, these interfaces will not scale to the massive throughput of COPY and SNOWPIPE.  Use them by all means, but not for large regular data loads.
7. **Avoid Scanning Files:** Using the COPY command to ingest data, use [partitioned staged data](https://docs.snowflake.com/en/user-guide/data-load-considerations-manage.html#partitioning-staged-data-files) files.  This reduces the effort of scanning large numbers of data files in cloud storage.
8. **Choose a suitable Virtual Warehouse size:**  Don’t assume an X6-LARGE warehouse will load huge data files any faster than an X-SMALL.  Each physical file is loaded sequentially on a single CPU, and it is more sensible to load most loads on an X-SMALL warehouse.  Consider splitting massive data files into 100-250MB chunks and loading them on a larger (perhaps MEDIUM size) warehouse.
9. **Ensure 3rd party tools push down:**  ETL tools like Ab Initio, Talend and Informatica were originally designed to extract data from source systems into an ETL server, transform the data and write them to the warehouse.  As Snowflake can draw upon massive on-demand compute resources and automatically scale out, it makes no sense to use and have data copied to an external server.  Instead, use the ELT (Extract, Load and Transform) method, and ensure the tools generate and execute SQL statements on Snowflake to maximise throughput and reduce costs.  Excellent examples include [DBT](https://www.getdbt.com/) and [Matillion](https://www.matillion.com/).
10. **Transform data in Steps:**  A common mistake by inexperienced data engineers is to write huge SQL statements that join, summarise and process lots of tables in the mistaken belief this is an efficient way of working.  In reality, the code becomes over-complex, difficult to maintain, and, worst still, often performs poorly.  Instead, break the transformation pipeline into multiple steps and write results to intermediate tables.  This makes it easier to test intermediate results, simplifies the code and often produces simple SQL code that runs faster.  
11. **Use Transient tables for intermediate results:**  During a complex ELT pipeline, write intermediate results to a [transient table](https://docs.snowflake.com/en/user-guide/tables-temp-transient.html#transient-tables) which may be truncated before the next load.  This reduces the time-travel storage to just one day and avoids an additional seven days of fail-safe storage.  By all means, use [temporary tables](https://docs.snowflake.com/en/user-guide/tables-temp-transient.html#temporary-tables) if sensible, but the option to check the results of intermediate steps in a complex ELT pipeline is often helpful.
12. **Avoid row-by-row processing:**  As described in the article on [Snowflake Query Tuning](https://www.analytics.today/blog/top-3-snowflake-performance-tuning-tactics), Snowflake is designed to ingest, process and analyse billions of rows at amazing speed.  This is often referred to as *set-at-a-time processing.*  However, people tend to think about *row-by-row processing*, which sometimes leads to programming loops that fetch and update rows one at a time.  Be aware that row-by-row processing is the biggest way to kill query performance.  Use SQL statements to process all table entries simultaneously and avoid row-by-row processing at all costs.
13. **Use Query Tags:**  When you start any multi-step transformation task, set the [session query tag using](https://docs.snowflake.com/en/sql-reference/sql/alter-session.html#alter-session):  **ALTER SESSION SET QUERY\_TAG = 'XXXXXX'** and **ALTER SESSION UNSET QUERY\_TAG**.  This stamps every SQL statement until reset with an identifier and is invaluable to System Administrators.  As every SQL statement (and QUERY\_TAG) is recorded in the [QUERY\_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/query_history.html#query-history-view) view, you can track the job performance over time.  This can be used to quickly identify when a task change has resulted in poor performance, identify inefficient transformation jobs or indicate when a job would be better executed on a larger or smaller warehouse.
14. **Keep it Simple:**  Probably the best indicator of an experienced data engineer is the value they place on ***simplicity***.  You can always make a job 10% faster, generic, or more elegant, and it *may* be beneficial, but it's *always* beneficial to simplify a solution.  Simple solutions are easier to understand, easier to diagnose problems and are therefore easier to maintain.  Around 50% of the performance challenges I face are difficult to resolve because the solution is a single, monolithic complex block of code.  The first thing I do is break down the solution into steps and only then identify the root cause.  

Would you agree with the above?  What would you add?",165,46,JohnAnthonyRyan,2023-05-13 16:15:07,https://www.reddit.com/r/dataengineering/comments/13gln9a/my_top_14_tips_for_snowflake_data_engineers_what/,False,False,False,False
13fmd1h,How do you handle junior people who are better than you in terms of technical ability?,"I try to be super open and receptive to feedback if someone has a better  more efficient way of doing things. I take notes obsessively or ask plenty questions.

When we do a code review someone who more junior than me corrects my code or says it can be improved somewhere. 

It isn’t overly pedantic corrections that junior people get into. However I do feel that little bit of insecurity that you aren’t good enough technically or that you lose respect if don’t shine in your logic.  

You can’t be a chef and not know how to cook a meal.

One thing I find, is it lights a bit of fire in your belly. If you’re expected to be a team lead you cannot be bad technically. It pushed self development.

How do you manage junior correcting you without coming across as overly submissive and still have their respect?",168,90,hositir,2023-05-12 14:12:23,https://www.reddit.com/r/dataengineering/comments/13fmd1h/how_do_you_handle_junior_people_who_are_better/,False,False,False,False
1aw7368,Open source DBT core alternative written in Rust (30x faster),"Hey everyone,

**TLDR:** Louis here from Quary - we have spent the past few months re-engineering DBT core (python) into rust to create a fast data transformation (SQL inference & modelling) package in Rust.

With DBT Core (Data Build Tool) you would need to spin up a server to run Python to make the package work. Thanks to Rust WASM we are able to make the transformation engine portable so that it runs entirely in the browser.

We wanted to give back to this community so we have decided to Open Source the entire project under an MIT license to give back to this community.

Looking forward to hearing your thoughts in the comments! (A GitHub star is always appreciated 😃)

***EDIT: adding clarification***

**Quary will be easier to build on top of**

Because of our Rust core, we can expose JS, Python, and other bindings, making it easier to build additional tooling on top of Quary. For example, we've built our ""cloud"" offering on a WASM compilation.

**Column-Level Lineage**

The Quary core contains column-level lineage directly and this enables us to offer unique capabilities. For instance:

**Automated Inference and Documentation**

Our system can intelligently infer tests and generate documentation, significantly reducing your manual workload.

**Testing Efficiency**: By avoiding inferrable tests, we can skip tests that Quary knows to be true. In our template for example, we can skip around 1/3 of tests.

**Quary is better for handling sensitive data like PII.**

Because we can compile to WASM, our ""cloud offering"" interacts with your data warehouse from the client. Data doesn't flow through any of our servers, protecting your information.

**Speed-up for developer experience**

While the impact on database performance is minimal, one of the true benefits of Quary's fast core is developer experience. The fast core allows us to build experiences where we can provide faster feedback.

https://preview.redd.it/3kisasoefwjc1.jpg?width=3612&format=pjpg&auto=webp&s=38666d5d348282797edbc19aa69d0df040fc3ed5",161,73,Background_Call6280,2024-02-21 08:25:56,https://www.reddit.com/r/dataengineering/comments/1aw7368/open_source_dbt_core_alternative_written_in_rust/,False,False,False,False
10sxj6r,Why do all my BI initiatives end up like this? 😩,,164,22,Salmon-Advantage,2023-02-03 22:36:33,https://v.redd.it/3miuaya2e3ga1,False,False,False,False
xl4sag,All-in-one tool for data pipelines!,"Our team at [Mage](https://mage.ai) have been working diligently on this new open-source tool for building, running, and managing your data pipelines at scale.

https://preview.redd.it/tn5w1wur4gp91.png?width=4336&format=png&auto=webp&s=d59e720ce9a68bc416896ef6b14c357a0b452abd

Drop us a comment with your thoughts, questions, or feedback!

Check it out: [https://github.com/mage-ai/mage-ai](https://github.com/mage-ai/mage-ai)  
Try the live demo (explore without installing): [http://demo.mage.ai](http://demo.mage.ai)  
Slack: [https://mage.ai/chat](https://mage.ai/chat)

Cheers!",162,37,tchungry,2022-09-22 15:38:53,https://www.reddit.com/r/dataengineering/comments/xl4sag/allinone_tool_for_data_pipelines/,False,False,False,False
17fr8d5,To my data engineers: why do you like working as a data engineer?,What made you get into data engineering and what is keeping you as one? I recently started self learning to become one but i’m sure learning about data engineering is much different than actually being an engineer. Thanks,164,166,naq98,2023-10-24 23:51:59,https://www.reddit.com/r/dataengineering/comments/17fr8d5/to_my_data_engineers_why_do_you_like_working_as_a/,False,False,False,False
15gzgne,Polars gets seed round of $4 million to build a compute platform,,165,51,mailed,2023-08-03 09:40:24,https://www.pola.rs/posts/company-announcement/,False,False,False,False
zq4eg6,explaining what the data modeler on the team does,,165,5,DiceboyT,2022-12-19 21:39:30,https://i.redd.it/4svii7iyty6a1.gif,False,False,False,False
ufl9tx,Apache Airflow 2.3.0 is out !,"&#x200B;

https://preview.redd.it/dzqcw8txnqw81.png?width=914&format=png&auto=webp&s=e528d30f4107b723a2ace301375a03f348df340b

Apache Airflow 2.3.0 is out! Soo many things to talk about 👇👇👇

➡️ This is the biggest **Apache Airflow** release since 2.0.0

➡️ 700+ commits since 2.2 including 50 new features, 99 improvements, 85 bug fixes 

The following are the biggest & noteworthy changes👇👇👇:

👉 Dynamic Task Mapping: [https://airflow.apache.org/docs/apache-airflow/2.3.0/concepts/dynamic-task-mapping.html](https://airflow.apache.org/docs/apache-airflow/2.3.0/concepts/dynamic-task-mapping.html)

👉 Grid View replaces Tree View

👉 The new \`airflow db clean\` CLI command for purging old records

👉 First class support for DB downgrade - \`airflow db downgrade\` command - [https://airflow.apache.org/docs/apache-airflow/2.3.0/usage-cli.html#downgrading-airflow](https://airflow.apache.org/docs/apache-airflow/2.3.0/usage-cli.html#downgrading-airflow)

👉 New Executor: LocalKubernetesExecutor

👉 Create Connection in native JSON format - no need to figure out the URI format

👉 And a new ""SmoothOperator"" -- This is a surprise ! And a very powerful feature, try it out and let me know what you think about it 😃

&#x200B;

📦 PyPI: [https://pypi.org/project/apache-airflow/2.3.0/](https://pypi.org/project/apache-airflow/2.3.0/)

📚 Docs: [https://airflow.apache.org/docs/apache-airflow/2.3.0](https://airflow.apache.org/docs/apache-airflow/2.3.0)

🛠️ Changelog: [https://airflow.apache.org/docs/apache-airflow/2.3.0/release\_notes.html](https://airflow.apache.org/docs/apache-airflow/2.3.0/release_notes.html)

🚢 Docker Image: ""docker pull apache/airflow:2.3.0""

🚏 Constraints: [https://github.com/apache/airflow/tree/constraints-2.3.0](https://github.com/apache/airflow/tree/constraints-2.3.0)

&#x200B;

\------

Details around the features

&#x200B;

**👉 Dynamic Task Mapping: No longer hacking around dynamic tasks !!**

Allows a way for a workflow to create a number of tasks at runtime based upon current data, rather than the DAG author having to know in advance how many tasks would be needed.

[https://airflow.apache.org/docs/apache-airflow/2.3.0/concepts/dynamic-task-mapping.html](https://airflow.apache.org/docs/apache-airflow/2.3.0/concepts/dynamic-task-mapping.html)

&#x200B;

https://preview.redd.it/sgn12pn6oqw81.png?width=914&format=png&auto=webp&s=283e0076f6de0b960bd36f7c481a9b8b54a8c918

👉 **Grid View replaces Tree View!!**

Show runs and tasks but leave dependency lines to the graph view and handles Task Groups better!

Paves way for DAG Versioning - to easily show versions, which was impossible to handle in Tree View ! yay!

PR: [https://github.com/apache/airflow/pull/18675](https://github.com/apache/airflow/pull/18675)

&#x200B;

https://preview.redd.it/36dn2tgboqw81.png?width=2384&format=png&auto=webp&s=39e279496d955a63ed6750a89ca7b96cb5d77b0e

https://preview.redd.it/2s5pqsgboqw81.png?width=1576&format=png&auto=webp&s=cda96e655fb4c715c0078be21599e523e2d3100d

&#x200B;

👉 **Create Connection in native JSON format - no need to figure out the URI format**

&#x200B;

https://preview.redd.it/5gjjcg0eoqw81.png?width=956&format=png&auto=webp&s=572745d6464e8439bb84f93587738b8ac8dca862

**👉 First class support for DB downgrade - \`airflow db downgrade\` command -** 

You can downgrade to a particular Airflow version or a to a specific Alembic revision id.

Includes a ""--show-sql-only"" to output all the SQL so that you can run it yourself!

[https://airflow.apache.org/docs/apache-airflow/2.3.0/usage-cli.html#downgrading-airflow](https://airflow.apache.org/docs/apache-airflow/2.3.0/usage-cli.html#downgrading-airflow)

&#x200B;

https://preview.redd.it/l4cibc2joqw81.png?width=2048&format=png&auto=webp&s=dceae470c316c61b67935c4ec6d632369b6504e7

**👉 The new \`airflow db clean\` CLI command for purging old records.** 

This will help reduce time when running DB Migrations (when updating Airflow version)

No need to use Maintenance DAGs anymore!

&#x200B;

https://preview.redd.it/x3f8jz4moqw81.png?width=2048&format=png&auto=webp&s=8eaca8a761232ce1fd7db94f66d7af7ce5766f13

**👉 New Executor: LocalKubernetesExecutor**

It provides the capability of running tasks with either LocalExecutor, which runs tasks within the scheduler service, or with KubernetesExecutor, which runs each task

in its own pod on a kubernetes cluster based on the task's queue

&#x200B;

**👉 DagProcessorManager can be run as standalone process now.** 

As it runs user code, separating it from the scheduler process and running it as an independent process in a different host is a good idea.

Run it with ""airflow dag-processor"" CLI coomand

📚 [https://airflow.apache.org/docs/apache-airflow/2.3.0/configurations-ref.html#standalone\_dag\_processor](https://airflow.apache.org/docs/apache-airflow/2.3.0/configurations-ref.html#standalone_dag_processor)

&#x200B;

👉 A single page to check release notes instead of UPDATING.md on GitHub  & Changelog on Airflow website: https://airflow.apache.org/docs/apache-airflow/2.3.0/release\_notes.html 

👉 And a new ""**SmoothOperator**"" - ""from airflow.operators.smooth import SmoothOperator""

This is a surprise! And a very powerful feature, try it out and let me know what you think about it 😃",159,17,kaxil_naik,2022-04-30 22:07:54,https://www.reddit.com/r/dataengineering/comments/ufl9tx/apache_airflow_230_is_out/,False,False,False,False
ueiklm,I did it!,"I've landed a data engineering job in my current company! I'm currently working as a marketing data analyst at a bank. After deciding on becoming a data engineer about a year ago, and lurking on this sub for even longer, I have finally succeeded.

How I did it?

* A little over a year ago **I expressed my desire to become a data engineer** to my manager and was very transparent about what I like most about my job (transforming data, building dashboards, writing SQL, etc.) and what I didn't like (generating insights, marketing fluff). As my manager is all about doing what you love to do as much as possible, my transparency set me up to be able to focus on projects that would play into my skills and ambitions.
* **I looked out for projects that have to do with data engineering**. My team is currently migrating to Azure stack for all data analytics. My team was in need of someone who was able to have a technical conversation with IT/Data Engineers about my team's requirements, so I've put myself forward. I've learned so much from just talking to other data engineers. More important, this is basically where I got my foot in the door.
* **Where did I get the knowledge to have any sort of conversation about data engineering? Read, read, read.** This sub (thank you all!), books (e.g., Designing Data-Intensive Applications), and blogs. But also by just trying stuff out myself. Tinkering with PostgreSQL, docker, Airflow, etc. I know everyone is very keen on building big projects for your portfolio, but my attention span in my spare time is too limited to focus on 1 big project for too long. So I did little things like running a Postgres database locally, extracting data from an api, trigger some python scripts with Airflow.
* **Certificates!** This was an important one, and was recommend by someone from this sub. As my company is on Azure, I got the Azure Fundamentals, Azure Data Fundamentals and Azure Data Engineer Associate certificates. They not only thought me a lot about Azure, but also about the cloud in general and other important data engineering concepts.
* **Start reaching out and get interviews.** This was the easiest part. One major selling point is that I was an analyst first, and therefore know how data and platform will be used from a user/analyst perspective. Any technical shortcomings are easy to overcome. Also, being able to speak both engineering/it language and analyst/end-user language was appreciated. Apparently, engineers at my company only speak engineering. Lucky me!

I hope this is of use to anyone! Either way, big thanks to this sub as it guided me every step of the way.",165,19,chonbee,2022-04-29 10:05:36,https://www.reddit.com/r/dataengineering/comments/ueiklm/i_did_it/,False,False,False,False
sit0ep,"Data engineers who've cracked FAANG and other top tech companies (Adobe, Microsoft etc)..how did you do it?","1. What are the data engineering specific skills you've developed and what resources have you used? 

2. What do you put down on your resume? Are Cloud certs important?

3. What are the technologies you are working on right? How accurate was the job description when compared to what actually constitutes your daily tasks?",163,59,None,2022-02-02 16:34:10,https://www.reddit.com/r/dataengineering/comments/sit0ep/data_engineers_whove_cracked_faang_and_other_top/,False,False,False,False
gq2bmf,Data Engineering project for beginners,"Hi all,

Recently I saw a post on this sub reddit asking for beginner DE projects using common cloud services and DE tools. I have been asked this same question by my friends and colleagues who are trying to move into the data engineering field. So I decided to write a blog post explaining how to setup and build a simple batch based data processing pipeline using Airflow and AWS.

Initially I wanted to do it with both batch and streaming pipelines, but it soon got out of hand so decided to only do batch based first and depending on interest will do stream processing.

Blog: [https://www.startdataengineering.com/post/data-engineering-project-for-beginners-batch-edition](https://www.startdataengineering.com/post/data-engineering-project-for-beginners-batch-edition)

Repo: [https://github.com/josephmachado/beginner\_de\_project](https://github.com/josephmachado/beginner_de_project)

Appreciate any questions, feedback, comments. Hope this helps someone.",161,34,joseph_machado,2020-05-25 02:05:55,https://www.reddit.com/r/dataengineering/comments/gq2bmf/data_engineering_project_for_beginners/,False,False,False,False
18cj54r,Keep in mind the following when reading about anything tech online lol,,161,17,NFeruch,2023-12-07 00:47:01,https://i.redd.it/ny2e7tayrr4c1.png,False,False,False,False
y8o1sy,Why use Spark at all?,"It  has been 5 years working in data space, I always found something better to solve a problem at hands than Spark. If I do intensive data application, probably the data would pipe through a distributed message queue like Kafka and then I will have a cluster to ingestion sequentially the data into the application -> no need for spark. If I wanna do real time analytics I would use something like Flink.  


If I want to do transformation, it's sql and dbt. I even go as far as using Trino or Presto to ingest csv directly (given that we need to have the schema anyways)  


I found no need for Spark, and I have no issues, sometimes I wonder why everyone is using spark, what is the point of it all.  


Ye that's all.",161,141,noobgolang,2022-10-20 04:17:12,https://www.reddit.com/r/dataengineering/comments/y8o1sy/why_use_spark_at_all/,False,False,False,False
166ah28,"Instacart, Databricks and Snowflake drama","Was reading an interesting blog post about how instacart migrated to databricks that mysteriously disappeared: https://www.instacart.com/company/how-its-made/how-instacart-ads-modularized-data-pipelines-with-lakehouse-architecture-and-spark/
I went looking for info and found some twitter threads and it
turns out instacart had saved about 70% on snowflake costs in 2023 by migrating to databricks. Databricks even advertised the [case study](https://pbs.twimg.com/media/F4s0qIcXYAAuvYD?format=jpg&name=medium) . One problem though, snowflakes CEO sits on instacarts board, which means a normally transparent blog had to delete its findings.

[Quote:](https://twitter.com/GergelyOrosz/status/1697192807801184561)
>Instacart cut Snowflake spend by 70% in 2023, while starting to migrate ETL loads to Databricks - then deletes blog post detailing migration. I email Instacart press team with questions but Snowlake press team comes back with a comment on behalf of Instacart 🤯. Snowflake’s CEO is on the board of directors for Instacart. The thing that blew my mind is how my email addressed only to Instacart’s press team ended up at Snowflake (who I never contacted) and why Snowflake makes/can make definite statements on behalf of Instacart. Emailed Instacart, and then Snowflake press team landed in my inbox referencing things that I only sent to Instacart, saying they hear I am writing an article and they want to give me facts. Never contacted them. Feels like Instacart pinged them.


So now databricks removed the [case study](https://pbs.twimg.com/media/F4s0qIYXAAArUkz?format=jpg&name=large) and snowflake even posted a [response](https://www.snowflake.com/blog/snowflake-and-instacart-the-facts/) which says its countering 'social media' misinformation but most of the details came from putting two and two together with instacarts own blog post.

Stumbled upon some drama just reading a tech blog, I have a feeling Instacarts tech blog team is getting a serious talking to and now will have to pass anything they post by the board. It was a really good post though, detailed and well thought out, I was looking to share the info with my team today.

Threads here: [1](https://twitter.com/GergelyOrosz/status/1696435748071772333) [2](https://twitter.com/modestproposal1/status/1695177654822191184) [3](https://twitter.com/GergelyOrosz/status/1697192807801184561)",159,61,TerriblyRare,2023-08-31 12:58:14,https://www.reddit.com/r/dataengineering/comments/166ah28/instacart_databricks_and_snowflake_drama/,False,False,False,False
13gckaq,Snowflake SELECT * EXCLUDE,"So, i feel like a caveman discovering fire.  Apologies if this is something which is generally know, but just in case this helps anyone else with a typical SQL limitation.

I've just found out that on Snowflake you can do a ""SELECT *"" query and exclude specific column. for example:

 SELECT * EXCLUDE (field1, field2...) from tableName;

 I feel like ive been wanting this for YEARS in SQL but didn't know Snowflake had it! 

For me personally, It works as a shorthand way of making checksums for very wide tables when I'm having to implement SCD2/CDC without a usable date check field. Such as:

SELECT id, HASH(*) AS checksum FROM (SELECT * EXCLUDE (field1, fields2...) FROM tableName );",161,53,andyby2k26,2023-05-13 09:16:28,https://www.reddit.com/r/dataengineering/comments/13gckaq/snowflake_select_exclude/,False,False,False,False
yqohhm,Anyone here being let go by Meta?,"11,000 is a big number. I just wonder if any DE's are being let go. If so, whats your plan moving forward? Looks like the severance package is good though.",163,102,w_savage,2022-11-09 16:53:46,https://www.reddit.com/r/dataengineering/comments/yqohhm/anyone_here_being_let_go_by_meta/,False,False,False,False
qfiz51,I deleted data from production,"Over the last 7 years I've done some mistakes, as a therapeutic post I'm trying to write my experience about it to maybe help others but also to help me. Below an overlook of the stories.



# 1 — First I removed /usr 👤

It was late 2014. I was setuping an Hadoop cluster with Ambari. Everything was ok until I removed the `/usr` folder because the Namenode main partition was full and I was copy-pasting solutions from the internet in order to solve by myself rather than asking IT help.



What happened next? When you remove /usr you lost all the binaries and even if you can get back the sudo command is not usable anymore because it needs to belong to uid:0. So it's done you are locked outside of the server if you don't have root user access. 




# 2 — Then I removed /data in hdfs 📉

Around 2017. I had a working Hadoop cluster with Hive on Tez that was working like a charm. One Sunday I wake up and do a Slack check over the alerts channel and see that everything failed over the weekend.



After a small deep-dive I discover that the `/data` folder is missing in hdfs. What a weird issue. Why the `/data` folder is missing? I'm then asking to the team if they changed something to the cluster configuration. But as I go deeper I discover that I was responsible of the `hdfs dfs -rm /data` command run. Outch. 



Even today I don't know what happened this day and it took me around 3 days to get 60% of the data back. The rest was lost for ever. I learnt a lot from this situation.




# 3 — A colleague ran terraform destroy 💣

Someone from the team I was managing ran terraform destroy and we lost a GKE cluster, all our GCE instances and some SQL instances. We noticed the issue because someone asked in Slack: ""Is Metabase down?"" and then I looked over my shoulder and saw my colleague screen fully red with the terraform destroy command at the top.


By chance we did not lose any data because our buckets and BigQuery were not managed by terraform, it took us around 4 hours to get everything back up again.


# Conclusion

This post is a adapted version of a [longer version in my blog](https://www.blef.fr/data-deleted-from-production/) where I put some takeaways. I also want to start a discussion to say that this is normal to make mistakes. What is important is how you deal with it and how you learn from it.


Much love ❤️.",160,26,blef__,2021-10-25 15:16:43,https://www.reddit.com/r/dataengineering/comments/qfiz51/i_deleted_data_from_production/,False,False,False,False
11eezyq,"Brace yourselves... ""professional"" Data Mesh developer job ads incoming!",,156,100,whichalps,2023-02-28 17:50:41,https://i.redd.it/u0xw0g77wyka1.jpg,False,False,False,False
o9w1mx,today I started in my new role as an ETL Developer after 5 years as a data analyst,I’m just very happy and just wanted to share!,161,57,jolllof,2021-06-28 23:26:11,https://www.reddit.com/r/dataengineering/comments/o9w1mx/today_i_started_in_my_new_role_as_an_etl/,False,False,False,False
1b34q4i,I bombed the interviuw and feel like the dumbest person in the world,"I (M20) just had a second round of 1 on 1 session for data engineer trainee in a company. 

I was asked to reverse a string in python and I forgot the syntax of while loop. And this one mistake just put me in a downward spiral for the entire hour of the session. So much so that once he asked me if two null values will be equal and I said no, and he asked why but I could not bring myself to be confident enough to say anything about memory addresses even after knowing about it, he asked me about indexing in database and I could only answer it in very simple terms.

I feel really low right now, what can I do to improve and get better at interviewing.",158,99,pmme_ur_titsandclits,2024-02-29 16:39:33,https://www.reddit.com/r/dataengineering/comments/1b34q4i/i_bombed_the_interviuw_and_feel_like_the_dumbest/,False,False,False,False
17p20y6,Why don't a lot of data engineers consider themselves software engineers?,"During my time in data engineering, I've noticed a lot of data engineers discount their own experience compared to software engineers who do not work in data.  Do a lot of data engineers not consider themselves a type of software engineer?



I find that strange, because during my career I was able to do a lot of work in python, java, SQL, and Terraform.  I also have a lot of experience setting up CI/CD pipelines and building cloud infrastructure.  In many cases, I feel like our field overlaps a lot with backend engineering.",158,158,level_126_programmer,2023-11-06 12:51:04,https://www.reddit.com/r/dataengineering/comments/17p20y6/why_dont_a_lot_of_data_engineers_consider/,False,False,False,False
15y7v97,What do Data Engineers do after the data platform is completely setup and automated?,"I lead a small DE team at a medium sized retailer. Over the years we’ve setup the data platform on GCP, lambda architecture feeding data into BQ with DBT transforms. We’re reaching a point where we’ve pretty much ingested all the data sources used by the business, completed the data models, reverse ETL and automation CI/CD. I’m starting to draw a blank on what business focussed initiatives we should start taking up at this mature stage and would appreciate any thoughts from this community. 

Some things I’m considering are improving data quality checks, improving the data catalog and adding more metrics to the semantic layer. 

We’ve already done several rounds of optimisation and cloud cost control so I don’t see much more opportunity there. Most of our data usage in the business is batch focused including ML model training so we don’t really see the need to move to a kappa architecture either.",154,137,Hackerjurassicpark,2023-08-22 14:43:23,https://www.reddit.com/r/dataengineering/comments/15y7v97/what_do_data_engineers_do_after_the_data_platform/,False,False,False,False
ykbtnb,I'm sorry but is that low end of the salary range a joke? Especially for a big company like CVS .I made more at my very first analyst job which was all Excel/Access,,155,105,DrRedmondNYC,2022-11-02 17:46:34,https://i.redd.it/b6psfp5k9mx91.png,False,False,False,False
rw929w,5 Data Engineering Projects To Add To Your Resume - Seattle Data Guy,,157,13,SeattleDataGuy,2022-01-05 00:23:33,https://www.theseattledataguy.com/5-data-engineering-projects-to-add-to-your-resume/,False,False,False,False
1bv6cm9,"Better way to query a large (15TB) dataset that does not cost $40,000","# UPDATE

Took me a while to get back to this post and update what I did, my bad! In the comments to this post, I got multiple ideas, listing them down here and what happened when I tried them:

* ***(THIS WORKED)*** Broadcasting the smaller CSV dataset; I set spark's broadcast threshold to be 200 MB (CSV file was 140 MB, went higher for good measure) `spark.conf.set(""spark.sql.autoBroadcastJoinThreshold"", 200 * 1024 * 1024)` . then, I converted from spark SQL to dataframe API `big_patient_df.join(broadcast(control_patients_df),big_patient_df[""patient_id""] == control_patients_df[""control""],""left_semi"")`. This ran under 7 minutes on a 100 DPU AWS Glue job which cost me just around $14! WITHOUT the broadcast, a single subset of this would need 320DPU and run for over 3 hours costing $400. Also, the shuffle used to go as high as 400GB across the cluster but after using the broadcast, the shuffle went down to ZERO! thanks u/johne898.
* Use Athena to query the dataset: I first wrote the DDL statements to define the CSV file as an external table and also defined the large parquet dataset as an external table as well. I wrote an inner join query as follows `SELECT * FROM BIG_TRANSACTION_TABLE B INNER JOIN CUSTOMER_LIST_TABLE C ON B.CUSTOMER_ID = C.CUSTOMER_ID`. Athena was able to scan up to 400GB of data and then it failed due to timeout after 30 mins. I could've requested a quota increase but seeing that it couldn't scan even half the dataset I thought that to be futile.
* ***(THIS ALSO HELPED)*** Use inner/semi join instead of doing a subquery: I printed the execution plan of the original subquery, inner join, as well as semi join. The spark optimizer converts the subquery into an inner join by itself. However, the semi join is more efficient since we just need to do an existence check in the large dataset based on the ids in the smaller CSV file.
* Bucketing by the join field: Since the cardinality was already high of the join field and this was the only query to be run on the dataset, the shuffle caused by the bucketing did not make much difference.
* Partitioning the dataset on the join key: big nope, too high of a cardinality to make this work.
* Special mention for u/xilong89 for his Redshift LOAD approach that he even benchmarked for me! I couldn't give it a shot though.

# Original post

Hi! I am fairly new to data engineering and have been assigned a task to query a large 15TB dataset stored on AWS S3. Any help would be much appreciated!

**Details of the dataset**

The dataset is stored on S3 as parquet files and contains transaction details of 300M+ customers, each customer having \~175 transactions on average. The dataset contains columns like customer\_id, transaction\_date, transaction\_amount, etc. There are around 140k parquet files containing the data. (EDIT: customer\_id is varchar/string)

Our data analyst has come up with a list of 10M customer id that they are interested in, and want to pull all the transactions of the these customers. This list of 7.5M customer id is stored as a CSV file of 200MB on S3 as well.

Currently, they are running an AWS Glue job where they are essentially loading the large dataset from the AWS Glue catalog and the small customer id list cut into smaller batches, and doing an inner join to get the outputs.

EDIT: The query looks like this

`SELECT * FROM BIG_TRANSACTION_TABLE WHERE CUSTOMER_ID IN (SELECT CUSTOMER_ID FROM CUSTOMER_LIST_TABLE where BATCH=4)`

However, doing this will run a bill close to $40,000 based off our calculation.

What would be a better way to do this? I had a few ideas:

1. create an EMR cluster and load the entire dataset and do the query
2. broadcast the csv file and run the query to minimize shuffle
3. Read the parquet files in batches instead of AWS Glue catalog and run the query.",153,163,sarkaysm,2024-04-03 22:26:10,https://www.reddit.com/r/dataengineering/comments/1bv6cm9/better_way_to_query_a_large_15tb_dataset_that/,False,False,False,False
16nlvln,A senior engineer's experience in the current job market,"A lot of posts I see  are around how bad the market is / how tough it is right now / putting out 100s of resumes / leet-coding / not getting any responses, so I'd like to submit another data point. 

For context: I've been affected by layoffs twice over the past 2 years, so I've been in ""this market"" twice. I have non-faang bay area experience on my resume, 5-10 years of experience, and typically apply for senior / staff type roles. 

Both times I've entered the market were pretty much the same: find 2-4 companies to apply to (I didn't rely on connections -- but that's typically the best way --, these were cold applications). Get 2 phone screens. Go through the process (usually a mix of behavioral and technical. Usually at least 1-2 live coding sessions) and end up with 2 offers at the end of it to decide between / bounce off of each other.

I am not a rockstar coder who can code any ds/a out there. I have a wide breadth of experience in big data technologies, but wouldn't consider myself an expert in any of them. I think I'm just a fairly smart problem solver who can talk to people and happens to have some company name-recognition on my resume. 

This is more aimed at senior engineers / people with 3+ years experience. I think the market is very similar looking at a macro scale to what it's always looked like (outside of prime covid). Entry level jobs don't exist for DE, mid-level jobs are also a bit rare and tough to get, and senior level talent is still needed by most companies. 

To those of you who are looking for your first role and putting out tons of applications and getting no responses back -- I'd recommend looking at smaller companies where you can wear a lot of hats, even if they're posted as analyst roles. As long as you get a database connection and can use sql, that's where most of us start. Alternatively, if you can learn just a little frontend, you might be able to get interviews for a jr. dev. The market is just super saturated with juniors and people making career changes post-covid, so it's really tough for new entrants. 

Anywho, enough rambling. Happy to answer any questions and am curious to see what other senior folks' experiences have been either getting hired or hiring.",154,50,Purple_Read2064,2023-09-20 13:53:08,https://www.reddit.com/r/dataengineering/comments/16nlvln/a_senior_engineers_experience_in_the_current_job/,False,False,False,False
127aca3,Has anyone else moved into data engineering just to discover they hate it?,"A couple years ago I needed a job and took one on a hybrid data/full stack team, thought days engineering sounded cool. We had a reorg and I was moved from there to a data platform team. At first I kinda dug it since we had a good manager, but he got laid off, we share one manager with two other teams now so it's chaotic. 

On top of that it just doesn't feel like I actually build anything, it's just writing pipelines, dealing with different warehousing tools that someone else built, making sure all of the tables are organized for other people to use, and hooking up integrations for various tools. I feel like my engineering skills have gotten rusty, and I've applied around for other jobs but since I was too chicken to make a move during last years wild market I feel pigeonholed into data now that employers are pickier, and it's just going to get worse the longer I'm here. Doesn't help that I'm starting to realize that data engineers seem to make less for some reason.

I'm kinda just venting but I'm really tired of feeling like a back room data monkey, this job has me burnt out on coding overall and wanting to try to switch to product, or just move out of tech to do I don't even know what.",156,63,KookaB,2023-03-31 04:41:22,https://www.reddit.com/r/dataengineering/comments/127aca3/has_anyone_else_moved_into_data_engineering_just/,False,False,False,False
xpoxj1,« What is an ETL? » and other hard questions.,"Hello fellow data engineers!

A junior is supposed to join my team and work directly with me. On the menu?
- databricks with PySpark
- AWS S3, glue, lambda etc.
- Data pipelines to monitor, with some scheduling
- Features for our data scientists etc.

Anyway, our recruitment is aimed at hiring somebody capable yet junior.

The expected experience is 1-2 year, knowledge of Python and SQL is required, we welcome AWS experience but it’s not necessary.

Of course we have a technical interview where we try to check who is best fit for joining us. And well. To be frank. It’s not great.

Almost every candidates stop at the question “what is an ETL”. The one that do know what it is look at us with a blank face when we ask “what would you do if the ETL you work on fails and the senior DE isn’t there to help you?”. We are talking about situational “technical” questions. And yet everyone stumbles.

SQL window functions? Ever heard of it? “Nope.”
Somebody dropped our prod DB, what do you do? “Well, if it’s being dropped, we get a pop up window telling us not to do it”

We also send a small piece of Python code, 30 lines or so, with instructions, that they can check but don’t have to complete before the interview:
1. A request to a public API endpoint via a try/catch (to the iris dataset)
2. Then a couple of comments that they should filter out the petal width and the species
3. And write as CSV.

Gosh. Like the amount of people that were just like “yeah here there is an if, and here else, I saw that before”, or that simply tell us “you didn’t give me an API”…

An AI PhD student (?) told me that he is learning programming languages like html, css and flask because he doesn’t need JavaScript for web dev (???) and couldn’t read Python code (?????).

Anyway, this is like, all our candidates. I have to work later with one of these people if we recruit them. Yet, the person that helps me interview them, questions if what we ask is too hard? I told them that no. I don’t care if they haven’t scaled thousands of pipeline, deployed a ML model to power a social network, how to optimise PySpark processing or architect a real time DB: I ask them what is an ETL.

I can’t train somebody from scratched when they can’t even read Python code. It’s like hiring a sous chef that doesn’t know what is the difference between boiling and frying ingredients! I just want to scrap the recruitment process and wait to start it later because this is depressing. I don’t know, am I unrealistic in the expectations for a junior? What is the lowest bar you set when recruiting juniors?

TL:DR; got poor DE candidates from my perspective (no knowledge of ETL). Fellow recruiter thinks the questions are too hard. How do you hire your juniors?

Edit: located in Europe, so maybe a different market than US based?",154,87,MadT3acher,2022-09-27 18:40:44,https://www.reddit.com/r/dataengineering/comments/xpoxj1/what_is_an_etl_and_other_hard_questions/,False,False,False,False
plmukq,Some of these job postings out there are absolutely hilarious. WTF is this jumbled mess of shit?,,152,89,Imaginary-Ad2828,2021-09-10 15:04:56,https://i.redd.it/jx0bfb06yom71.jpg,False,False,False,False
1bleg24,Should I learn data engineering? Got shamed in a team meeting.,"I am a data analyst by profession and majority of the time I spend time in building power bi reports. One of the SQL database we get data from is getting deprecated and the client team moved the data to Azure data lake. The client just asked our team (IT services) to figure how do we setup the data pipelines (they suggested  synapse)

Being the individual contributor in project I sought help from my company  management for a data engineer to pitch in to set this up or at least guide, instead I got shamed that I should have figured everything by now and I shouldn't have accepted to synapse approach in first place. They kept on asking questions about the data lake storage which I don't have experience working on.

Am I supposed to know data engineering as well, is it a bad move that I sought help as I don't have experience in data engineering. My management literally bullied me for saying I don't know data engineering. Am I wrong for not figuring it out, I know the data roles overlap but this was completely out of my expertise. Felt so bad and demotivated.

Edited(added more details) - I have been highlighting this to the management for almost a month, They arranged a data engineer from another project to give a 30 minutes lecture on synapse and its possibilities and vanished from the scene. I needed more help which my company didnt want to accommodate as it didnt involve extra billing. Customer was not ready to give extra money citing  SOW. I took over the project 4 months back with the roles and responsibilities aligned to descriptive stats and dashboards.

  
**Latest Update: The customer insists on a synapse setup, So my manager tried to sweet talk me to accept to do the work within a very short deadline, while masking the fact from the customer that I dont have any experience in this. I explicitly told the customer that I dont have any hands on in Synapse, they were shocked. I gave an ultimatum to my manager that I will build a PoC to try this out and will implement the whole setup within 4 weeks, while a data engineer will be guiding me for an hour/day.  If they want to get this done within the given deadline ( 6 days) they have to bring in a Data engineer, I am not management and I dont care whether they get billing or not. I told my manager that if If they dont accept to my proposal, they can release me from the project.** ",149,104,urbanguy22,2024-03-23 00:09:01,https://www.reddit.com/r/dataengineering/comments/1bleg24/should_i_learn_data_engineering_got_shamed_in_a/,False,False,False,False
zzvb1o,"Free ""dbt for beginners"" course",,152,30,oleg_agapov,2022-12-31 14:30:46,https://dataschool.alterclass.school/courses/dbt-for-beginners-352402536678818389,False,False,False,False
15la888,What is the most unproductive task you have to do as a data engineer?,I am new to data engineering and learning basic stuff but I am curious to know what's the most unproductive task you have to do as a data engineer.,153,129,vinayak_singh_k,2023-08-08 06:53:02,https://www.reddit.com/r/dataengineering/comments/15la888/what_is_the_most_unproductive_task_you_have_to_do/,False,False,False,False
13dlgrr,Are SQL Query optimization skills important and demanded for data scientists/data engineers?,I don't know if SQL Query optimizations skills are demanded or relevant for data scientists/data engineers and data science/data engineering businesses. But I wonder if one with SQL Query optimization skills can stand out from the crowd of data scientists and data engineers and earn higher paychecks?,152,100,Born-Comment3359,2023-05-10 09:35:03,https://www.reddit.com/r/dataengineering/comments/13dlgrr/are_sql_query_optimization_skills_important_and/,False,False,False,False
wcw0nt,What is in your Data Stack? - Thread,"It would be really useful to get a sense of what data tools companies use to get an idea of what are the best options.

There are a relatively standard set of things most data teams do, but many product options to serve these

Got the idea from the quarterly salary thread, which is a really good resource.

To contribute, post info in the following format:

1. **ETL**
2. **Data Warehouse**
3. **Data Transformation**
4. **BI**
5. **Exploratory Data Analysis**
6. **Company Size** (approx # employees) *\[optional\]*
7. **Company Industry** *\[optional\]*
8. **Company HQ** (city, country) *\[optional\]*

\[Disclaimer - work at a data company\]",154,139,Evidence-dev,2022-07-31 18:56:32,https://www.reddit.com/r/dataengineering/comments/wcw0nt/what_is_in_your_data_stack_thread/,False,False,False,False
tcdycl,Data Engineering Handbook,"Hi,I recently came across the GitLab data engineering handbook ([https://about.gitlab.com/handbook/business-technology/data-team/organization/engineering/](https://about.gitlab.com/handbook/business-technology/data-team/organization/engineering/))   
[https://handbook.mattermost.com/operations/research-and-development/engineering/data-engineering](https://handbook.mattermost.com/operations/research-and-development/engineering/data-engineering) and I really enjoyed it. Is there any similar recommendation/links from other companies?

Thank you

Edit: Added a few more links.
Edit1: updated name to gitlab",151,16,dna_o_O,2022-03-12 10:50:21,https://www.reddit.com/r/dataengineering/comments/tcdycl/data_engineering_handbook/,False,False,False,False
rfpsnt,Data Engineering Zoomcamp - free Data Engineering course starting in January,"At [DataTalks.Club](https://DataTalks.Club) we're running a free data engineer course next month

We'll cover:

* Data warehousing (BigQuery)
* Batch processing (Airflow, Spark)
* Analytics engineering (DBT)
* Stream processing (Kafka)

And other things! 

&#x200B;

Learn more here: [https://github.com/DataTalksClub/data-engineering-zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)

And sign up here: [https://airtable.com/shr6oVXeQvSI5HuWD](https://airtable.com/shr6oVXeQvSI5HuWD)

&#x200B;

See you soon on the course!",152,18,stolzen,2021-12-13 20:53:05,https://www.reddit.com/r/dataengineering/comments/rfpsnt/data_engineering_zoomcamp_free_data_engineering/,False,False,False,False
oe687q,Spark learning resource that I have been maintaining with care. Use for learning new concepts/preparing for interviews. (Recommendations are welcome),,153,29,ankurchavda,2021-07-05 12:19:32,https://github.com/ankurchavda/SparkLearning#spark-learning-guide,False,False,False,False
1ao16gb,Who uses DuckDB for real?,I need to know. I like the tool but I still didn’t find where it could fit my stack. I’m wondering if it’s still hype or if there is an actual real world use case for it. Wdyt?,151,134,marclamberti,2024-02-11 06:09:40,https://www.reddit.com/r/dataengineering/comments/1ao16gb/who_uses_duckdb_for_real/,False,False,False,False
1al3d2f,"Are data engineers really just ""software engineers""?","Ok, to preface, I'm venting a bit here but it's also somewhat of a genuine question.   
Story - I recently applied to a senior DE position for a well known consulting company. For the record, I've worked in Senior DE/BI roles over the past few years and I have a number of former colleagues and friends who work at this specific company so I know their tech stack and business fairly well. Also, for the record I am not a software engineer. I can hack my way through python or an OOP/functional language but SQL is my native dialect. Anyways, I applied for this role and the only glaring omission on my resume was Python experience. Given that I qualified in every other way the recruiter had me move forward to the technical assessment. The assessment was conducted in codility and there were three parts, a python coding portion, a sql coding portion and AWS questions. Coming out of the assessment I felt pretty good but I knew full well that my python solution was pretty rudimentary (admittedly), however it was functional and passed the test cases correctly. Anyways, I find out a few days later from the internal recruiter that my test results didn't fare so well. Although my sql solution was excellent and most of the AWS questions I answered correctly, my python solution wasn't efficient enough and failed on too many edge cases. As such the technical team couldn't recommend I move forward with the interview process (much to my dismay). Now, again... I never said I was a competent Python programmer, in fact I fully admitted that I had very little hands on experience in a business setting coding with python but I'm very familiar with OOP concepts and can pick up any language if/when needed. Either way it seemed like in this case my solution needed to impress the team more than it did.   
So, this brings me back to something the recruiter told me initially... her exact words were ""our data engineers are really software engineers at heart"". I'm wondering if this is becoming more and more the case as time goes on. When I got into BI and DE years ago SQL was the language of most importance (at least in my past roles)... now it seems that that isn't quite the case anymore. Thoughts?",151,128,MasterKluch,2024-02-07 13:50:55,https://www.reddit.com/r/dataengineering/comments/1al3d2f/are_data_engineers_really_just_software_engineers/,False,False,False,False
12han02,Which tools helps you make such animated gif for data pipelines?,,154,73,unmeshshah1988,2023-04-10 07:20:38,https://i.redd.it/x1x1abmtu1ta1.gif,False,False,False,False
zgpcsx,r/dataengineering wrapped,,146,3,theporterhaus,2022-12-09 06:40:48,https://i.redd.it/wjtxidai0v4a1.jpg,False,False,False,False
q63r5q,Google Cloud Data Engineer Certification available for free for a month on Coursera (till Nov 6).,Essentially all the google cloud courses are available for a month for free. Here's a link to the page on coursera - https://www.coursera.org/promo/google-cloud-free-courses-2021?utm_source=googlecloud&utm_medium=institutions&utm_campaign=NoCostTraining_Oct21_newsletter,150,41,noNSFWcontent,2021-10-11 19:32:59,https://www.reddit.com/r/dataengineering/comments/q63r5q/google_cloud_data_engineer_certification/,False,False,False,False
14qzt8y,Just got certified! - Databricks certified associate developer for apache spark 3.0 in Python,"Just got certified! I am a new data analyst who wants to hopefully move into the data engineering field.

I have done a few projects just finding it hard in the current market to find a job. Decided to keep working at my current job and in the meantime finish off a few certs to hopefully attract a few recruiters. Gonna go for the data engineer associate and professional next

For anyone wanting to get it, I highly recommend getting it, it stays forever(no expiry), fairly simple took me 2 weeks assuming you have general python syntax knowledge, plus access to documentation in the exam.

The resources I used were:

1. [https://www.udemy.com/course/databricks-certified-developer-for-apache-spark-30-practice-exams/](https://www.udemy.com/course/databricks-certified-developer-for-apache-spark-30-practice-exams/) \- used this to test my knowledge and basically research what topics are more likely to appear, has a nice breadth of important topics
2. [https://chowdhury-joyjit.medium.com/field-notes-for-the-databricks-certified-spark-developer-exam-ca0b6eb452fc](https://chowdhury-joyjit.medium.com/field-notes-for-the-databricks-certified-spark-developer-exam-ca0b6eb452fc) \- a good reference guide
3. [https://www.udemy.com/course/databricks-certified-developer-apache-spark-30-python/?referralCode=D20A15144B8AF7D2C5CF](https://www.udemy.com/course/databricks-certified-developer-apache-spark-30-python/?referralCode=D20A15144B8AF7D2C5CF) \- essentially 90% of the actual exam questions pretty decent explanations  


P.S Sorry couldn't forget to include [Spark Internals Explanation](https://www.youtube.com/watch?v=7ooZ4S7Ay6Y)! A phenomenal resource to dive deep on how Spark works under the hood",150,55,Background_Debate_94,2023-07-05 04:13:09,https://www.reddit.com/r/dataengineering/comments/14qzt8y/just_got_certified_databricks_certified_associate/,False,False,False,False
qo0mun,"Learning SQL, beyond the basics","Hello, fellow Redditors,

Learning SQL beyond the basics can be difficult without a real project. With this in mind, I wrote an article that covers some concepts and techniques that can help you get better at SQL for data warehouses. I hope that these techniques + deliberate practice can help level up your sql skills.

[https://www.startdataengineering.com/post/improve-sql-skills-de/](https://www.startdataengineering.com/post/improve-sql-skills-de/)

Hope this helps someone :). Any feedback is appreciated.",147,11,joseph_machado,2021-11-06 13:49:12,https://www.reddit.com/r/dataengineering/comments/qo0mun/learning_sql_beyond_the_basics/,False,False,False,False
16qscvf,Everyone on this subreddit should be aware of market salaries,"I saw this post in cscareerquestionsEU. So far there is not so much data for DE job in EU. Please share and fill your data to make it transparent. I am just sharing the post and am not affiliated to anyone -

&#x200B;

Here is the original post -

Here are some sources, none of them are self-promotion nor intended as promotion. I am not being compensated for this, I just strongly believe in the value that salary transaprency brings:

1. [The Trimodal nature of salaries](https://blog.pragmaticengineer.com/software-engineering-salaries-in-the-netherlands-and-europe/)
2. [https://levels.fyi](https://levels.fyi/) \- make sure to filter for your location
3. [https://techpays.eu](https://techpays.eu/) \- basically a levels clone focused on EU with a subset of companies

tl;dr: Switzerland or HFT pays about US rates but has low # of companies and jobs, Netherlands can pay US rates but you have to know which companies, then you get 2nd tier US rates (regions like Austin, or companies like blue chip companies) in Munich, Berlin, London. Most other cities and countries don't have US competitive salaries. Poland is a favorite pick for US companies to have US teams so they pay around 50k (junior) to 150k (staff+) and COL is very low so savings rates can be quite high. Because they work in English, limited need to know Polish AFAIK.",148,17,Diligent_Fondant6761,2023-09-24 08:26:56,https://www.reddit.com/r/dataengineering/comments/16qscvf/everyone_on_this_subreddit_should_be_aware_of/,False,False,False,False
14nywsi,"Created my first Data Engineering Project which integrates F1 data using Prefect, Terraform, dbt, BigQuery and Looker Studio","## Overview

The pipeline collects data from the Ergast F1 API and downloads it as CSV files. Then the files are uploaded to Google Cloud Storage which acts as a data lake. From those files, the tables are created into BigQuery, then dbt kicks in and creates the required models which are used to calculate the metrics for every driver and constructor, which at the end are visualised in the dashboard.

[Github](https://github.com/InosRahul/f1-data-pipeline)

Architecture

&#x200B;

https://preview.redd.it/xvffk1orod9b1.png?width=3624&format=png&auto=webp&s=3cd2d18a4939f8b1000f1d572dffe8647fe51133

## Dashboard Demo

  


https://i.redd.it/ukolugwppd9b1.gif

[Dashboard](https://lookerstudio.google.com/reporting/9fd225dd-a9b8-45d9-87dc-7d7dbae0c841)

&#x200B;

## Improvements

* Schedule the pipeline a day after every race, currently it's run manually
* Use prefect deployment for scheduling it.
* Add tests.

[Data Source](https://ergast.com/mrd/)",147,27,mortysdad44,2023-07-01 16:13:08,https://www.reddit.com/r/dataengineering/comments/14nywsi/created_my_first_data_engineering_project_which/,False,False,False,False
z1p5ny,Apache Spark™ for Dummies,"I wrote a high-level introduction about Apache Spark. Feel free to leave any feedback!

*When choosing a compute engine, there is no way around Spark. But where does Spark come from and why is it so popular?....*

[More on Medium <:](https://medium.com/microsoft-data-platform-community-hamburg/apache-spark-for-dummies-b77384e33c91)

&#x200B;",146,16,keevee94,2022-11-22 09:20:01,https://www.reddit.com/r/dataengineering/comments/z1p5ny/apache_spark_for_dummies/,False,False,False,False
y7ynko,"The data architecture ""pyramid of doom"" according to Dremio. This has lit up quite a discussion in my team's chat 😆 What do you think?",,144,58,CaptainP,2022-10-19 10:06:40,https://i.redd.it/be6qt0dmynu91.png,False,False,False,False
16e4d6v,"Data engineering project: Apache Spark, Delta Lake, & Great Expectations running on Docker; Explaining some best practices!","Hello everyone,

There are many DE project posts out there. But they don't explain exactly why a specific approach was chosen. If you are trying to improve your data engineering skills or are the sole data person in your company, it can be hard to know how your technical skills are developing.

With this in mind, I wrote an article that explains high-level best practices (with links to specific approaches and a project), such as:

1. Using established data processing patterns

2. Data quality checks & code testing

3. Approach to make pipelines Idempotent

4. Metadata for debugging and tracking

I hope the posts explain the underlying concepts behind best practices and when to use them.

Blog: [https://www.startdataengineering.com/post/de\_best\_practices/](https://www.startdataengineering.com/post/de_best_practices/)

GitHub Code: [https://github.com/josephmachado/data\_engineering\_best\_practices](https://github.com/josephmachado/data_engineering_best_practices)

I appreciate any questions, feedback, or comments. I hope this helps someone.

&#x200B;",145,14,joseph_machado,2023-09-09 12:20:27,https://www.reddit.com/r/dataengineering/comments/16e4d6v/data_engineering_project_apache_spark_delta_lake/,False,False,False,False
zvfcjh,Discussion: Star schema and dimensional modeling is still the foundation of data engineering,"I keep myself abreast of trends in the industry as well as new technologies. It seems like most of the innovation in data engineering, aside from the release of Airflow and similar open source pipeline tools (to provide an alternative to SSIS and such) is related to handling huge and massive data volumes. Everything related to hadoop, spark, and even the parallel columnar databases like Redshift.

&#x200B;

Fundamentally, though, I still feel quite ""old school"" in my approach. I still find myself thinking in the Kimball mindset of conformed dimensions and an enterprise bus. Additivity, granularity, etc.

&#x200B;

Am I hopelessly behind the curve in some way? Is there some movement or phenomenon in data engineering that potentially makes classic concepts like this irrelevant?

&#x200B;

I saw that AWS was blasting info at the latest re:invent about how they are moving toward a ""no-ETL future"" by increasing interoperability between pieces in their ecosystem. I'm very skeptical of that because, IMO, most of the value of ETL is actually dimensional conformance and CDC/SCDs more than simply moving data from place to place.",146,69,internetstuff,2022-12-26 04:50:39,https://www.reddit.com/r/dataengineering/comments/zvfcjh/discussion_star_schema_and_dimensional_modeling/,False,False,False,False
nk7j14,What articles are must reads for data engineers?,It can be about anything in the field :-),147,38,None,2021-05-24 20:29:12,https://www.reddit.com/r/dataengineering/comments/nk7j14/what_articles_are_must_reads_for_data_engineers/,False,False,False,False
1bxt144,"what do people mean when they say stuff like ""ETL"" and ""building pipelines""?","I do understand it stands for Extract, Transform, Load, but whenever people say it, it always sounds like a big deal. In my current job, I do extract data from other servers/databases/tables, transform them and load them with SQL into the reports/tables where I need them. Can I also say I'm doing ETL? Or does it mean they are using some specialized programs? What about the pipelines? Could anyone explain these to me please?

Thanks in advance!",148,46,FuckingLovePlants,2024-04-07 02:15:47,https://www.reddit.com/r/dataengineering/comments/1bxt144/what_do_people_mean_when_they_say_stuff_like_etl/,False,False,False,False
1bo4nne,"Finding a new job, ridiculous ","Hello guys after finishing a contract in a company I’m searching for another opportunity in Europe based remotely and what I see in the job descriptions in LinkedIn are 27 technologies needed for the position and you have to be an expert, even not a senior position (I have 3.5 years of experience), what is happening here?

You need to know: python, pyspark, scala , JavaScript, java, azure, aws, gcp (and all the the technologies), databricks, airflow, Kafka, sql, no sql, data lakes, dwh, oracle, ETL’s, terraform, Jenkins, kubernetes… and more 

Ofc all of this fluent and proficient, lol



And not even senior positions… what would you recommend, guys?
I’ve been working with azure data factory/synapse/Databricks with python/pyspark and sql, doing etl/elt pipelines from on-premise ddbb or simple excels or cloud ddbb, or api’s.

",145,62,Irachar,2024-03-26 10:35:24,https://www.reddit.com/r/dataengineering/comments/1bo4nne/finding_a_new_job_ridiculous/,False,False,False,False
13rrzx2,What are some good publicly available real-time data sources?,"I am attempting to source via the wisdom of the crowd here. I often find it hard to find good real-time data sources for learning about streaming, prototyping, or building hobby projects. I started researching and then created an ""Awesome List"" in a GitHub repo - [https://github.com/bytewax/awesome-public-real-time-datasets](https://github.com/bytewax/awesome-public-real-time-datasets). 

Does anyone have a good source I should add to this list?",146,67,math-bw,2023-05-25 20:12:55,https://www.reddit.com/r/dataengineering/comments/13rrzx2/what_are_some_good_publicly_available_realtime/,False,False,False,False
yve7sf,Master's thesis finished - Thank you,"Hi everyone! A few months ago I defended my **Master Thesis on Big Data** and got the maximum grade of 10.0 with honors. I want to thank this subreddit for the help and advice received in one of my previous posts. Also, if you want to build something similar and you think the project can be usefull for you, feel free to ask me for the Github page (I cannot attach it here since it contains my name and I think it is against the PII data community rules).

As a summary, I built an **ETL process** to get information about the latest music listened to by **Twitter** users (by searching for the hashtag #NowPlaying) and then queried **Spotify** to get the song and artist data involved. I used **Spark** to run the ETL process, **Cassandra** to store the data, a custom web application for the final visualization (**Flask** \+ table with DataTables + graph with Graph.js) and **Airflow** to orchestrate the data flow.

In the end I could not include the Cloud part, except for a deployment in a virtual machine (using GCP's Compute Engine) to make it accessible to the evaluation board and which is currently deactivated. However, now that I have finished it I plan to make small extensions in GCP, such as implementing the Data Warehouse or making some visualizations in Big Query, but without focusing so much on the documentation work.

Any feedback on your final impression of this project would be appreciated, as my idea is to try to use it to get a junior DE position in Europe! And enjoy my skills creating gifs with PowerPoint 🤣

https://i.redd.it/trlt7kqunzz91.gif

P.S. Sorry for the delay in the responses, but I have been banned from Reddit for 3 days for sharing so many times the same link via chat 🥲 To avoid another (presumably longer) ban, if you type ""**Masters Thesis on Big Data GitHub Twitter Spotify**"" in Google, the project should be the first result in the list 🙂",146,93,Riesco,2022-11-14 22:07:29,https://www.reddit.com/r/dataengineering/comments/yve7sf/masters_thesis_finished_thank_you/,False,False,False,False
15ni579,Got my first Data Engineer job,"Hi everyone, I wanted to say a big thanks to this sub-Reddit, as I just got my first job as a Data Engineer. 

After losing my job I decided to make a career change into Data Engineering from Data Science. From reading posts here for the past year or so my interest has grown in the area, leading up to this, so thanks everyone for all of the interesting and useful posts!

Any advice for topic area to read up on before I start? Or maybe courses/YouTube series to help me be ready? 

Thanks!",145,50,Impressive_Fact_6561,2023-08-10 17:07:22,https://www.reddit.com/r/dataengineering/comments/15ni579/got_my_first_data_engineer_job/,False,False,False,False
124mi0z,"SMBC-comics.com ""now squeeze your points together to make your results look big""",,143,6,rackhamlerouge9,2023-03-28 11:59:23,https://i.redd.it/cz9tbicqygqa1.png,True,False,False,False
sjjaao,Discuss a data pipeline that you've worked on,"The vision of this subreddit is to learn by sharing our experiences. So, let's discuss in short about at least one data pipeline that we have worked on.

I'll put my 2 cents on the table.

I work in a medium sized bank in the engineering department that builds solutions for various teams present here. 

A data pipeline out of these will be one in which we provide our investment team with an insight about companies they are interested in (for investment). This insight is in the form of providing a status of outstanding charges (loans) that those companies have taken from other banks. 

In the first step of the pipeline data is scraped by crawlers from a publicly available government website and stored on a mongo cluster with timestamps of the time when the data was extracted.
In the second step this data is written on a kafka raw topic
Some transformations/cleaning are carried out and the fresh data is now written to a kafka rich topic
Next we write a Cassandra Loader which reads data from the rich topic and with the help of a mapping file loads data on to a cassandra namespace to it's corresponding column families. Cassandra is used as it helps to retain different versions of the same data based on a clustering key of yearmonth.
For faster insights (search queries) one version (the latest one) of this data is written on elasticsearch so that various indices can be created based on interested columns as per business requirements.

This is an example of an ETL pipeline (leaving aside the elasticsearch part). It can also be seen as an EtLT pipeline as transformations are also carried on top of the data in Cassandra by writing adhoc Spark jobs.

P.S : if you are taking away something from this post, please add some cents on the table so we all can become rich 😛",146,31,No_Spread_2566,2022-02-03 13:32:33,https://www.reddit.com/r/dataengineering/comments/sjjaao/discuss_a_data_pipeline_that_youve_worked_on/,False,False,False,False
scmpdl,Gitlab's Data Team Platform (in depth look at their stack),,145,13,fhoffa,2022-01-25 20:11:25,https://about.gitlab.com/handbook/business-technology/data-team/platform/,False,False,False,False
oolpqh,"Saw this on Twitter. Is the specialization of data teams a bad thing, and will that change going forward?",,145,84,None,2021-07-21 08:35:10,https://i.redd.it/neb6prf42jc71.jpg,False,False,False,False
l0qg2l,Introduction to Databases for Data Engineers,,144,28,oleg_agapov,2021-01-19 18:54:53,https://github.com/oleg-agapov/data-engineering-book/blob/master/book/2-beginner-path/2-1-databases/databases.md,False,False,False,False
16o883v,"Python Pareto Principle - what is the 20% (algos, functions, libraries) that lets you develop 80% of code related to Data Engineering?",.,146,71,CrimsonMentone30,2023-09-21 06:14:35,https://www.reddit.com/r/dataengineering/comments/16o883v/python_pareto_principle_what_is_the_20_algos/,False,False,False,False
13eo3b3,Is it worth learning Apache Spark in 2023?,According to stack overflow survey 2022 Apache Spark is one of the highest paying technologies. But I am not sure if I can trust this survey. I am really afraid I will waste my time . So people with more experience could you please let me know if Apache Spark is high demanded and high paying skill? Will learning internals of it worth my time?,143,128,Born-Comment3359,2023-05-11 13:54:23,https://www.reddit.com/r/dataengineering/comments/13eo3b3/is_it_worth_learning_apache_spark_in_2023/,False,False,False,False
nuvhli,Starting A Data Engineering Project Series,"&#x200B;

 I am starting to put together a series on developing a data engineering project for your resume. 

I think I have seen a lot of people on this subreddit ask for it, so I am hoping it will really help you out!

That being said, I would love to hear about what you would like to see in a data engineering project series. Even if I don't use it now, I am sure I will use it later.

I did just put out the [first video in the series](https://www.youtube.com/watch?v=LJkVvNWlO0g). 

**But the TL;DR is here are 5 sources I reference in the video that you can get from online that you can practice setting up some form of data pipeline on. Even if the data is only updated once a quarter, its still good practice.** 

[San Fransisco Has A Lot Of Great Options In terms](https://datasf.org/opendata/) \- I don't like the US government's main data site as much as I like SFs. It's just easier to use

[Real estate APIs](https://gist.github.com/patpohler/36c731113fd113418c0806f62cbb9e30) \- These do often cost a little, but the cheapest one on the list was around $40 a month, which isn't terrible (as long as you finish your project in a month or two)

[Census Data](https://www.census.gov/data/datasets.html) 

[Consumer Finance Complaints](https://www.consumerfinance.gov/data-research/consumer-complaints/) 

[News RSS Feeds](https://github.com/damklis/DataEngineeringProject)

**Why did I start a youtube channel?** 

Honestly, I really want to start getting data engineering a little more lime light. 

I don't think we will ever have the same shine as data science. However, I think we play a very important role that I personally really enjoy and I want to save some people who think they want to be data scientists, who really want to be data engineers.",141,29,nonkeymn,2021-06-08 03:42:15,https://www.reddit.com/r/dataengineering/comments/nuvhli/starting_a_data_engineering_project_series/,False,False,False,False
kx7tul,"We don't need data scientists, we need data engineers",,144,36,joshdick,2021-01-14 15:30:16,https://www.mihaileric.com/posts/we-need-data-engineers-not-data-scientists/,False,False,False,False
162rlqg,How are you lowering your data platform costs?,"Going by the recent S-1 filling from Instacart there is a tangible example of cost cutting.
Their Snowflake bill was $13M, $28M, $51M in 2020/21/22. 2023 bill will be $15M.
What they did behind the scenes hasn’t been revealed but I am pretty sure this isn’t the first of such changes. If you know more about this, do share.

How is this community thinking about costs in these economic times? If you aren’t focused on costs and already run pretty well, do share that too.",143,120,None,2023-08-27 14:18:01,https://www.reddit.com/r/dataengineering/comments/162rlqg/how_are_you_lowering_your_data_platform_costs/,False,False,False,False
wjhw2w,What exactly is a data brick?,"I am looking to get into data engineering and had an interview recently where data bricks were mentioned. Can someone explain to me exactly what a data brick is, and how it relates to a SQL table or Data Mart. From how it was explained to me, it was a self contained database that has its own flavor of SQL used to query it. 

I know I can simply Google this question but I want to hear from other data engineers about it.",142,40,DrRedmondNYC,2022-08-08 19:33:01,https://www.reddit.com/r/dataengineering/comments/wjhw2w/what_exactly_is_a_data_brick/,False,False,False,False
ruz2v1,Dont understand this job post no pyspark but extensive experience of spark jobs with python?,,141,89,Fragrant-Lobster4276,2022-01-03 10:53:50,https://i.redd.it/qn0vtn37eg981.jpg,False,False,False,False
1913k8k,Who are the GOATS of DE?,"This can a subjective question, DE is still niche and there is no such thing as a ranking but wanted to know if you guys have a high role model in the area. 

For example in programming there are well respected names on the likes of Linus Torvald or Guido Van Rossum.

This can be any inspiring youtuber, book writer, DE influencer or whatever.",141,116,hot-bulbasur,2024-01-07 21:46:35,https://www.reddit.com/r/dataengineering/comments/1913k8k/who_are_the_goats_of_de/,False,False,False,False
11x4u47,What is the hottest tech stack in Data Engineering world now?,I know we all work in different tech stacks. But what is the hottest tech stack at the moment (also have good future perspective) for any data engineers to pursue? Thank you.,139,162,jimmy3579,2023-03-21 02:56:17,https://www.reddit.com/r/dataengineering/comments/11x4u47/what_is_the_hottest_tech_stack_in_data/,False,False,False,False
10ijsqy,I built an LLM-powered tool that can understand the structure of any website and extract the desired data in the format you want.,,141,15,madredditscientist,2023-01-22 13:34:45,https://v.redd.it/me6caa1sklda1,False,False,False,False
vi2iv0,(Almost) OpenSource data stack for a personal DE project. Before jumping on the project I would have liked to have some advice on things to fix or improve in this structure! do you think that this stack could work?,,140,66,magna_987,2022-06-22 10:59:35,https://i.redd.it/b7can373m5791.png,False,False,False,False
vemxch,Does anyone think the cost of Snowflake is a problem?,"Recently, I talked with a data team leader of a healthcare startup. The data team leader is hesitant to export data from MySQL to Snowflake because the cost of Snowflake is a bit high for his company. Does anyone also concern about the cost of Snowflake?",142,103,PangolinMiserable817,2022-06-17 18:39:00,https://www.reddit.com/r/dataengineering/comments/vemxch/does_anyone_think_the_cost_of_snowflake_is_a/,False,False,False,False
sunt2i,Apparently 90% of all the Azure Data products are 7 years old in 2022. This is the job desc for a DE of a billion dollar pharma. It looks pointless to me to have 5+ yrs of exp into something that is just turning 7this year. Unrealistic tech expectations!,,140,48,johnyjohnyespappa,2022-02-17 12:36:29,https://i.redd.it/uzqvf10j1ei81.jpg,False,False,False,False
si6cjt,What resources do you use for staying up to date with the data engineering landscape?,"Hey all, curious what you follow to stay up to date with the data engineering world? Here's some of [my favorite](https://twitter.com/iporollo/status/1488590270044131332?s=20&t=tgf2-5R7h7cQs-fwW-G35Q) communities / blogs / newsletters / YouTube channels that I follow:

Communities:

dbt slack community - [https://www.getdbt.com/community/join-the-community](https://www.getdbt.com/community/join-the-community)

Locally Optimistic slack community - [https://locallyoptimistic.com/community/](https://locallyoptimistic.com/community/)

DataTalksClub slack community - [https://datatalks.club/](https://datatalks.club/)

Newsletters:

[https://roundup.getdbt.com/](https://roundup.getdbt.com/)

[https://benn.substack.com/](https://benn.substack.com/)

[https://www.dataengineeringweekly.com/](https://www.dataengineeringweekly.com/)

[https://dataespresso.substack.com/](https://dataespresso.substack.com/)

[https://davidsj.substack.com/](https://davidsj.substack.com/)

[https://www.blef.fr/](https://www.blef.fr/)

[https://seattledataguy.substack.com/](https://seattledataguy.substack.com/) 

[https://letters.moderndatastack.xyz/](https://letters.moderndatastack.xyz/)

[https://scientistemily.substack.com/](https://scientistemily.substack.com/)

YouTube Channels:

[SeattleDataGuy](https://www.youtube.com/c/SeattleDataGuy)

[DataCouncil](https://www.youtube.com/c/DataCouncil)

[Monday Morning Data Chat](https://www.youtube.com/playlist?list=PLIlqnK97FLdtQEad5pi22BefNuaSeBt1e)

TikTok Channels:

[the.data.guy](https://www.tiktok.com/@the.data.guy)

[SQream](https://www.tiktok.com/@sqreamtech)

[Sidcodes](https://www.tiktok.com/@sidcodes)

Curious to see what other people are reading / watching / listening to in this space!

EDIT: Found a few more!

EDIT 2: Fixed broken dataengineeringweekly link",138,22,ivy_p,2022-02-01 21:36:20,https://www.reddit.com/r/dataengineering/comments/si6cjt/what_resources_do_you_use_for_staying_up_to_date/,False,False,False,False
1ccv2r3,Whats your horror story with SAP Integration?,"I currently work at a large firm with a very large sap erp enterprise instance. Over the past two years, I've encountered more issues with SAP product teams and consultants than with any technology in my entire career prior. 

 SAP is such a shitty company; it's just disgusting. Lately, they disallowed the use of ODP RFC replication services, which basically outlawed any integration tool that uses this method to replicate SAP data to the cloud, e.g., Qlik, Azure Data Factory, Talend, and many more. 

It’s no coincidence that this change coincided with the launch of their ""new"" rebranded data warehouse, Data Sphere, where the costs of moving data into cloud services are exorbitant. Additionally, they've deliberately limited access to Data Sphere via their oData API for replication services. 

I know this is basically a rant but the amount of bullshit is just baffling. How do you guys deal with the SAP virus and what its your funny story?",139,79,Ok-Sentence-8542,2024-04-25 15:37:48,https://www.reddit.com/r/dataengineering/comments/1ccv2r3/whats_your_horror_story_with_sap_integration/,False,False,False,False
13ptsio,Microsoft announces Fabric data platform,"Looks interesting.. What do you guys think?

https://azure.microsoft.com/en-us/blog/introducing-microsoft-fabric-data-analytics-for-the-era-of-ai/",140,135,aj_here_,2023-05-23 16:43:17,https://www.reddit.com/r/dataengineering/comments/13ptsio/microsoft_announces_fabric_data_platform/,False,False,False,False
13cprs6,How Instagram handles data?,"I was looking at Mr. Beast’s new post for his recent giveaway. The number of comments are staggeringly high. It has more comments than the likes for the post. 

I was wondering how these comments are stored or handled within IG? Are they using SQL tables for this? And every time, I go the comments section does it read the database every time? Do we see real time numbers? 

If you have worked in such a system. Can you give your two cents please? Thank you.",139,52,MaintenanceSad6825,2023-05-09 13:23:20,https://www.reddit.com/r/dataengineering/comments/13cprs6/how_instagram_handles_data/,False,False,False,False
yrjwcf,"Out of work for 8 months, trying something new with my resume. I have to imagine this is easier for hiring managers to look at. And much easier to have in front of anyone conducting your interview. But then again, I'm not a hiring manager. What are people's thoughts on this format?",,139,61,Cli4ordtheBRD,2022-11-10 16:20:22,https://i.redd.it/2h61ob5wf5z91.png,False,False,False,False
mvdteu,/r/dataengineering hit 30k subscribers yesterday,,138,4,TrendingB0T,2021-04-21 11:41:22,https://frontpagemetrics.com/r/dataengineering,False,False,False,False
1c3gjhb,questions asked at Amazon's L4-L5 Data Engineering,"Hi guys,

My friend is L5 (Senior) Data Engineer at Amazon (EU) Luxembourg and she helped me to prepare set of question that according to her often asked at Amazon. They might not be 1 to 1 but pretty close.. i've posted it on [prepare.sh/engineering/de/amazon](http://prepare.sh/engineering/de/amazon)  
Hope this won't be seen as ad, as this is an open source website (its on github) that I keep updating to with useful questions to help communities.",135,25,Dubinko,2024-04-14 00:25:04,https://www.reddit.com/r/dataengineering/comments/1c3gjhb/questions_asked_at_amazons_l4l5_data_engineering/,False,False,False,False
11y6b3o,Where can I find online projects end-to-end?,"Two years in the industry, came from a non-tech background, but landed a job as a data engineer. I have worked on small tasks such as maintaining an already built ETL pipeline.

But I want to learn more. I want to build things from scratch.

Data modelling, data cleaning, ETL, etc.

Midnlessly solving SQL and python problems won't get me there.


Any help?


Note: This is for LEARNING. I don't want to sneak ANYTHING into my resume. I want to get my hands dirty.",141,34,Aick_Aleck,2023-03-22 04:04:35,https://www.reddit.com/r/dataengineering/comments/11y6b3o/where_can_i_find_online_projects_endtoend/,False,False,False,False
yw3mk5,After Airflow. Where next for DE?,"Airflow was released in **2014** by Maxime Beauchemin (of Airbnb). In those 8 years it’s really dominated DE in the wild. However, many teams are putting out ideas to succeed Airflow.

\----\[Edit\] **BTW** \- Seeing this has turned into a vendor conference **(Sorry!)** 

We are designing to support:

* DAGs on AWS Lamdba
*  Airflow Transpilation to allow productivity gains with no migration risk.

[typhoondata.io](https://typhoondata.io/)

[https://github.com/typhoon-data-org/typhoon-orchestrator](https://github.com/typhoon-data-org/typhoon-orchestrator)

Any feedback on our very early stage project is welcome!

\----

I’d like to start a discussion:

Is DE collectively moving on from Airflow, or … i***f it ain’t broke, why fix it?!***

What could be improved on Airflow (even v2):

&#x200B;

* Lack of ability to test easily
* Data sharing between tasks (TaskFlow API is a step in the right direction, but still limited by underlying design choices)
* End up with NxM for sources and targets

How might the market develop? (not 1 winner, for sure):

* **Streaming, Kafka**
   * Backbone of many of the large tech now
   * Very advanced for many teams
   * Will it completely replace batch?
* **Fivetran and other tools that automate DE**
   * Reduces reliance on hard to find DEs
   * New pricing models based on credits make it more affordable
* **Airbyte and DBT, Simple Airflow and DBT**
   * simple model of commoditizing the \[EL\] and then DBT for the \[T\]
   * makes ‘analytics engineers‘ (analysts) much more productive but is limited for complex workloads perhaps (will adding python hooks change this)
* **Dagster & Prefect**
   * Better composability
   * Shared data between tasks
   * Big enough feature improvements to move from Airflow?
   * Is the community big enough yet?
* **AWS Lambda (serverless)**
   * Tooling underdeveloped
   * 15 minute limit per lambda run
* **Stick with Airflow 2**
   * FOSS
   * Move to Astronomer for managed services is growing somewhat
   * Despite some productivity challenges, it is easy to support

**What do you think?**

\- What do you plan (concretely) on using in next 6 months?- What are you using now?

&#x200B;

**Full disclosure:**  I hope this is an interesting discussion question! We are of course making [a new alternative (typhoondata.io)](https://typhoondata.io/)   but want to learn from the forum so I have not included our option directly in the list.",138,97,Beautiful_Yam_8090,2022-11-15 17:20:40,https://www.reddit.com/r/dataengineering/comments/yw3mk5/after_airflow_where_next_for_de/,False,False,False,False
nc8zxh,Great Resource! YouTube walkthrough of 50+ Leetcode SQL problems,,140,19,None,2021-05-14 13:46:49,https://www.youtube.com/watch?v=DoGrxxa6kow&list=PLdrw9_aIADIPAMJW8I_S-S747oyiRtzpS,False,False,False,False
1bifhj9,O’Reilly data engineering reference books on sale! (Includes reference books on pyspark and scaling up pipelines),"Hope this post is ok, as I don't work for either O'Reilly or Humble Bundle. Given the number of questions on this thread for getting books on the topic, thought maybe some of you might be interested in this too! Personally, I'd been wanting to get, ""Data Algorithms with Spark,"" but had been hesitating due to the price. I was super thrilled seeing this included in the book bundle.

This is an organization that partners with others to offer books (and games) at a super low price. Part of the proceeds goes to charity. I've been a huge fan of them since discovering them a while ago.

[https://www.humblebundle.com/books/pipelines-and-nosql-oreilly-books](https://www.humblebundle.com/books/pipelines-and-nosql-oreilly-books)",137,27,truckbot101,2024-03-19 08:37:42,https://www.reddit.com/r/dataengineering/comments/1bifhj9/oreilly_data_engineering_reference_books_on_sale/,False,False,False,False
1b8e72j,Will Dbt just taker over the world ?,"So I started my first project on Dbt and how boy, this tool is INSANE. I just feel like any tool similar to Azure Data Factory, or Talend Cloud Platform are LIGHT-YEARS away from the power of this tool. If you think about modularity, pricing, agility, time to market, documentation, versioning, frameworks with reusability, etc. Dbt is just SO MUCH better.

If you were about to start a new cloud project, why would you not choose Fivetran/Stitch + Dbt ?",138,141,Ownards,2024-03-06 23:02:56,https://www.reddit.com/r/dataengineering/comments/1b8e72j/will_dbt_just_taker_over_the_world/,False,False,False,False
11jzbx6,"Pandas 2.0 and its Ecosystem (Arrow, Polars, DuckDB)",,136,12,sspaeti,2023-03-06 13:44:14,https://airbyte.com/blog/pandas-2-0-ecosystem-arrow-polars-duckdb/,False,False,False,False
11fyslh,Any other DEs here not involved in data warehousing / data modeling? Where's the love for the infrastructure and ingestion guys?,"I feel sometimes like this sub is a bit of a dbt / snowflake / sql love-in. Nothing wrong with that stack or skillset btw but I thought that stuff was more for analytics engineers?

I am more involved in the platforms & ingestion side of things. Lots of IaC for setting up data infrastructure, maintaining a streaming solution, working with SWE teams to ingest data from their apps and OLTP systems in a transactional and performant manner, and implementing things like data contracts and schema validation to stop upstream breaking changes. Some custom integrations using python and various AWS services to pull external data sources. I'm also pretty good with spark and do some initial validation, transformations & optimizations etc in the warehouse before handing over to the modelers.

Got to be other people like me here but I guess we're in a minority? Curious to hear where the boundary between DE and AE lies in your business?",140,69,the-data-scientist,2023-03-02 09:47:30,https://www.reddit.com/r/dataengineering/comments/11fyslh/any_other_des_here_not_involved_in_data/,False,False,False,False
v7pgim,Pandas-like library to build SQL models,"Hi! We built an open source python library called Bach\[1\] that talks Pandas, and outputs SQL that can be run directly on a data store (as cloud data stores can handle this now). It’s available on PyPI\[2\].

For example, let’s say we have the following DataFrame (a representation of what’s in the table):

|Index|Column A|Column B|
|:-|:-|:-|
|0|1|None|
|1|None|'a'|
|2|None|'b'|
|3|2|None|
|4|None|None|

…we can run a Bach operation equivalent to \`pandas.DataFrame.ffill()\`:

    df = df.ffill(sort_by=[‘index’], ascending=True)

|Index|Column A|Column B|
|:-|:-|:-|
|0|1|None|
|1|1|'a'|
|2|1|'b'|
|3|2|'b'|
|4|2|'b'|

… which gets translated to SQL:

    ""fillna_partitioning___2db01eb2e4973434d297e8ccc20cccd8"" AS(
       SELECT ""index_0""  AS ""index_0"",
        		""Column A"" AS ""Column A"",
               ""Column B"" AS ""Column B"",
              cast(
    sum(CASE
                         WHEN ""Column A"" IS NULL THEN 0
                         ELSE 1
                      END) 
    OVER ( ORDER BY ""index_0"" ASC rows BETWEEN UNBOUNDED     PRECEDING AND CURRENT row) 
             AS bigint) AS ""__partition_Column A"",
             cast(
    sum(CASE
                        WHEN ""Column B"" IS NULL THEN 0
                        ELSE 1
                      END) 
                     OVER ( ORDER BY ""index_0"" ASC rows BETWEEN UNBOUNDED PRECEDING AND CURRENT row) 
        AS bigint) AS ""__partition_Column B""
             FROM ""your_super_awesome_table"" ORDER BY ""index_0"" ASC 
    )
    SELECT ""index_0""                                                                                                             AS ""index_0"",
    
    first_value(""Column A"") OVER (partition BY ""__partition_Column A"" range BETWEEN UNBOUNDED PRECEDING AND    CURRENT row) AS ""Column A"",
    
    first_value(""Column B"") OVER (partition BY ""__partition_Column B"" range BETWEEN UNBOUNDED PRECEDING AND    CURRENT row) AS ""Column B""
    FROM   
    ""fillna_partitioning___2db01eb2e4973434d297e8ccc20cccd8""

We started working on this over a year ago, as we wanted to:

* Not write sometimes very complicated SQL, but use familiar pandas APIs.
* Not deal with data store SQL dialects and other quirks. (Currently supported data stores are PG & BQ, later Redshift, Databricks, etc.)
* Not have to port all data models if we switch data stores.
* Not build pipelines to get (quickly outdated) sample data, but just choose to get a sample from the data store or run an operation on the full set when needed.

Once a model is done, getting it to production is one operation, by exporting the resulting SQL to e.g. dbt, a BI tool, etc. So no need to port a model to SQL first anymore.

We already support a subset of pandas operations\[3\], but did we miss anything you use frequently? 

\[1\] [https://github.com/objectiv/objectiv-analytics](https://github.com/objectiv/objectiv-analytics)

\[2\] [https://pypi.org/project/objectiv-bach/](https://pypi.org/project/objectiv-bach/)

\[3\] [https://objectiv.io/docs/modeling/bach/api-reference/](https://objectiv.io/docs/modeling/bach/api-reference/)",139,43,ivarpruijn,2022-06-08 13:17:47,https://www.reddit.com/r/dataengineering/comments/v7pgim/pandaslike_library_to_build_sql_models/,False,False,False,False
v3xn34,How do you develop proficiency in Apache Spark?,"I've been working as a Data Engineer over a year, and hail from software engineering background. In my work role we have sorted life since we use Azure's ecosystem to process mostly structured data coming from different systems and blob sources. 

Recently, I gave an interview for an exciting role in an exciting startup and I realised I don't know spark in depth as much as I thought it would. 

The questions were trivial yet difficult, mosty situation based like: the architecture of spark, how data gets diistributed and processed in the framework, various edge cases of operations involving joins and data movements, the scenarios when data failure occurs (when the data is much bigger and skewed than individual RAM's of the worker nodes) and strategy to process data in such scenarios, how to monitor performance, failure handling etc and many more.

I have familiarity with big data ecosystems but not to the levels. Now, I have resolved to gain an expertise in spark and want your help with resources or strategies to achieve that goal. 

Thanks in Advance ❤️",140,21,kryon-a,2022-06-03 11:50:29,https://www.reddit.com/r/dataengineering/comments/v3xn34/how_do_you_develop_proficiency_in_apache_spark/,False,False,False,False
r8pa3i,Why is Data Build Tool (DBT) is so popular? What are some other alternatives?,"Hi, can some one please explain why DBT is so popular?",137,83,arezki123,2021-12-04 13:04:59,https://www.reddit.com/r/dataengineering/comments/r8pa3i/why_is_data_build_tool_dbt_is_so_popular_what_are/,False,False,False,False
1883wyz,Doom predictions for Data Engineering,"Before end of year I hear many data influencers talking about shrinking data teams, modern data stack tools dying and AI taking over the data world. Do you guys see data engineering in such a perspective? Maybe I am wrong, but looking at the real world (not the influencer clickbait, but down to earth real world we work in), I do not see data engineering shrinking in the nearest  10 years. Most of customers I deal with are big corporates and they enjoy idea of deploying AI, cutting costs but thats just idea and branding. When you look at their stack, rate of change and business mentality (like trusting AI, governance, etc), I do not see any critical shifts nearby. For sure, AI will help writing code, analytics, but nowhere near to replace architects, devs and ops admins. Whats your take? ",135,177,vee920,2023-12-01 05:21:39,https://www.reddit.com/r/dataengineering/comments/1883wyz/doom_predictions_for_data_engineering/,False,False,False,False
ve2qbp,Why does dbt have so much hype/ metions in this subreddit?,I’ve been a lurker on this sub for a little over a 1.5 years and over this past year dbt has been popping up in every post/comment section it possibly could be. I don’t I understand the hype about it. Especially to the degree it’s being said in this thread. Of all of my friends and colleagues in the tech/data sphere none of them have heard or even used dbt at all. It almost feels like it’s an slow astroturf campaign to try and get people to use their product. You don’t need dbt to set up a system to have version control for sql files. Or to use those version controlled sql files to run in your Prod pipeline. I understand the benefits of ELT but you don’t need dbt to do that. It’s one tool of many. Can someone explain they hype to me?,137,132,jalopagosisland,2022-06-17 02:46:29,https://www.reddit.com/r/dataengineering/comments/ve2qbp/why_does_dbt_have_so_much_hype_metions_in_this/,False,False,False,False
11zh526,Magic: The Gathering dashboard | First complete DE project ever | Feedback welcome,"Hi everyone,

I am fairly new to DE, learning Python since December 2022, and coming from a non-tech background. I took part in the [DataTalksClub Zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp). I started using these tools used in the project in January 2023.

Project link: [GitHub repo for Magic: The Gathering](https://github.com/VincenzoGalante/magic-the-gathering)

Project background:

* I used to play Magic: The Gathering a lot back in the 90s
* I wanted to understand the game from a meta perspective and tried to answer questions that I was interested in

Technologies used:

* Infrastructure via terraform, and GCP as cloud
* I read the [scryfall API](https://scryfall.com/) for card data
* Push them to my storage bucket
* Push needed data points to BigQuery 
* Transform the data there with DBT
* Visualize the final dataset with [Looker](https://lookerstudio.google.com/u/0/reporting/ebdf68e1-27f7-435b-8add-a4018681f801)

I am somewhat proud to having finished this, as I never would have thought to learn all this. I did put a lot of long evenings, early mornings and weekends into this. In the future I plan to do more projects and apply for a Data Engineering  or Analytics Engineering position - preferably at my current company. 

Please feel free to leave constructive feedback on code, visualization or any other part of the project. 

Thanks 🧙🏼‍♂️ 🔮",132,40,binchentso,2023-03-23 11:13:22,https://www.reddit.com/r/dataengineering/comments/11zh526/magic_the_gathering_dashboard_first_complete_de/,False,False,False,False
10z37l5,Valentine's for your data sweetheart 🫶,,135,4,Straight_House8628,2023-02-10 21:12:02,https://www.reddit.com/gallery/10z37l5,False,False,False,False
z01v13,World's Simplest Data Pipeline - I wrote a post about data engineering fundamentals. What do you think?,"
https://dantelore.com/posts/simplest-data-pipeline/

The idea was to capture the important stuff with the absolute minimum code/complexity.

Genuinely interested in people's opinions...",133,23,DanteLore1,2022-11-20 11:30:46,https://i.redd.it/nltkgi4xu41a1.png,False,False,False,False
lnj4lh,"We Don’t Need Data Scientists, We Need Data Engineers - KDnuggets",,135,45,Pitiful-Cupcake-2991,2021-02-19 15:55:00,https://www.kdnuggets.com/2021/02/dont-need-data-scientists-need-data-engineers.html#.YC_fCeWg11A.reddit,False,False,False,False
1b96lr0,Dagster University | Dagster & dbt,,133,26,floydophone,2024-03-07 21:53:37,https://courses.dagster.io/courses/dagster-dbt,False,False,False,False
11irw04,Is it true that Apache Spark (especially with Python) skills are in very high demand and paying well?,"I am checking stack overflow survey 2022 (https://survey.stackoverflow.co/2022/#top-paying-technologies-other-frameworks-and-libraries), and I see that apache spark is the highest paying framework under other frameworks category. I want to upskill myself and to be more demanded in job market. So is it worth learning Apache Spark (PySpark) in 2023?",132,62,Born-Comment3359,2023-03-05 08:30:17,https://www.reddit.com/r/dataengineering/comments/11irw04/is_it_true_that_apache_spark_especially_with/,False,False,False,False
rva6nc,How did you become a SQL pro?,"I can solve all Hackerrank/Leercode SQL problems but I feel like I don’t understand SQL as a whole, as such, I would not be able to handle a data cleansing/wrangling project.

Are there any practice SQL projects available? What would you recommend?",134,47,randomusicjunkie,2022-01-03 19:45:56,https://www.reddit.com/r/dataengineering/comments/rva6nc/how_did_you_become_a_sql_pro/,False,False,False,False
1ao15gx,"I built my first end to end data project to compare US cities for affordability against walk, transit and biking score. Plus, built a cost of living calculator to discover ideal city and relocate!","Found no site to compare city metrics score with affordability. So built a one.

Web app - [CityVista](https://cityvista.streamlit.app/)

An end-to-end pipeline -

1) Python Data Scraping scripts  
Extracted relevant city metrics from diverse sources such as US Census, Zillow and Walkscore.

2) Ingestion of Raw Data   
The extracted data is ingested and stored in Snowflake data warehouse. 

3) Quality Checks  
Used dbt to perform data quality checks on both raw and transformed data.

4) Building dbt Models  
Data is transformed using dbt modular approach.

5) Streamlit Web Application  
Developed  a user-friendly web application using Streamlit.  


Not the greatest project but yeah achieved what I wanted to make.

&#x200B;

https://preview.redd.it/zz09hu5cdwhc1.png?width=1893&format=png&auto=webp&s=d326943976474f484f4def373fa16d46c283a7a2

https://preview.redd.it/xe30su5cdwhc1.png?width=1827&format=png&auto=webp&s=8b29c34af31c1deae92958f14949609a5913c917

&#x200B;

https://preview.redd.it/24xzpwwldwhc1.png?width=4420&format=png&auto=webp&s=ddc293c0b23b1329fc345ee764efb136f0d167ac",133,24,datainsightguy,2024-02-11 06:07:55,https://www.reddit.com/r/dataengineering/comments/1ao15gx/i_built_my_first_end_to_end_data_project_to/,False,False,False,False
1abov9g,"Something for fun, what abilities would you give this card?",,134,24,AMDataLake,2024-01-26 18:12:55,https://i.redd.it/8om179jgstec1.jpeg,False,False,False,False
15iu5ya,"Don't fall for the ""Data is Beautiful"" post with the mug. It is an ad. Mods, is there anything we can do about shit like this?","I have been blocked on the post, but OP is clearly running a sock puppet network as I detailed in (rapidly downvoted) comments in the post.",133,26,mojitz,2023-08-05 12:14:18,https://www.reddit.com/r/dataengineering/comments/15iu5ya/dont_fall_for_the_data_is_beautiful_post_with_the/,False,False,False,False
12anr2k,"COVID-19 data pipeline on AWS feat. Glue/PySpark, Docker, Great Expectations, Airflow, and Redshift, templated in CF/CDK, deployable via Github Actions",,133,37,smoochie100,2023-04-03 15:24:44,https://i.redd.it/4qpi4llisora1.png,False,False,False,False
xkb4h8,hmmm,,130,29,gerciuz,2022-09-21 17:23:37,https://i.redd.it/9jmsuq6ww8p91.png,False,False,False,False
u99v1q,Data engineering blogs worth reading,"Hi guys!

I'm looking for valuable data engineering/data science blogs in English. Any recommendations?

Is there anyone you follow on medium?",135,20,Anna-Kraska,2022-04-22 08:38:22,https://www.reddit.com/r/dataengineering/comments/u99v1q/data_engineering_blogs_worth_reading/,False,False,False,False
144hveq,GlareDB: An open source SQL database to query and analyze distributed data,"Hi everyone, founder at GlareDB here.

We've just open sourced GlareDB, a database for querying distributed data with SQL. Check out the repo here: [https://github.com/GlareDB/glaredb](https://github.com/GlareDB/glaredb)

We have integrations with Postgres, Snowflake, files in S3 (Parquet, CSV), and more. Our goal is to make it easy to run analytics across disparate data sources using just SQL, reducing the need to set up ETL pipelines to move data around.  Take a look at our [docs](https://docs.glaredb.com/docs/working-with-your-data/querying.html#querying-multiple-data-sources) to see what querying multiple data sources looks like. We've also recently merged in a [PR](https://github.com/GlareDB/glaredb/pull/1086) letting you run queries like `select * from read_postgres(...)`.

GlareDB is still early stages, and we have a lot planned the next few months. Have a use case that you think GlareDB is a good fit for? Let us know! And if you have any feature request for things you'd like to see, feel free to open up an issue.",135,24,sean-glaredb,2023-06-08 19:02:21,https://www.reddit.com/r/dataengineering/comments/144hveq/glaredb_an_open_source_sql_database_to_query_and/,False,False,False,False
12th15p,Live coding interview hatred,I DESPISE live coding interviews. I’m a good engineer and I can talk through skills and whiteboard and data model interview just fine. But seriously ask me a basic select statement in sql live and I barely remember how to do that. Panic sets in immediately and I barely make it through. I promise give me an hour to code something real and it will be done but just don’t make me live code. I have almost 10 years experience and can barely write sql in a coding interview. It’s just really rough.,134,54,k-dani-b,2023-04-20 22:15:30,https://www.reddit.com/r/dataengineering/comments/12th15p/live_coding_interview_hatred/,False,False,False,False
12r8x5c,"Zillacode Premium finally done, Leetcode for PySpark, Spark and Pandas at Zillacode.com",,131,29,dmage5000,2023-04-18 23:37:58,https://i.redd.it/64htp9vsaqua1.jpg,False,False,False,False
zamewl,"What's ""wrong"" with dbt ?","I'm looking to learn more about dbt(core) and more specifically, what challenges teams have with it. There is no shortage of ""pro"" dbt content on the internet, but I'd like to have a discussion about what's *wrong* with it. Not to hate on it, just to discuss what it could do better and/or differently (in your opinion).

For the sake of this discussion, let's assume everyone is bought into the idea of ELT and doing the T in the (presumably cloud based) warehouse using SQL. If you want to debate dbt vs a tool like Spark, then please start another thread. Full disclosure: I've never worked somewhere that uses dbt (I *have* played with it) but I know that there is a high probability my next employer(regardless of who that is) will already be using dbt. I also know enough to believe that dbt is the best choice out there for managing SQL transforms, but is that only because it is the only choice?

Ok, I'll start.

* I hate that dbt makes me use references to build the DAG. Why can't it just parse my SQL and infer the DAG from that? (Maybe it can and it just isn't obvious?)",130,87,dadaengineering,2022-12-02 13:34:26,https://www.reddit.com/r/dataengineering/comments/zamewl/whats_wrong_with_dbt/,False,False,False,False
z52hou,Scaled to 1M cores in EKS,,132,61,buachaill_beorach,2022-11-26 08:34:28,https://i.redd.it/18sxhhpxsa2a1.jpg,False,False,False,False
xx5gv0,Rant: Frustrating employer and salaries,"Just need to rant for a second.

Employer led on a qualified, hungry coworker who would have been an excellent addition to my team. Ultimately didn't hire him as a DE because they thought he was ""absolutely qualified and worth what he was asking, but they want to pay someone much less"". Couldn't do anything to convince them otherwise, short of threatening to quit, which is a bluff I can't afford to carry out in these economic times.

I just feel apathetic about everything now. I'm taken care of, but my coworker and friend got screwed, and it's taken me from being hungry and ambitious to ""quiet quitting"" (hate the term). I have no desire to go beyond my duties/hours, even if it means more compensation, and certainly no desire to move forward with hiring a new Jr.

It's like being pregnant in a post-apocalyptic world.

Anyway, thanks for listening, rant over.",132,64,MakeoutPoint,2022-10-06 13:36:39,https://www.reddit.com/r/dataengineering/comments/xx5gv0/rant_frustrating_employer_and_salaries/,False,False,False,False
vwcl1q,Data Science is like playing with Chiellini,,130,7,VedraiSpA,2022-07-11 07:26:45,https://i.redd.it/53v40c9e5wa91.jpg,False,False,False,False
s8gegf,Graduating from ETL Developer to Data Engineer,,130,51,FortunOfficial,2022-01-20 10:55:49,https://medium.com/google-cloud/graduating-from-etl-developer-to-data-engineer-7663dfbdfd2d,False,False,False,False
q0alnb,Please Critique my Resume: Data Analyst transitioning to Data Engineer,,135,68,JustusPaulus,2021-10-03 04:23:45,https://i.redd.it/73brtbxjw5r71.png,False,False,False,False
l66fyz,8 Data Engineering Evangelists to Follow,"Hi folks, I want to share my list of scientists/business leaders who I follow to stay up-to-date about what is going on on the data engineering scene.

Feel free to share your bookmarks in the comments.

👉  [John Lafleur](https://medium.com/@jeanlafleur)  \- Co-Founder of Airbyte - writes about ETL /  ELT and his startup journey

👉  [Tristan Handy](https://medium.com/@jthandy) \- Co-founder of Fishtown, created by dbt -  about startups, trends in data analytics and a little bit about Fishtown

👉  [Connor Shorten](https://connorshorten300.medium.com/) \- Computer Science Ph.D. at FAU - about Computer Vision, Natural Language Processing, Graph Embeddings, Generative Adversarial Networks, Reinforcement Learning, and more

👉  [Jürgen Schmidhuber](http://people.idsia.ch/~juergen/) \- computer scientist, researcher, keynote speaker, co-director of the Dalle Molle Institute for Artificial Intelligence Research - about the science of AI

👉 [Sébastien Derivaux](https://dataintoresults.com/post/category/thoughts/) \- shareholder and board member of many startups - about data science and startups

👉 [Jesse Anderson](https://www.jesse-anderson.com/category/blog/) \- data engineering evangelist - all-around data engineering in a simple way

👉 [David Layton](https://medium.com/@dmlayton) \- former CERN physicists, data engineer & scientist - about agile, data management, tools

👉 [George Fraser](https://twitter.com/frasergeorgew) \- CEO of Fivetran - about trends in data preparation and management",131,25,an_tonova,2021-01-27 15:11:14,https://www.reddit.com/r/dataengineering/comments/l66fyz/8_data_engineering_evangelists_to_follow/,False,False,False,False
11xcy2g,I don't understand DuckDB,"I understand it's an in process OLAP database. Ok, fine, that's what everyone have been saying since its inception.  I understand it's great and everyone loves it.

I also have no ducking clue what I'm supposed to do with it. Is it supposed to be a drop in replacement for pandas? Or spark?

Or am I supposed to ship it alongside an analytics app to make fast calculations, like Hex does?

Or maybe run it in a pod and use that to perform the T in ETL? Or all of the above?

\---

Can you ELI5 it to me?",128,146,wtfzambo,2023-03-21 10:27:40,https://www.reddit.com/r/dataengineering/comments/11xcy2g/i_dont_understand_duckdb/,False,False,False,False
xbvuul,Rewriting the data pipeline,,132,2,threddyrex,2022-09-11 22:32:28,https://i.redd.it/kbya3b0i3bn91.jpg,False,False,False,False
x5thfz,"Definition of the Data Engineer role, IMHO.",,131,15,schenkd,2022-09-04 18:07:31,https://medium.com/@pydave/definition-of-the-data-engineer-role-imho-ea04558861fd,False,False,False,False
vgwcgg,This is actually what broke her heart.,,131,21,juan_solo_,2022-06-20 21:28:48,https://i.redd.it/y10g6lbmgu691.png,False,False,False,False
14xhi13,Is it normal to feel completely lost during initial months of your data engineering job ?,"I got into a data engineering role, it's my first job as a DE. And i am feeling absolutely lost, i don't understand what's happening, everything is everywhere, my team mates are very busy so no one properly explains what's happening and some structural change is happening in the whole section of DE teams. And I feel absolutely overwhelmed.
How do you tackle this?",130,31,jojobaoil68,2023-07-12 07:44:51,https://www.reddit.com/r/dataengineering/comments/14xhi13/is_it_normal_to_feel_completely_lost_during/,False,False,False,False
13ip8e5,"Is there something wrong with me, I hate dbt, what am I missing ?","Fairly self explanatory. It’s not very fast, no nice REPL for prototyping, large ETL seem to end up being a big mess of SQL in different models. I find developing using the tool extremely boring. Errors aren’t caught when run, dbt expectations is less expressive than if you just wrote your own assertions etc, etc. As far as I can tell the only benefit relative to a tool like DBR which also optimises DAGs is DDL statement automation. Could someone please tell me what I’m missing here.",133,110,PeruseAndSnooze,2023-05-16 00:09:00,https://www.reddit.com/r/dataengineering/comments/13ip8e5/is_there_something_wrong_with_me_i_hate_dbt_what/,False,False,False,False
11xbpjy,Beware of Fivetran and other ELT tools.,"I posted this on another thread but felt like more data engineers should be aware of these issues with Fivetran and other ELT tools:

Fivetran is terrible for these reasons:

- slow to fix issues or problems when they are discovered
- they alter field names and change data structure thereby making it very difficult to migrate to other options if the need arises.
- for some data sources they force you to ingest all objects thereby increasing your costs - great for them as it makes them more money
- they constantly have issues - we would get emails very regularly identifying problems with their system
- within 6 months of us cancelling we identified an issue where Fivetran was incorrectly identifying primary keys with the Pendo trackevents object.  We raised this with the support team and they denied there was an issue.  Maybe 4 weeks later they sent out an email admitting they had an issue and refused to credit us for the reprocessing of data we incurred trying to fix it.  Their fix also took about 2 months to implement.  We later learned we had dropped over 1 billion rows of data due to this issue.
- lack of transparency with all the transformations and adjustments they make (yes I know they have schema charts but the transparency goes beyond this)
- enormous expenses for loading data - we were getting charged around 30k to reload Pendo data when we were able to do it ourselves for about 3k.
- SLAs are non existent.  They have a 12 hour buffer.  Most integrations get flagged as “delayed” and there are no clear answers why.
- They pick and chose what data on each object they pull in.  Don’t assume they bring in all fields that are available on all endpoints.

We used fivetran for a few years and got off it last November.  

If you have the skill set to develop and support your own integration framework (Python in our case) I highly recommend it. It is much cheaper, you have full visibility into your data, you don’t get locked into anyone’s architecture, you can troubleshoot issues very quickly, and you can validate the accuracy of the data you are receiving.

For reference we are supporting over 700 objects with only one headcount.  If you build out a strong well thought out foundation you don’t need a ton of people.",127,118,TheCauthon,2023-03-21 09:13:30,https://www.reddit.com/r/dataengineering/comments/11xbpjy/beware_of_fivetran_and_other_elt_tools/,False,False,False,False
107rt91,Azure Synapse Analytics is absolute trash for anything bigger than a few extractions,"I have tried to work with this platform. I have given it every opportunity I can, but never have I seen so much trouble from a tool or framework I had to work with. I started the new year with a new years resolution: to write down every problem as I encounter them. I don't know if I should continue with it, since I might fill up my e-reader before the year is over.

In no particular order:

1. Synapse expressions have no support for the case() function. Have fun chaining if else's!
2. Not all activities have retries. What do you mean you want to put a retry on a pipeline? That's crazy talk!
3. SQL scripts are saved as JSON. Not even JSON5. Fucking JSON. I hope you like diff checking one fucking long ass line of SQL!
4. Browser IDE... Whoever thought this up deserves fish hooks up their ass. You can't even save properly in the stupid thing, because every single thing is counted as a commit. And before some asshat suggests committing after the work is done: you deserve the fish hook too.
5. Pipeline variables do not include int's (and some other types for that matter) for whatever reason. Converting everything to string and back is fun isn't it?
6. Pipelines don't have output parameters, so even if I wanted to make reusable modules for missing Synapse functionality I literally can't unless I start using them as error parameters (and if you try to suggest that I WILL shank you).
7. No global parameters EVEN THOUGH DATA FACTORY HAD IT AND IT'S BEEN REQUESTED FOR ALMOST 2 YEARS.
8. 2022 and still no dark mode. This has been requested for data factory in 2016. Yes you're old and so am I.
9. The SQL editor has all the great functionalities notepad has.
10. I hope you didn't name one of your workspaces incorrectly. Oh you did? RIP, time to remake it.
11. Local timezones? You mean UTC? What do you mean + or -? You craycray!
12. Nesting loops or if tests can't be done. You need to make separate pipelines for it and before someone asks: no, just because it's a nesting it REALLY doesn't mean it belongs in a separate pipeline.
13. Speaking about nesting: if you use an if test in a foreach, you can't access the current item of the loop. Haven't you learned by now? You're using Synapse. Now eat shit.
14. Self-Hosted Integration Runtimes cannot be shared, while you can actually do that in data factory. This means you need to run three separate runtimes just to get to on-premise sources.
15. On the topic of self-hosted integration runtimes: why do you even need them at all? Why is it not possible to peer a vnet that contains synapse and be done with it?
16. What's even the use of the SQL scripts if you can't access them from pipelines. You get the script activity, but that's just another fucking place to dump your SQL in. And if you dare to suggest to use the API to trigger/request the SQL script. I will adopt a dog just to feed you to it.
17. Testing? HAHAHAHAHAHAHAHAHAHA- fuck you.
18. You want to rename a variable? Lol you little shit now you have to search for every instance to rename it. Refactoring? Just become a 100x dev yo.
19. Parameters in pipeline templates? Why would you want parameters in your pipeline templates? You talk like you want to reuse the code you wrote or something. Oh you do? ........oof.
20. Git....lab? Sounds like a dangerous cocaine facility. Hope you weren't planning to attach that to synapse! Or like literally any other option since you have to attach git to synapse in the first place.

These are the ones from THIS year, so after one week. I have had the displeasure of using it for a year now. So here's my advice after this long-winded rant: use it for a quick prototype of anything and don't use it for anything bigger than that. If you do, you have learned nothing of all the improvements that people have brought to the art that is development and you should probably touch grass instead of sucking cock on LinkedIn.",131,41,AirisuB,2023-01-09 22:12:14,https://www.reddit.com/r/dataengineering/comments/107rt91/azure_synapse_analytics_is_absolute_trash_for/,False,False,False,False
vjkarw,"ELT of my own Strava data using the Strava API, MySQL, Python, S3, Redshift, and Airflow","Hi everyone! Long time lurker on this subreddit - I really enjoy the content and feel like I learn a lot so thank you! 

I’m a MLE (with 2 years experience) and wanted to become more familiar with some data engineering concepts so built a little personal project. I build an EtLT pipeline to ingest my Strava data from the Strava API and load it into a Redshift data warehouse. This pipeline is then run once a week using Airflow to extract any new activity data. The end goal is then to use this data warehouse to build an automatically updating dashboard in Tableau and also to trigger automatic re-training of my Strava Kudos Prediction model.

The GitHub repo can be found here: https://github.com/jackmleitch/StravaDataPipline
A corresponding blog post can also be found here: https://jackmleitch.com/blog/Strava-Data-Pipeline

I was wondering if anyone had any thoughts on it, and was looking for some general advice on what to build/look at next! 

Some things of my further considerations/thoughts are: 

- Improve Airflow with Docker: I could have used the docker image of Airflow to run the pipeline in a Docker container which would've made things more robust. This would also make deploying the pipeline at scale much easier!

- Implement more validation tests: For a real production pipeline, I would implement more validation tests all through the pipeline. I could, for example, have used an open-source tool like Great Expectations.

- Simplify the process: The pipeline could probably be run in a much simpler way. An alternative could be to use Cron for orchestration and PostgreSQL or SQLite for storage. Also could use something more simple like Prefect instead of Airflow! 

- Data streaming: To keep the Dashboard consistently up to date we could benefit from something like Kafka.

- Automatically build out cloud infra with something like Terraform.

- Use something like dbt to manage data transformation dependencies etc.

Any advice/criticism very much welcome, thanks in advance :)",129,26,BraveCoconut98,2022-06-24 09:21:03,https://www.reddit.com/r/dataengineering/comments/vjkarw/elt_of_my_own_strava_data_using_the_strava_api/,False,False,False,False
gpie3f,"Couldn't find a good comprehensive article on setting up Airflow 6 months ago. I wrote one here: a setup using docker-compose, and included instructions on setting up PyCharm too! Hope you could get something out of it!",,133,7,teddyhar,2020-05-24 03:14:23,https://medium.com/ninjavan-tech/setting-up-a-complete-local-development-environment-for-airflow-docker-pycharm-and-tests-3577ddb4ca94,False,False,False,False
173fj4h,Anyone Else Seeing Salaries Collapse?,"Had to leav my $130K remote job in August after 6 toxic months.  Had to take a 100% on site gig for $110K last month at a bad company to keep food on the table. Been still interviewing heavily to find something better. 

When I was casually looking around late last year, jobs were around $140K - $160K at my experience level, but I couldn’t land anything. 

 Even though I make $110K now, I tell recruiters I make $150K in my new role. Most refute and say that their max budget is $120Kish and full on site. 

This is ~20% lower pay than what was being thrown around late last year, and on site. Anyone experiencing similiar?

I’m also might just be in a bad spot since I had to take this new gig, and people see my 2 short tenures in a row as a red flag. Advice?",128,93,None,2023-10-09 01:21:20,https://www.reddit.com/r/dataengineering/comments/173fj4h/anyone_else_seeing_salaries_collapse/,False,False,False,False
1497ngt,I missed you guys,That is all,132,32,None,2023-06-14 13:16:53,https://www.reddit.com/r/dataengineering/comments/1497ngt/i_missed_you_guys/,False,False,False,False
ug9r0w,The best SQL question you have been asked in a DE/DS interview?,"I have my interviews coming up, i would really appreciate if you could provide me with your favourite/ most interesting SQL questions you have encountered so far in interviews.

P.S - I think many of us will have this question
Thanks in Advance 🙂",127,109,lucky-Chipmunk-119,2022-05-01 21:22:30,https://www.reddit.com/r/dataengineering/comments/ug9r0w/the_best_sql_question_you_have_been_asked_in_a/,False,False,False,False
euuelg,Trying to architect a new dataset be like...,,127,6,Not-NedFlanders,2020-01-27 21:05:20,https://i.redd.it/l8nxppstydd41.jpg,False,False,False,False
150mfrd,Why do data engineers have so much to learn?,"I am a student preparing to get a job as a data engineer.

1. linux, Python, SQL, JAVA or Scala
2. Cloud Knowledge
3. Docker, Kubernetes
4. Server security + data security/quality
5. Database (""Cassandra"", ""Mongo"", ""Mysql"", ""postgres"", ""redis"")
6. ELK or (fluentd, Opensearch)
7. Kafka
8. Spark Stream or Flink
9. Spark or Trino
10. table format ( Iceberg/deltalake/ Hudi)
11. snowflake/ Redshfit / Bigquery   
12.  OLAP Data Modeling  
13. Airflow

  
I only wrote down the essentials.  
There are many good tools like dbt, lakeFS, etcBut in addition  
Companies may still require Hadoop Echo System (Hive/HDFS/Hbase).  
They might ask us to build a dashboard with javascript or python.  
They can also ask us to create the web.  


From number 1 to 13, each one has a very difficult and difficult concept to master.What I'm really curious about is how much I need to know and how much I need to master them to apply for a company.

I'm sorry. Actually, I was whining because I got hit with reality while studying.",126,90,Hankaul,2023-07-15 20:40:52,https://www.reddit.com/r/dataengineering/comments/150mfrd/why_do_data_engineers_have_so_much_to_learn/,False,False,False,False
13xkeov,"Orchestration: Thoughts on Dagster, Airflow and Prefect?","I’ve read the articles, looked at the websites, but want to hear from people who’ve actually done it. How do the three compare? What are the downsides of each? What’s your thought process in choosing an orchestrator anyway?",129,110,MrMosBiggestFan,2023-06-01 15:23:31,https://www.reddit.com/r/dataengineering/comments/13xkeov/orchestration_thoughts_on_dagster_airflow_and/,False,False,False,False
11mtgx0,What dataset would you pay good money to get your hands on?,"Just a lighter post for a change. 

I'll go first, I'd sell my car for an api access to the myFitnessPal data.

So many interesting findings out of it. What do people eat? Per country? Age group? How does it relate to their fitness goal? Such a gold mine.",126,92,holiquetal,2023-03-09 14:22:46,https://www.reddit.com/r/dataengineering/comments/11mtgx0/what_dataset_would_you_pay_good_money_to_get_your/,False,False,False,False
xwfkd6,"me irl: ""I've been using SQL for fifteen years! How hard could python be?""",,128,18,manpace,2022-10-05 16:42:09,https://www.youtube.com/watch?v=DF_brfZPmjM,False,False,False,False
r2853m,Why is learning data engineering so opaque,"I am a full stack developer trying to learn more about data engineering, but so far everything is so damn opaque. I know front end has its own messes, but at least at this point everyone is unified under one programming language, a couple open source frameworks, and a couple open source package managers. It’s not hard to find tutorials frontend or backend that have you developing locally in minutes with commonly accepted tools on sound examples.

Meanwhile, under Azure there are so many services that seem like they should do the same thing. Synapse and Databricks. Data Explorer and Analysis Services. Data Factory and HDInsight. Iot hub and stream analytics. And learning any one of them is an annoying exercise in setting up an account and learning a user interface that will be swapped out in a couple years. There are no definitive starting places it feels like.

Where should I start? I know Python/SQL well and I’ve read Kimball and Designing Data Intensive Applications and want to start applying this stuff but can’t even begin to know what tech to choose.",129,35,ClittoryHinton,2021-11-25 22:26:52,https://www.reddit.com/r/dataengineering/comments/r2853m/why_is_learning_data_engineering_so_opaque/,False,False,False,False
1bly2h0,Feel like an absolute loser,"Hey, I live in Canada and I’m going to be 27 soon. I studied mechanical engineering and working in auto for a few years before getting a job in the tech industry as a product analyst. My role is has a analytics component to it but it’s a small team so it’s harder to learn when you’ve failed and how you can improve your queries.

I completed a data engineering bootcamp last year and I’m struggling to land a role, the market is abysmal. I’ve had 3 interviews so far and some of them I failed the technical and others I was rejected. 

I’m kinda just looking at where my life is going and it’s just embarrassing - 27 and you still don’t have your life figured out and ur basically entry level.

Idk why in posting this it’s basically just a rant.",128,75,seikoalpinist197,2024-03-23 17:43:51,https://www.reddit.com/r/dataengineering/comments/1bly2h0/feel_like_an_absolute_loser/,False,False,False,False
1axd7cy,What are your Top SQL Query Optimization tips?,"Share your favorite tips for writing better SQL, your pet peeves and best practices.",127,73,data_engineer_,2024-02-22 18:09:24,https://www.reddit.com/r/dataengineering/comments/1axd7cy/what_are_your_top_sql_query_optimization_tips/,False,False,False,False
10vnmzj,I’m organizing an AMA with Joe Reis (Co-author of fundamentals of data engineering). Feel free to send in your questions!,"Send in your questions. If you’d like to join the linkedin live, DM me for the link.

Edit: Updating the link to LinkedIn Live: https://www.linkedin.com/video/event/urn:li:ugcPost:7028827342466662400/",124,24,vino_and_data,2023-02-07 00:49:54,https://www.reddit.com/r/dataengineering/comments/10vnmzj/im_organizing_an_ama_with_joe_reis_coauthor_of/,False,False,False,False
ymz0oh,Can you share some of the best software engineering practices you can apply to data pipelines?,"The ones you feel are critical, but are missing, especially if the pipeline was developed by beginners.",127,77,swapripper,2022-11-05 16:36:30,https://www.reddit.com/r/dataengineering/comments/ymz0oh/can_you_share_some_of_the_best_software/,False,False,False,False
rekhhn,Guide to read The Data Warehouse Toolkit to save you from reading cover-cover and outdated topics.,[https://www.holistics.io/blog/how-to-read-data-warehouse-toolkit/](https://www.holistics.io/blog/how-to-read-data-warehouse-toolkit/),128,27,None,2021-12-12 08:13:59,https://www.reddit.com/r/dataengineering/comments/rekhhn/guide_to_read_the_data_warehouse_toolkit_to_save/,False,False,False,False
1c3c04e,Are take-home assignments for Data Engineering roles getting out of hand?,"I'm interviewing for lead and senior data engineering positions, and I'm starting to feel like companies are getting free work out of me. These take-home assignments are ridiculously long, and I'm essentially working for free. Is this normal?

One startup asked me to design their entire data platform, outline data science workloads, and address a 5-page list of requirements – all for a job. This felt less like an 'assignment' and more like a full-fledged consulting project.",126,40,randomusicjunkie,2024-04-13 20:58:33,https://www.reddit.com/r/dataengineering/comments/1c3c04e/are_takehome_assignments_for_data_engineering/,False,False,False,False
1al2r0o,One Trillion Row Challenge (1TRC),"I really liked the simplicity of the [One Billion Row Challenge (1BRC)](https://github.com/gunnarmorling/1brc) that took off last month.  It was fun to see lots of people apply different tools to the same simple-yet-clear problem “How do you parse, process, and aggregate a large CSV file as quickly as possible?”For fun, my colleagues and I made a One Trillion Row Challenge (1TRC) dataset 🙂.  

Data lives on S3 in Parquet format (CSV made zero sense here) in a public bucket at s3://coiled-datasets-rp/1trc and is roughly 12 TiB uncompressed.

We (the Dask team) were able to complete the TRC query in around six minutes for around $1.10.For more information see [this blogpost](https://medium.com/coiled-hq/one-trillion-row-challenge-5bfd4c3b8aef) and [this repository](https://github.com/coiled/1trc/)",126,14,mrocklin,2024-02-07 13:19:59,https://www.reddit.com/r/dataengineering/comments/1al2r0o/one_trillion_row_challenge_1trc/,False,False,False,False
16iaaku,Love you guys!,"This is one of the most helpful subs I have come across personally. For me, I have learnt about how to progress in my career, help for interviews and certs also.
So, thank you to everyone who keeps commenting and figuring out ways to help others with their queries.",126,25,xander800,2023-09-14 05:59:35,https://www.reddit.com/r/dataengineering/comments/16iaaku/love_you_guys/,False,False,False,False
p4lm79,My Path to Becoming a Data Engineer in FAANG,"I hope I'm not violating any rules by posting this here, but I wanted to share a blog post that I wrote that outlines my background, work experience, and the interview process I recently went through to become a Data Engineer at Facebook. The blog was written in response to some questions/inquiries I was getting in another post, so I apologize if you're seeing it twice.

The blog can be found [here](https://tibblesnbits.com/posts/de-interview-faang), and I'm hopeful that it helps at least one person thinking about applying to a role like this. Spoiler alert: you're more qualified than you think you are!",125,36,therealtibblesnbits,2021-08-15 02:56:10,https://www.reddit.com/r/dataengineering/comments/p4lm79/my_path_to_becoming_a_data_engineer_in_faang/,False,False,False,False
1ccsf39,The Data Engineering Hype Cycle is beginning (??),"Credit to Julien H for sharing this on Linkedin. Kinda didn't believe it at first but there you go

&#x200B;

https://preview.redd.it/j62ujlacrmwc1.png?width=1884&format=png&auto=webp&s=4f324f6092a8dab48f6bc919cc13d8bcd5b10b57

Looks like people are realising our importance for Gen AI slowly, but surely.",126,76,engineer_of-sorts,2024-04-25 13:49:13,https://www.reddit.com/r/dataengineering/comments/1ccsf39/the_data_engineering_hype_cycle_is_beginning/,False,False,False,False
t2mh65,Why this subreddit dislikes the so-called Modern Data Stack?,"Everything is in the title.

I feel like a lot of posts over here are gatekeeping data engineering. For example, I have seen messages where people do not understand dbt popularity (""this is just templatized SQL""). Another example I see is Datalake (S3+Spark) >>> Data Warehouse solutions (Redshift, Snowflake, Bigquery).

Why?

For me, they all help me to:

\- Avoid spending time on maintaining brittle in-house solutions

\- Avoid spending time developing boring ingestion pipelines

\- Avoid spending time setting up complex platform (Spark)

\- Focus on impactful projects

\- Reduce the time to insights

\- Facilitate collaborations with other data functions.

The ""ETL way"" requires a lot more data engineering people and effort vs the Modern Data Stack that can be nearly created by a data analyst. This is a big win for the company.

This is not a troll. I feel like I got hyped by the Modern Data Stack but for now, I only see wins. I'm interested in other perspectives and where the Spark and co. architecture would fit.",127,109,Alert_Dragonfly,2022-02-27 11:41:16,https://www.reddit.com/r/dataengineering/comments/t2mh65/why_this_subreddit_dislikes_the_socalled_modern/,False,False,False,False
1750zdx,Is Python our fate?,"Is there any of you who love data engineering but feels frustrated to be literally forced to use Python for everything while you'd prefer to use a proper statistically typed language like Scala, Java or Go?

I currently do most of the services in Java. I did some Scala before. 
We also use a bit of Go and Python mainly for Airflow DAGs. 

Python is nice dynamic language. I have nothing against it. 
I see people adding types hints, static checkers like MyPy, etc... 
We're turning Python into Typescript basically. And why not? That's one way to go to achieve a better type safety. 
But ...can we do ourselves a favor and use a proper statically typed language? 😂

Perhaps we should develop better data ecosystems in other languages as well. 
Just like backend people have been doing. 

I know this post will get some hate. 

Is there any of you who wish to have more variety in the data engineering job market or you're all fully satisfied working with Python for everything?

Have a good day :)",126,287,yinshangyi,2023-10-11 00:37:34,https://www.reddit.com/r/dataengineering/comments/1750zdx/is_python_our_fate/,False,False,False,False
167x47m,Data Engineers overcomplicate things,"As a DE with a few years under my belt, I am starting to believe that with the glutt of tools available, the industry is now over-saturated to the point that we are making it un-needlessly overcomplicated for ourselves. Trying to automate everything and get a one size fits all solution for data that is anything but. What is wrong with having one tool for one thing, and one for another? Why have an airflow instance that calls 150 different dependencies? Make 10 that call 15. Getting bored of the whole ""my airflow instance launches 300 notebooks and can't find the root cause"" posts. 

Let's get back to basics, simplify data, write good documentation and spend time managing 30 DPL's that work rather than unpicking one fucking giant one that never does.",124,54,None,2023-09-02 08:35:48,https://www.reddit.com/r/dataengineering/comments/167x47m/data_engineers_overcomplicate_things/,False,False,False,False
10rudcp,How do you handle increasing stress?,"I'm a junior DE working with a small team. Recently I was shadowing a senior DE who abruptly quit. I've been given their entire work load and feel completely overwhelmed. I also found out from my manager that the information the senior DE was giving me was wrong, to the point where my manager said he thinks they were sabotaging me but doesn't know why they would do that. The senior DE also deleted all of their data/workflows/processes and code.

So now were set back in some instances nearly two years and I'm working 14-16 hour days trying to rebuild things that are completely out of my area of knowledge and at the same time I'm getting pressure from different stakeholders to deliver data and products that I haven't even had enough time to rebuild yet or even learn about.

I hate to sound like a cry baby but I feel totally overwhelmed and like a duck drowning.

My manager is trying to intercept as many stakeholders as he can to give me time while nudging me along.

How do you all handle it? Any tools or tips?",127,106,xxEiGhTyxx,2023-02-02 16:44:58,https://www.reddit.com/r/dataengineering/comments/10rudcp/how_do_you_handle_increasing_stress/,False,False,False,False
xw7vzv,Apache Iceberg Reduced Our Amazon S3 Cost by 90%,,123,19,dnzprmksz,2022-10-05 11:19:43,https://medium.com/insiderengineering/apache-iceberg-reduced-our-amazon-s3-cost-by-90-997cde5ce931,False,False,False,False
vywuj2,Requirements Gathering is terrible. What is your best method.," Ok I’m just going to say it requirements gathering is terrible and you have to pry every bit of useful information. No this is not one of the hundreds of sales posts on this sub but a DE that has spent years fighting with users over requirements. I just crossed a decade, been in multiple lead/architect/ manager roles but still requirement are the bane of my work life all these years. 

Either I don’t ask enough questions and don’t spend enough time with the users which causes rework OR I I go overboard and they hate me for having to specify every minute detail. I have never any many attempts been able to get the requirements perfect and the users happy. What do you do? What are your best practices.",122,37,wytesmurf,2022-07-14 14:12:01,https://www.reddit.com/r/dataengineering/comments/vywuj2/requirements_gathering_is_terrible_what_is_your/,False,False,False,False
uzr5ks,The inflated world of data: have you actually seen a business decision taken based on your analytics work?," Let me clarify a bit. I got experience as a Data Analyst and a Data Engineer, built ETL pipelines, wrote loads of SQL scripts, built a small ETL platform in Python, worked with all sorts of data tools and did more than healthy amount of data modeling.

All for the sake of analytics / machine learning.

Yet I have never ever seen or heard about the results myself. With my colleagues we created dashboards in Tableau, no idea how it got used. I built pipelines and dwh-s, no idea if actual actionable insight was ever made out of these beyond after I built it.

* How often does the moment when a decision maker looks at a visualization and says, ""aha! We gonna do this from now on"" exist?
* How often does an actual ML product makes it into active live prod state bringing in the money or saving on the expenses?
* How often do orgs build a DWH and then just ignore it, because they realize they don't know what to do with it?

Managements love to dream big and depending on managers and ideas, I can absolutely love it, because it's innovative and smart **or** I can hate it, because it's an abstract bullshit. I experienced both. But how often do we get to make these ideas into something tangible value? Do we, as data professionals, really provide the value business are hoping for?

Sometimes i'm afraid that the data world is significantly more inflated, than anyone can guess, but I hope I'm wrong, because I really like my chosen profession. What do you think? If you got a story to share for both pro or con, I would be really interested.",126,47,Labanc_,2022-05-28 16:44:44,https://www.reddit.com/r/dataengineering/comments/uzr5ks/the_inflated_world_of_data_have_you_actually_seen/,False,False,False,False
uet6ty,Got a job!,"I wanted to say thanks - I'm mostly a lurker but I have been paying attention, especially to resume and interview advice and have just accepted a job as a Data Engineer (officially: Senior Software Engineer) with a 36% raise (to $140k from $103k) from my current Data Analyst role. I'm super stoked!

Sounds like they mostly use Python for ETL and aren't in the cloud so its a pretty simple stack for now...",127,33,QueenScorp,2022-04-29 19:11:36,https://www.reddit.com/r/dataengineering/comments/uet6ty/got_a_job/,False,False,False,False
tnejkq,Really Detailed Company Interview Guide for DE and Other Data Jobs,,125,15,el_jeep0,2022-03-25 05:01:53,https://www.interviewquery.com/companies,False,False,False,False
t0jn12,"So now dbt is worth $4.2b! Yes, that's a ""b"" for billion.","[https://blog.getdbt.com/next-layer-of-the-modern-data-stack/](https://blog.getdbt.com/next-layer-of-the-modern-data-stack/)

Seems all you need is 1800 customers (each is worth $2.3M), 25,000 humans on Slack and to call people ""humans"" a lot.

Is this not a supreme example of VC hubris?",124,55,Revolutionary-Mix739,2022-02-24 19:48:45,https://www.reddit.com/r/dataengineering/comments/t0jn12/so_now_dbt_is_worth_42b_yes_thats_a_b_for_billion/,False,False,False,False
15h6qw5,"Just had a technical interview, got roasted on streaming, distributed computing and k8s 😬","So I just got out of a technical interview with the tech advisor of a small company I recently applied for, and while the first half went well, the 2nd I'm afraid not so much.

I was asked several questions about the aforementioned technologies, which I ""know"" only superficially, but I never really had to deal with them first hand.

I would like to deepen my knowledge about them, but admittedly at my current company we don't really have a use case for them.

So I'm asking you, what resources would you recommend to learn about them on my own?

Any contribution is more than welcome :)",125,120,wtfzambo,2023-08-03 15:05:11,https://www.reddit.com/r/dataengineering/comments/15h6qw5/just_had_a_technical_interview_got_roasted_on/,False,False,False,False
158sqwa,What's the best strategy to merge 5500 excel files?,"I'm working with a client that has about 5500 excel files stored on a shared drive, and I need to merge them into a single csv file. 

The files have common format, so I wrote a simple python script to loop through the drive, load each file into a dataframe, standardize column headers, and then union to an output dataframe.

Some initial testing shows that it takes an average of 40 seconds to process each file, which means it would take about 60 hours to do everything.

Is there a faster way to do this?

Edit:
Thanks for all the advice. I switched to polars and it ran dramatically faster. I got the total time down to about 10 hours and ran it overnight.

Answering a couple questions that people brought up:

* It took 40 seconds to go through each file because all files were in xlsm format, and it seems like pandas is just slow to read those. There are a ton of posts online about this. The average rowcount per file was also about 60k
* All files had the same content, but did not have standardized column headers or sheet names. I needed to rename the columns using a mapping template before unioning them.
* There was a lot of good feedback about breaking up the script into more discrete steps (copy all files locally, convert to csv, cleanup/transformations, union, db load). This is great feedback and I wish I had thought of this when I started. I'm still learning and trying to break the bad habit of writing a giant monoscript.
* It was important to improve the speed for two reasons: the business wanted to go through a couple iterations (grabbing different field/sheet/file) combinations, and it wasn't practical to wait 60 hours between iterations. There was also a very expensive issue caused by having a giant shitpile of excel files that needed to be fixed ASAP.",123,175,Ein_Bear,2023-07-25 00:49:22,https://www.reddit.com/r/dataengineering/comments/158sqwa/whats_the_best_strategy_to_merge_5500_excel_files/,False,False,False,False
shabud,DBT: Interactive search and selection of dbt models directly in your terminal,,120,11,Kimcha87,2022-01-31 19:51:42,https://raw.githubusercontent.com/Infused-Insight/fzf-dbt/main/screenshots/fzf-dbt_demo.gif,False,False,False,False
rlmhvx,"Facebook's reputation is so bad, the company must pay even more now to hire and retain talent. Some are calling it a 'brand tax' as tech workers fear a 'black mark' on their careers.",,126,77,theporterhaus,2021-12-21 19:45:15,https://www.businessinsider.com/facebook-pays-brand-tax-hire-talent-fears-career-black-mark-2021-12,False,False,False,False
o1gfgc,"How Taco Bell Updates Over 7,000 Restaurant Menus Daily",,120,14,theporterhaus,2021-06-16 21:45:35,https://youtu.be/hf9nMAG9XoU,False,False,False,False
jym5v1,I'm Officially a Data Engineer!!!,"I recently just got the news for promotion from an Application Developer to a Data Engineer, any advice you guys could give me. My new role starts in about two weeks.",122,35,None,2020-11-22 00:40:42,https://www.reddit.com/r/dataengineering/comments/jym5v1/im_officially_a_data_engineer/,False,False,False,False
1b427r1,150$ Coupon for Airflow certification ,"**\[ Expired !!!! \]**



Hey guys just wanted to share that you can grab 150$ coupon and take the airflow fundamentals certification for free

Use coupon ""m-fundamentals-free-cert""

Happy weekend guys !",123,48,_T0fuu_,2024-03-01 18:54:25,https://www.reddit.com/r/dataengineering/comments/1b427r1/150_coupon_for_airflow_certification/,False,False,False,False
uwsm9g,Am I overdressed? Met engineers from our parent company and they were all in heavy metal t-shirts on Zoo.,"I just wear a collared shirt with a sweater or something, like old man.  These guys were older than me and dressed awesome. 

So thought this would be a fun morning post. 

For all I know, this could be the norm and I'm just naive.",121,67,throw_data_way,2022-05-24 14:51:55,https://www.reddit.com/r/dataengineering/comments/uwsm9g/am_i_overdressed_met_engineers_from_our_parent/,False,False,False,False
1ajq0td,Just got a LC Hard in an interview,"Not necessarily a complaint post.

Companies are obviously allowed to interview however they want and ask whatever they want. I’m a senior level DE and my background was perfect for what they wanted. 

For context, I got asked LC 84 Largest Rectangle in a Histogram. I’ll admit my LC knowledge is not great. I’ve been working on it but this one is beyond where I’m at right now. But I do think it’s a little funny that this particular question was asked. 

Leetcodes like 84 really make me question my intelligence sometimes. I could’ve looked at that problem for 3 hours and I might not have even been able to brute force it. Even the stack answer doesn’t make sense after seeing it, let alone the dynamic programming solutions.",121,95,JustASimpleDE,2024-02-05 20:03:31,https://www.reddit.com/r/dataengineering/comments/1ajq0td/just_got_a_lc_hard_in_an_interview/,False,False,False,False
18l9aak,What are they looking for with title data science full stack engineer 😂,"How can someone with 2 years of experience with knowledge of frontend ,backend, data science, data engineering . 
And with a salary of fresher 😂",121,52,Foot_Straight,2023-12-18 13:56:29,https://i.redd.it/nj9g27m5727c1.png,False,False,False,False
10uu1j4,Best books or material to learn the basics of data engineering.,"Hi everyone, I’m currently a senior data analyst and I’m looking to make the move to the DE side. My day to day deals with SQL, Power BI and some python for the manipulation of datasets. Started being involved with some DE projects and it seemed very interesting. Can you guys recommend the best books or material for me to learn the basics of DE?",122,33,lramirez27,2023-02-06 01:38:14,https://www.reddit.com/r/dataengineering/comments/10uu1j4/best_books_or_material_to_learn_the_basics_of/,False,False,False,False
wlyhrs,What can I do at home to become a better data engineer?,"I feel a sense of imposter syndrome. I have a title “Data Engineer” but most of what I do is sql to pull data from an existing data warehouse and tableau to visualize and make dashboards. I feel more like an analyst. Sometimes I use Python to move data from excel files into the dwh. 

I don’t want my skill set to stagnate. What kind of work or projects would be a good suggestion to keep up to date with the field of data engineering today? What kinds of tools or technologies can I use from home without having access to terabytes of data or a bank of servers. Does it make sense to get cloud certified?",120,67,R3boot,2022-08-11 18:27:45,https://www.reddit.com/r/dataengineering/comments/wlyhrs/what_can_i_do_at_home_to_become_a_better_data/,False,False,False,False
p6wvzm,What’s An OLAP Cube,,122,16,fhoffa,2021-08-18 18:17:26,https://analyticsengineers.club/whats-an-olap-cube/,False,False,False,False
hi0tme,100% OFF Udemy Databricks & Apache Spark 3.0.0 Course,"Hi Data enthusiasts,

If you are new to **Databricks** and **Apache Spark** and want to learn it step by step, then I have a brand-new course on Udemy for you.

[The course](https://www.udemy.com/course/databricks-fundamentals-apache-spark-core/?couponCode=B0F1F71238845CB68C4C) covers all you need to know to get started with Apache Spark and Databricks

[ENROLL NOW FOR FREE](https://www.udemy.com/course/databricks-fundamentals-apache-spark-core/?couponCode=B0F1F71238845CB68C4C)",124,13,gwadson,2020-06-29 14:27:07,https://www.reddit.com/r/dataengineering/comments/hi0tme/100_off_udemy_databricks_apache_spark_300_course/,False,False,False,False
1bdzrkh,Data Engineer vs Data Analyst Salary,Which profession would earn you most money in the long run? I think data analyst salaries usually don’t surpass $200k while DE can make $300k and more. What has been your experience or what have you seen salary wise for DE and DA?,120,114,Mergirl610,2024-03-13 19:04:40,https://www.reddit.com/r/dataengineering/comments/1bdzrkh/data_engineer_vs_data_analyst_salary/,False,False,False,False
15b85up,Who has worked with both Snowflake and Databricks and what do you enjoy/dislike about each?,"As someone who works in Snowflake day to day but has had little to no exposure to Databricks I'm curious to know from those who have worked with both, which do you prefer and what do you like/dislike about each?   
Yes, I know it's not exactly an apples to apples comparison but no one can deny these two companies are trying to compete with each other in the data marketplace.   
This isn't meant to be a comparison of which you think is necessarily *BETTER*... but more so which do you prefer working with, what have you enjoyed or disliked about either/both. Honestly just curious to hear opinions.",122,82,MasterKluch,2023-07-27 17:37:53,https://www.reddit.com/r/dataengineering/comments/15b85up/who_has_worked_with_both_snowflake_and_databricks/,False,False,False,False
13y5aks,What are some books that had an impact on your career?,"Doesn’t necessarily have to be technical. But things that helped you with productivity, communication, or business. I feel like my technical skills are growing but my business acumen, time management, and communication skills are falling behind. 

I want to ensure I have a long career.",120,73,cheanerman,2023-06-02 06:47:14,https://www.reddit.com/r/dataengineering/comments/13y5aks/what_are_some_books_that_had_an_impact_on_your/,False,False,False,False
13ltiiu,What are the most advanced DE frameworks skills that DE employers value the most?,"This question is part of my upskilling strategy so any advice is appreciated.
I want to hear about technologies that not anyone can learn and master and is really sought after and can make me a standout",122,103,Born-Comment3359,2023-05-19 12:03:43,https://www.reddit.com/r/dataengineering/comments/13ltiiu/what_are_the_most_advanced_de_frameworks_skills/,False,False,False,False
10jjsfp,"Another data project, this time with Python, Go, (some SQL), Docker, Google Cloud Services, Streamlit, and GitHub Actions","This is my second data project. I wanted to build an automated dashboard that refreshed daily with data/statistics from the current season of the Premier League. After a couple of months of building, it's now fully automated.

I used Python to extract data from [API-FOOTBALL](https://rapidapi.com/api-sports/api/API-FOOTBALL) which is hosted on RapidAPI (very easy to work with), clean up the data and build dataframes, then load in BigQuery.

The API didn't have data on stadium locations (lat and lon coordinates) so I took the opportunity to build one with Go and Gin. This API endpoint is hosted on [Cloud Run](https://cloud.google.com/run/docs/overview/what-is-cloud-run). I used [this guide](https://go.dev/doc/tutorial/web-service-gin) to build it.

All of the Python files are in a Docker container which is hosted on [Artifact Registry](https://cloud.google.com/artifact-registry).

The infrastructure takes places on Google Cloud. I use [Cloud Scheduler](https://cloud.google.com/scheduler) to trigger the execution of a Cloud Run Job which in turn runs `main.py` which runs the classes from the other Python files. (a Job is different than a Service. Jobs are still in preview). The Job uses the latest Docker digest (image) that is in Artifact Registry.

I was going to stop the project there but decided that learning/implementing CI/CD would only benefit the project and myself so I use [GitHub Actions](https://docs.github.com/en/actions) to build a new Docker image, upload it to Artifact Registry, then deploy to Cloud Run as a Job when a commit is made to the `main` branch.

One caveat with the workflow is that it only supports deploying as a Service which didn't work for this project. Luckily, I found this [pull request](https://github.com/google-github-actions/deploy-cloudrun/pull/422) where a user modified the code to allow deployment as a Job. This was a godsend and was the final piece of the puzzle.

Here is the [Streamlit dashboard](https://premierleague.streamlit.app/). It’s not great but will continue to improve it now that the backbone is in place.

Here is the [GitHub repo](https://github.com/digitalghost-dev/football-data-pipeline).

Here is a more [detailed document](https://storage.googleapis.com/website-storage-bucket/docs/football-data-pipeline-doc.html) on what's needed to build it.

Flowchart:

(Sorry if it's a mess. It's the best design I could think of.

&#x200B;

[Flowchart](https://preview.redd.it/hfvokmmiy0ea1.png?width=2711&format=png&auto=webp&s=5eabbc6cdaa400e59a01b71881c37296ac6356f2)",119,41,digitalghost-dev,2023-01-23 18:36:37,https://www.reddit.com/r/dataengineering/comments/10jjsfp/another_data_project_this_time_with_python_go/,False,False,False,False
npxcqc,Quarterly Salary Discussion,"This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering. Please comment below and include the following:

1. Current title

2. Years of experience (YOE)

3. Location

4. Base salary & currency (dollars, euro, pesos, etc.)

5. Bonuses/Equity (optional)

6. Industry (optional)",119,161,AutoModerator,2021-06-01 16:00:17,https://www.reddit.com/r/dataengineering/comments/npxcqc/quarterly_salary_discussion/,False,False,False,False
1ajx26t,How come so many people seem to think DE is incredibly easy? Is my perspective skewed due to my experiences?,"I've read a lot of people in this and other tech subs saying that DE is one of the easiest subsets of software engineering, some people saying it's just one step harder than regular frontend web design.

It's not that I want to pat my own back, but I've actually always thought that DE is an incredibly complex subset of SE. Even before I was a DE.

But then I compare the work these people claim to do with what the companies I've been at do. People who say those things seem to be working exclusively with ""pandas scripts that are done running in 1 minute"", like their business needs are just sending a csv with 3 columns to some sales people.

Hell, I even saw a guy saying that setting up a script to send 35000 daily emails with a csv file was a better alternative to setting a table with a consumer view to feed a dashboard, and it had a lot of upvotes, and it really made me wonder what did they think DE was?

At the companies I've worked at, the work has always been super complex, extremely heavy duty pipelines needing to process hundreds of thousands of events per second, designing scalable architecture, processes for data governance, working on external services integrations and constant feature requests. Yet it seems most people here only describe those pd.read_csv cases?

Am I out of touch or is it the kids who are wrong?",117,112,yourAvgSE,2024-02-06 00:57:50,https://www.reddit.com/r/dataengineering/comments/1ajx26t/how_come_so_many_people_seem_to_think_de_is/,False,False,False,False
16076go,"If anyone is wondering what it's like working for a garbage company, then read this","I'm a seasoned data professional That has worked at mainly big companies throughout my career, so all of my experiences lie primarily in Fortune 500 firms. Most of those experiences have been generally very positive. My latest company treated me pretty fairly, up until about 2 months ago when it became very clear what they are really like.

**The reason I'm sharing this is not to rant. A lot of people seem to end up in a cruddy company and think: ""No one else ever experiences this, do they? I'm cursed!"" I'm here to tell you it's not just you. This DOES happen, and you need to look out for red flags.**


&#x200B;

&#x200B;

\- Mandatory social outings and extracurricular ""fun"" activities. For example, we were forced to go out to a park during the week and play soccer against another team in our department. Our manager showed up 40 minutes late and didn't even play. Just milled around, cheered our team on, and we lost.  Also had goofy t-shirts we were required to wear that said ""the eliminators"" or something like that

&#x200B;

&#x200B;

\- Tech stack extremely out of date and organization very siled. Some teams were very lucky to have Google BigQuery, and even though I had access to it, was often told I'm only allowed to use and create data sources in Microsoft Access, have to work exclusively out of Excel, have to be very very adept at using SQL, but I'm not allowed to create data tables in BigQuery, I have to do everything in legacy Microsoft Access because that's what the team is using and has used for many years, they're not able to or ready to transition into more modern data sources. We have Tableau, but we prefer to make everything in Excel. For example, using VBA to create tables repeatedly that could easily just be done in Tableau. Reinventing the entire wheel in Excel is absolutely insane

&#x200B;

&#x200B;

\- Manager was extremely lazy, unknowledgeable, unhelpful, and hardly ever did their job. This manager of course was included in the layoff, rightfully so. But the fact that they worked at the company for about 5 years is astounding to me. They would often schedule meetings and not even show up on time, arrive 20 minutes late, or they are driving in their car you can hear their turn signal, every single time, they have an excuse for why they are not doing their job during business hours. While you are sitting squarely at home being trustworthy independable, they are not doing their job. And their leaders were completely unaware of this for years? But they make it really hard for you to get promoted....

&#x200B;

&#x200B;

\- No in-role promotions. This was the first time I've ever heard of this in my life. I have never heard of a company that says you can't be promoted in the role that you are in. If you are an analyst, you can't possibly become a senior analyst. That's not possible. If you are senior analyst, you can't become the manager. A new entire role with different responsibilities has to be created by HR and you have to apply for it, compete against people throughout the company who probably aren't even in your department, you have like a 10% chance of getting hired into it, but likely you won't. So you're expected to stay in your job as long as possible even though they tell you after a year you can apply to other positions in the company, they frown upon that because they don't want turnover

&#x200B;

\- No advancement. I was working as data analyst, AND data engineer. Creating extracts, automatic table updates, data warehouses, BI stuff. Told I can mentor with other DEs, but I can never get that job because of glass ceiling bullsh\*t. I have to leave company, work as a ""real DE"", then re-apply. WTF? 

&#x200B;

\- Almost no yearly increase in salary. I was told that I was lucky, because as a hard worker, this year I was getting a 1.5% increase which is more than a lot of people were getting

&#x200B;

&#x200B;

\- No employee discount of any kind for the products that our company sells. Anytime this was brought up, the reason was that we get a bonus on top of that. Rivaling companies give discounts for their products, hours doesn't. So we have a 0% discount, and a bonus. But they sell it to us like we are so lucky to get this bonus, even though every single company offers a bonus

&#x200B;

&#x200B;

\- Laid off completely at random by our director, who read a message off a script in a monotone voice. Was told that I cannot apply to other positions as internal candidate, I would have to use the external career site and apply as if I am not an employee anymore, so in other words, follow the external applicant process. \*\*They also laid off another person on my team who was pregnant and about to deliver a new child\*\*. I considered myself very lucky. I found it extremely unprofessional, and downright evil that they would lay this person off Right before They are set to deliver a child. What kind of company could be so evil as to do that? There were other people on our team that were less qualified, and barely understood how to use Excel, and they chose someone who is extremely vulnerable and laid them off like that. Pretty crazy.... 

&#x200B;

&#x200B;

&#x200B;

\- Put me through a very rigorous application process like I am some random dude off the street. Admittedly, this is kind of normal I suppose, because they want to make sure they are making the right hiring decision. But some of the things I had to do and hoops I had to jump through were borderline insane. Take a full blown Excel test, take an SQL coding assignment, which is so weird because people in my previous department spoke to my skills and abilities. I was considered an expert in SQL, Python, Excel. So it was no mystery that I was extremely well versed in all of these things. Yet I had to do it anyway. 

&#x200B;

&#x200B;

\- I was working remote previously, but this one is fully in office 5 days a week. No possibility to be remote. So now, I have to go from being fully remote to fully in office, costing me time, and resources, 10 hours a week in commuting back and forth completely erased from my life

&#x200B;

&#x200B;

\- After I provided my start date for the job that would be included on my offer letter, was contacted by HR and asked to start immediately, and gave me an extremely hard time about a trip that I had already planned for next week, flights, hotels, everything booked and paid for unable to be moved around. Their solution? I could take unpaid vacation to take the trip. What is the real reason you might ask that they have pushed my start date up so I have literally 3 days after getting the offer letter to start the job? \*\*Because they would have to pay me out on my severance and then bring me back as a brand new employee.\*\* 

&#x200B;

\- Overall lack of respect for their employees. Lay me off completely at random, put me through external hiring process like I'm a nobody even though I moved here for this company and job, then push my start date further without any care of consideration about what I have going on in my personal life

&#x200B;

&#x200B;

\- Company leaders are often in the news for a very negative reasons. For example, contributing to political action committees on behalf of the company for legislation that Is negatively targeting people of a certain ethnicity, extremely pro-conservative far right ideology throughout the entire company, and contributes financially to those sorts of political organizations. I personally am not going to voice my political opinions, but I'm just going to tell you right now, any company that contributes to political action committees with company funds, or the owners are very active and pushy and do that themselves is a really big red flag

&#x200B;

&#x200B;

&#x200B;

&#x200B;

I honestly feel my skin crawl and just feel so wronged thinking back about the last 6 months with his company, even though the first year and a half with my team was actually generally pretty great, it just kind of traumatized me seeing how quick they were to throw people out of the company and then treat them like nothing, literal dirt, and then try and get them back in the company. Zero bargaining power or respect for employees, 100% of the respect for their own company",119,50,databro92,2023-08-24 16:45:05,https://www.reddit.com/r/dataengineering/comments/16076go/if_anyone_is_wondering_what_its_like_working_for/,False,False,False,False
11p2dqg,How good is Databricks?,"I have not really used it, company is currently doing a POC and thinking of adopting it.

I am looking to see how good it is and whats your experience in general if you have used?

What are some major features that you use?

Also, if you have migrated from company owned data platform and data lake infra, how challenging was the migration?


Looking for your experience.

Thanks",119,140,mjfnd,2023-03-12 02:13:05,https://www.reddit.com/r/dataengineering/comments/11p2dqg/how_good_is_databricks/,False,False,False,False
10pqspk,Weekend Data Engineering Project-Building Spotify pipeline using Python and Airflow. Est.Time:[4–7 Hours],"This is my second data project. Creating an Extract Transform Load pipeline using python and automating with airflow.

# Problem Statement:

We need to use Spotify’s API to read the data and perform some basic transformations and Data Quality checks finally will load the retrieved data to PostgreSQL DB and then automate the entire process through airflow. **Est.Time:**\[4–7 Hours\]

# Tech Stack / Skill used:

1. Python
2. API’s
3. Docker
4. Airflow
5. PostgreSQL

# Learning Outcomes:

1. Understand how to interact with API to retrieve data
2. Handling Dataframe in pandas
3. Setting up Airflow and PostgreSQL through Docker-Compose.
4. Learning to Create DAGs in Airflow

Here is the [GitHub repo](https://github.com/sidharth1805/Spotify_etl).

Here is a blog where I have documented my project [Blog](https://medium.com/p/432dd8e4ffa3)

[Design Diagram](https://preview.redd.it/a6kh9au6nbfa1.png?width=2283&format=png&auto=webp&s=6296e53c9df7150048d40e89b5d9b6399b809628)

&#x200B;

[Tree View of Airflow DAG](https://preview.redd.it/7gqn7up8pbfa1.png?width=635&format=png&auto=webp&s=2870c95aa39fbd81b5b425093eb54d5ef5d678e3)",119,31,Sidharth_r,2023-01-31 06:26:46,https://www.reddit.com/r/dataengineering/comments/10pqspk/weekend_data_engineering_projectbuilding_spotify/,False,False,False,False
z7o6cp,Meta fined $276 million over Facebook data leak involving more than 533 million users,,120,18,vjmde,2022-11-29 08:40:39,https://www.theverge.com/2022/11/28/23481786/meta-fine-facebook-data-leak-ireland-dpc-gdpr,False,False,False,False
ol4l9j,Anyone else having trouble finding data engineers?,"Data engineering is such a unicorn skillset, and good ones are hard to find. We've been looking for a second data engineer for months. Most applicants come from traditional, enterprise backgrounds. They've never touched Docker. Never really used the command line. Don't really know any cloud tools. Don't think about testing and monitoring. It's really tough to find someone.

I feel like I can teach those things, but even then it's hard to find someone with the base skillset.

 

Anyone else having the same problem?

 

Those who have found good DEs, how did you find good candidates?

[EDIT]

Wow, did not expect this much discussion. Thanks everyone for contributing. Shared this with my team and we are learning a lot. Here are a few general thoughts to common questions and comments.
 
We cannot post jobs in this sub, but I'm happy to give a link in chat.

Will you teach me? I read this a few times. The most important skill for data engineers is the ability to learn new tools and processes. Everyone, jr. or sr., will need to learn a lot when taking a new DE job. That being said, most companies will require some base skills where you can be productive quickly.

I've never used X tool: I had little experience when I was hired with the tools I use most today (see above). My point in listing a few different tools wasn't to say everyone should know these specific tools. It was more about finding people who love technology and learning new things because they are interesting. Not just because they were assigned to learn. The fact that you are reading this on Reddit is a good sign that you want to learn.

DEs are difficult to find: I think we all agree on this. To get one, the company needs to offer great pay and benefits. A good working environment. Flexibility, time off, remote.... Unless you are a well-known company, that's not enough. You need to go out and find DEs. Build relationships, talk to people. DEs are not going to just walk through your door.

My best advice for companies is to start very early, long before you need to hire. Get on LinkedIn groups. Talk to people on Reddit. Have a list of people you would want to hire before you need it.

Will you teach me part 2: It would be great for this sub to create some sort of mentoring and training program. Something where we all contribute learning material and help each other complete real projects with real data. We should discuss this in a different thread.",119,191,leogodin217,2021-07-15 23:28:29,https://www.reddit.com/r/dataengineering/comments/ol4l9j/anyone_else_having_trouble_finding_data_engineers/,False,False,False,False
17gduz6,To my data engineers: what do you *not* like about being a data engineer?,"In contrast to my previous post, i wanted to ask you guys about the downsides of data engineering. So many people hype it up because of the salary, but whats the reality of being a data engineer? Thanks",118,183,naq98,2023-10-25 20:11:48,https://www.reddit.com/r/dataengineering/comments/17gduz6/to_my_data_engineers_what_do_you_not_like_about/,False,False,False,False
172ul0e,Data engineers in Europe,"Hello everyone, I was wondering if you guys would be interested in joining a Data engineers Slack channel for who are located or working for European companies. Would be interesting to meet new people, share experiences and maybe set meet ups.
Have a nice Sunday 😊

Edit: thanks for the great interest. 
Here is the invite link: https://join.slack.com/t/dataengineers-5fv9386/shared_invite/zt-24mmw5v85-Jg3EFRRgZpki1fBcgZxhBg",118,56,crazy_shredder,2023-10-08 09:06:13,https://www.reddit.com/r/dataengineering/comments/172ul0e/data_engineers_in_europe/,False,False,False,False
yluu6c,"What are some highly recommended courses for data engineers (that are already working in this field so not ""from zero"")?","I'm looking towards expanding my knowledge in this field. Are there some courses (or even books) that are maybe not specific to one technology, but are very useful to improve in this field and you can recommend?",116,25,mackbenc,2022-11-04 10:15:22,https://www.reddit.com/r/dataengineering/comments/yluu6c/what_are_some_highly_recommended_courses_for_data/,False,False,False,False
16imcbc,How to approach an long SQL query with no documentation?,"The whole thing is classic, honestly. Ancient, 750 lines long SQL query written in an esoteric dialect. No documentation, of course. I need to take this thing and rewrite it for Spark, but I have a hard time even approaching it, like, getting a mental image of what goes where.

How would you go about this task? Try to create a diagram? Miro, whiteboard, pen and paper?

Edit: thank you guys for the advice, this community is absolutely awesome!",119,125,a1ic3_g1a55,2023-09-14 16:06:46,https://www.reddit.com/r/dataengineering/comments/16imcbc/how_to_approach_an_long_sql_query_with_no/,False,False,False,False
12v2lcx,How do you feel about the return to SQL?,"Over the years people have started realising you don't need a distributed framework if you're not operating on that scale. SQL-first tooling such as DBT and others have also improved SQL-based workflows.

However as much as I like SQL before I start a project I always reflect on whether or not it's a good fit. Yes you can do everything with SQL but *should* you? There's times where queries are so far removed from intentions which is a no-go in most other places in software. Sometimes imperative paradigms are a better fit. 

Do you go for Python in these cases or does your shop stick to SQL for all tabular data? What are your opinions?",114,81,Odd-One8023,2023-04-22 11:01:21,https://www.reddit.com/r/dataengineering/comments/12v2lcx/how_do_you_feel_about_the_return_to_sql/,False,False,False,False
xhe2bg,What we learned after I deleted the main production database by mistake,,119,36,-segmentationfault-,2022-09-18 11:21:09,https://medium.com/@hugo.oliveira.rocha/13c0a5815de5,False,False,False,False
xbh053,Experts found 10 malicious packages on PyPI used to steal developers’ data,,115,7,napduoerr,2022-09-11 12:13:52,https://securityaffairs.co/wordpress/134253/malware/pypi-malicious-packages-3.html,False,False,False,False
w3mvbc,How to stay up to date with latest advances in data engineering?,"Hi all,

In the same way we use r/dataengineering to stay immersed in any news and discussions on data engineering, is there any other sources of information that you know of that we could use to stay up to date with perhaps new advances in tools and software, or some sort of blog or community pages in which best practices, tips, examples of projects/code etc. can be shared and displayed? In the past 3 months I've introduced myself to programming, computer science, data engineering etc. through youtube, software documentation etc., but I'm looking to be involved in a way that I can find information that I didn't know I was looking for/just become immersed in DE so I can be the best DE I can be.

Appreciate any response, hope the question made sense.",117,18,NineFiftySevenAyEm,2022-07-20 13:51:22,https://www.reddit.com/r/dataengineering/comments/w3mvbc/how_to_stay_up_to_date_with_latest_advances_in/,False,False,False,False
slolx6,What's your data engineering stack at your company?,"I wanted to know if there's a better stack than what we use at our company?

**Sources:** postgresql, 3rd party tools  
**Extraction:** Python scripts for API calls w/ 3rd party or other external sources; internal databases we schedule sql scripts   
**Loading/Transformation:** sql scripts (dbt thoughts?)  
**Storage:** Redshift (any thoughts on latency or other warehouses?)  
**Application:** Looker (migrated from mode)",119,134,crhumble,2022-02-06 03:19:26,https://www.reddit.com/r/dataengineering/comments/slolx6/whats_your_data_engineering_stack_at_your_company/,False,False,False,False
rg5rjp,Just got fired.,"Entry level. Offshore. Remote. 25 bucks an hour. Contract.

I signed an NDA. I am not mentioning the name of the company, the employer or any confidential or any specific information. If the client comes across this and  wants to have a fight about this he has my email.

So, I joined this 100% remote startup. They wanted to hire a fullstack developer but I begged and convinced my way into a Python+data engineering role. They had me on probation for a 2 week period.

*Day 0*

During the onboarding interview I was given a ""simple"" task where I needed to generate two analytics table each time they made a request to saturate their database for their data analysis stuff.

I was told that all I needed to do was run the `main.py` and everything was ready to go. They have a few lines of code on the README for setting up the repo. They were just docker configuration commands. I said, that config is never going to be that simple and the client was visibly surprised by that response. But as we know all know configuring a repo for the first time isn’t always plug and play type of deal. 

*Day 1*

So, I get started and things start to go south. First they didn't assign me with access permissions to the cloud. I waste some time figuring that out. Then the docker file shows that I need to setup some registry stuff for the cloud database access and setting up the machine learning models. That took a while to figure that out. I also needed to setup config dotfiles for that...... 

At this moment I have spent a day but I logged around 4.5 hours. The senior dev was super helpful and very kind but he was super busy. The client comes back and gets mad as all I supposedly needed was to run the `main.py` file and that was supposed to be it. I said we were trying to set up Docker for the python version, the machine learning libraries and python packages. He says run the requirement file. 

The machine learning libraries calls the database, processes and sends to another database. I needed to create an analytics log in the middle of that. But most of the functions simultaneously called, merged and uploaded data.

*Day 2 (Weekend)*

If you use multiple python versions in a project the requirements.txt installation using pip installing isn't exactly straightforward. After 2 hours of trying to figure out why I wasn’t able to install packages among other things, I discover that they were using two versions of Python and for the requirements file they used one package that wasn't supported by the python version they recommended me. I change the version of that package and finally install all the packages.

This might sound like an easy fix but I urge you to try this out.


Day 3

I run the file it doesn't work of course. Because this entire repo heavily depends on the Docker file and docker file sets up aliases for certain API calls and actions. Without docker configured the file will never work straight. I try my best to discover my way through the jungle that is that codebase. And even still I get permission issues. I get one permission issue which was sorted. And the entire file doesn't run anyway because I wasn't granted full permission of something I couldn’t figure out that day 

I say to the client that without cloud permissions I am not getting stuff done and he gets mad. He says that, my skills aren't that strong as I have indicated in my interview.

How the heck am I supposed to react to that. I said please re-read our conversation. How am I supposed to solve this issue without having permission and configuration issues ironed out.

In the begging of the project they have provided the schema for the analytics they needed, so they insisted it should be an easy task. I said, I can provide a code but I will be working be working blind without running the file top to bottom. They said, I have all the necessary things for me to get the job done.

Based on the schema alone, I comment out the sections that doesn't work and I write a code that outputs a CSV. But the client still isn’t happy. At this moment I let the client know let me know when I should leave. They indicate it is going to happen. And they ask me what I meant by the ""re-reading the conversation"" message 

Day 4 (Today)

I intentionally didn't reply to the ""re-read conversation"" message. Yeah. An unprofessional thing to say I know, but what am I supposed to say when the client thinks I am not getting things done while simultaneously locking me out from the very place I need to get things done in. And moreover questioning my ability just  triggered me a little bit.

I comment out each sections of code to find what is going on. They are simultaneously fetching and merging data and aggregating data upon layers upon layers function in multiple modules.

Then I discover they have another cloud service that they are using that I didn't knew I even needed access for. I sent a message saying that, I have found the problem for the issue and I need access to this service.

The response I get was, ""Pack your things and go"".

---

Overall I logged about 10 hours for this job but my idle time which I didn’t log was about double that time because I was stuck on stuff and I wasn’t getting any responses.  I obviously didn't get paid. Of the two task I was asked to do, the first one took me 20 minutes to do. But the setup and configuration and code discovery took up the entire logged time.

The first day when I joined slack, I came across a few profile and all of them were deactivated. Scrolling up I saw a guy quitting under 3 hours.

I don't plan to work for another company that doesn't provide some level of mentorship. I don't want to work with someone who isn't technically competent and is not willing to guide or hear me.

I understand I am supposed to be an expert because my rate is WAYYYYYYY too high for an offshore dev. But not again am I working without pair programming, routine video discussion or helpful seniors.

I really despise freelancing.

Edit: Was emotional made a ton of typos. It is a rant there is going to be a lot of typos.",115,86,anyfactor,2021-12-14 11:44:58,https://www.reddit.com/r/dataengineering/comments/rg5rjp/just_got_fired/,False,False,False,False
n5msew,The State of Data Engineering in 2021,,119,13,twopairisgood,2021-05-05 18:22:49,https://lakefs.io/the-state-of-data-engineering-in-2021/,False,False,False,False
1ajnqf4,Finally landed my first DE role,"Im a data analyst who just signed his first offer letter for an DE role. My manager told me that I won’t be a “true” data engineer until about a year in, he said the first year will be a lot of learning and that a lot of my job will be closely supervised so don’t feel like Im being micromanaged but once he feels comfortable i will be able to handle projects on my own. Is this normal?",116,35,tofin02,2024-02-05 18:32:02,https://www.reddit.com/r/dataengineering/comments/1ajnqf4/finally_landed_my_first_de_role/,False,False,False,False
1adsbnd,What happened recently with Snowflake?,"I am on a job hunt and I noticed that companies actively looking for someone with Snowflake experience.  I am not saying Snowflake was bad or anything, just never see that big demand/need for its use.

Do you use Snowflake in your company? Did you migrated to Snowflake recently? Any huge cons that made you this migration?",117,122,BubblyImpress7078,2024-01-29 10:34:43,https://www.reddit.com/r/dataengineering/comments/1adsbnd/what_happened_recently_with_snowflake/,False,False,False,False
1addsj4,What distinguishes production-grade data pipelines from amateur setups?,What do amateurs usually not do well?,115,64,CupcakeFun4746,2024-01-28 21:34:31,https://www.reddit.com/r/dataengineering/comments/1addsj4/what_distinguishes_productiongrade_data_pipelines/,False,False,False,False
124ozln,Big news! LambdaConf returns Sept 16-19th and is better than ever! 🔥,"Join us in the Rockies for an unforgettable conference featuring thought-provoking talks, workshops, craft beer tasting, hiking, and immersive experiences that will change the way you think about software development. Grab your Early Bird Ticket: [https://www.eventbrite.com/e/lambda-conf-colorado-usa-in-person-only-tickets-540887036687](https://www.eventbrite.com/e/lambda-conf-colorado-usa-in-person-only-tickets-540887036687)",116,9,Agataziverge,2023-03-28 13:35:41,https://www.reddit.com/r/dataengineering/comments/124ozln/big_news_lambdaconf_returns_sept_1619th_and_is/,False,False,False,False
zltid3,Any really good end-to-end walkthroughs?,"I've been reading the data warehouse toolkit as well as data engineering blog posts but I'm still having a hard time fleshing out a full picture of how everything integrates. Are there any really good resources that show a ""real-world"" data pipeline being built out? From modeling the data to designing the ETL/ELT pipeline to distributing the data via a data mart or dataset?  

Ideally, a guide or two that show how this would be applied on-prem as well as in the cloud would be helpful.",114,23,MonkeyMaster64,2022-12-14 15:12:32,https://www.reddit.com/r/dataengineering/comments/zltid3/any_really_good_endtoend_walkthroughs/,False,False,False,False
pscnk0,University of Helsinki is providing Big Data Platforms Mooc (https://big-data-platforms-21.mooc.fi/),"Finnish Universities(Helsinki & Aalto) are generous enough to provide some of the best courses for **absolutely free** on [https://www.mooc.fi/en/](https://www.mooc.fi/en/) in **English.** It only costs you money if you want university credits, or else everything is free including exams and certificates. 

[**https://big-data-platforms-21.mooc.fi/**](https://big-data-platforms-21.mooc.fi/) is one of them. 

Main topics are:

* distributed computing,
* Warehouse-Scale Computers,
* fault tolerance in distributed systems,
* distributed file systems,
* distributed batch processing with the MapReduce and the Apache Spark (PySpark) computing frameworks, and
* distributed cloud-based databases.

The course material will consist of lecture materials and exercises provided by the lecturer.",115,12,None,2021-09-21 05:53:28,https://www.reddit.com/r/dataengineering/comments/pscnk0/university_of_helsinki_is_providing_big_data/,False,False,False,False
1bq6k5q,"What's your prod, open source stack?","Looking into creating an open source ELT stack from scratch: if you have one, or have had one that worked well, what were the stack components?",116,104,Melodic_One4333,2024-03-28 20:59:58,https://www.reddit.com/r/dataengineering/comments/1bq6k5q/whats_your_prod_open_source_stack/,False,False,False,False
161zmp3,"Follow up on my previous post! Who are some of the no-fluff, not clickbaity data influencers you like and follow?","I’m looking to clean up my LinkedIn feed and would like recommendations on who to follow. 

Even if they have 500 followers, but post thoughtful, insightful content, I’m all for it. Let’s call it underrated gems lol. 

Netflix, Uber and LinkedIn engineering blogs are my go to. 

Medium, LinkedIn follow recommendations appreciated.",113,68,Winter-Cookie-4916,2023-08-26 16:25:24,https://www.reddit.com/r/dataengineering/comments/161zmp3/follow_up_on_my_previous_post_who_are_some_of_the/,False,False,False,False
xyeqfv,"Is there a list of SQL ""patterns"" or problem types for interview questions?","With some data structures and algorithm questions there's general topics and patterns to learn to pass interviews, for example array specific problems can be solved with a 2-pointer pattern.

Does anyone know of a list of patterns or even types of problems? I've seen 1 type of problem that's common and learned it's called the Top K Elements/by Group problem.

At the moment I'm doing random Leetcode DB questions and there isn't a good structure to my approach.

Thanks for any pointers.",116,37,Firm_Bit,2022-10-08 00:29:38,https://www.reddit.com/r/dataengineering/comments/xyeqfv/is_there_a_list_of_sql_patterns_or_problem_types/,False,False,False,False
rem26j,Data Engineering Jargon - Part 2,"Hi - this is the next 10.

1-10 is [here](https://www.reddit.com/r/dataengineering/comments/rdw3b3/data_engineering_jargon/?utm_source=share&utm_medium=web2x&context=3)

11-20 is below

21-30 is [here](https://www.reddit.com/r/dataengineering/comments/rfbuu8/data_engineering_jargon_part_3/)

31-40 is [here](https://www.reddit.com/r/dataengineering/comments/rg5vr0/data_engineering_jargon_part_4/)

**11. Ingestion**

Generally, the first step in a data pipeline, where data is inserted in tables in the platform.

*A pipeline where customer address data is inserted from source A.*

**12. Extract, Transform, Load (ETL)**

A 3-step process of extracting data and transforming it (by applying some kind of logic like aggregation) and loading the new information into the destination. It could be used as ELT where the destination tables transform the data instead.

*An extract of customer address data is taken from the customer relationship management tool and is then aggregated according to their cities and this new information is loaded into destination B.*

**13. Data Models**

A way of organising the data in a way that it can be understood in a real-world scenario.

*Taking a huge amount of data and logically grouping it into customer, product and location data.*

**14. Normalisation**

A method of organising the data in a granular enough format that it can be utilised for different purposes over time. Usually, this is done by normalising the data into different forms such as 1NF (normal form) or 3NF (3rd normal form) which is the most common.

*Taking customer order data and* *creating granular information model; order in one table, item ordered in another table, customer contact in another table, payment of the order in another table.* *This allows for the data to be re-used for different purposes over time.*

**15. Star schema**

The simplest way to model data into different quantitative and qualitative data is called facts and dimensions. Usually, the fact table is interpreted with the help of a dimensions table resembling a star.

*A Star schema of sales data with dimensions such as customer, product & time.*

**16. Facts**

A data warehousing term for quantitative information.

*The* *number of orders* *placed by a customer.*

**17. Dimensions**

A data warehousing term for qualitative information.

*Name of the customer* *or their* *country of residence.*

**18. Schemas**

A term for a collection of database objects. These are generally used to logically separate data within the database and apply access controls.

*Storing HR data in HR schema allows logical segregation from other data in the organisation.*

**19. SCD (slowly changing dimension) Type 1–6**

A method to deal with changes in the data over time in a data warehouse. Type 1 is when history is overwritten whereas Type 2 (most common) is when history is maintained each time a change occurs.

*When a customer changes their address; SCD Type 1 would overwrite the old address with the new one, whereas Type 2 would store both addresses to maintain history.*

**20. Business Intelligence**

A slightly out of date term for a combination of practices to derive business insights from data by predominantly using data warehousing, analytics and dashboarding.

*Creating a management dashboard to show customer demographics across the country.*

1-10 is [here](https://www.reddit.com/r/dataengineering/comments/rdw3b3/data_engineering_jargon/?utm_source=share&utm_medium=web2x&context=3)

21-30 is [here](https://www.reddit.com/r/dataengineering/comments/rfbuu8/data_engineering_jargon_part_3/)

31-40 is [here](https://www.reddit.com/r/dataengineering/comments/rg5vr0/data_engineering_jargon_part_4/)",115,18,Data_Cog,2021-12-12 10:06:17,https://www.reddit.com/r/dataengineering/comments/rem26j/data_engineering_jargon_part_2/,False,False,False,False
mqa99k,The deck we used to raise our Seed round for our open-source EL(T) platform,,115,30,jeanlaf,2021-04-13 19:59:20,https://airbyte.io/articles/our-story/the-deck-we-used-to-raise-our-seed-with-accel-in-13-days/,False,False,False,False
1bekue2,How to recover a deleted dataset from BigQuery? (Urgent),"So I’m in analytics and one of the seniors must have been high or something because he ended up deleting the Master dataset from the project.

The dataset had over 5000 tables that were used across the board. 

Most of the teams are panicking and there is a lot of chaos. Online articles and StackOverFlow don’t help.

Is there a way to restore it because we might lose the client at this rate? 

Sample id of a table: ‘project.Master.table1’

Update: Client directly got in touch with Google, they sorted out the mess but our reputation is tarnished. Lesson for everyone reading, MANAGE YOUR ACCESSES WELL. ",112,24,honpra,2024-03-14 13:12:05,https://www.reddit.com/r/dataengineering/comments/1bekue2/how_to_recover_a_deleted_dataset_from_bigquery/,False,False,False,False
wfk8l2,"One of a brainy friend of mine told me, redditors gives the best advice. So, here I am seeking advice/suggestions/feedbacks on my Backend/Data Engineer Resume.",,112,34,campaigner_mare,2022-08-03 22:35:30,https://i.redd.it/ee0bbgxxqkf91.png,False,False,False,False
nquv61,What are great books that helped you to become a better data engineer?,"As title suggests I am looking for resources that you think were a valuable read and has helped you to become a better data engineer (or software engineer). Has anyone read a book on programming philosophy (or other topic) that has changed the way you think about writing code and overall helped with your work (e.g. write cleaner code, employ better practices)?",114,34,caksters,2021-06-02 20:01:15,https://www.reddit.com/r/dataengineering/comments/nquv61/what_are_great_books_that_helped_you_to_become_a/,False,False,False,False
ln4fi7,Data-Ops-ish Design and Workflow Walkthrough,"In [another thread](https://www.reddit.com/r/dataengineering/comments/lmjhv1/ideas_to_build_a_regression_test_suite_for_data/), [u/brendersplide](https://www.reddit.com/user/brendersplide/) asked about building a regression test suite. In response to my comment, there were a few individuals who wanted to know more about the system I am currently running. ( u/isleepbad and u/htrp) It's more than what should be in a reply so I thought I would do a post for anyone else interested.

WARNING: This is a long post.

DISCLAIMER: This is, by no means, the best way to do things. We are constantly evolving and tweaking.

&#x200B;

As a little context, I run data and analytics at a start-up-ish financial company and have a very small team. In order to combat that constraint, we have done a lot to automate our workflow. We also rely on open-source as much as possible. We've tried to infuse this thought process into everything we do - from onboarding new hires to reproducing the entire infrastructure.

I'll break it down into the following sections:

**Main Sections**

1. Storage
2. ELT
3. Testing and QA
4. Infrastructure
5. Data Workflow Automation
6. Development
   1. Analytics
   2. Engineering
7. CI

**Follow-up Questions**

1. Edit 1:
   1. Code Testing
   2. Data Governance
   3. Data Versioning
   4. BI Tool
2. Edit 2:
   1. terratest
3. Edit 3:
   1. Why Looker?
   2. How long and how many people did this take?
4. Edit 4:
   1. How do you handle data that fails validations defined in dbt tests or great expectations?
   2. Once these failed records are corrected, how do you re-integrate them into the destination tables?
   3. What is the data issue turnaround time?
5. Edit 5:
   1. Positions and Personelle

&#x200B;

# Storage

Tools Used:

* [AWS S3](https://aws.amazon.com/s3/)
* [Snowflake](https://www.snowflake.com/)

The storage is quite simple. We use S3 as a landing zone from the vast majority of sources. We then pick up those files and load them into Snowflake. I call this the **Lake House** as it serves as both our Data Lake and Data Warehouse. What is really beneficial about Snowflake is how it treats 'databases'. They aren't DBs in the traditional sense but more akin to workspaces. Because of that, you can have as many as you would like. This comes in handy when you want to practices [Gitlab's concept of Infinite Data Warehouses](https://www.youtube.com/watch?v=eu623QBwakc).

&#x200B;

# ELT

Tools Used:

* [Fivetran](https://fivetran.com/)
* [dbt](https://www.getdbt.com/)
* python

We use Fivetran for third-party vendors. dbt is used in the Lake House. Once data arrives in the raw databases, dbt takes over the management. If you aren't familiar with this tool, make it your best friend. It allows you to write SQL and YAML to define your data models. It also comes with super handy features like test definition, lineage, and document generation.

Because of Snowflake's awesome ability to scale, we bring all data in its raw format and then do our transformations with dbt.

For data from source systems, we keep it easy with CRON jobs running python scripts. This is very manageable because we are essentially just replicating the source data into the S3 Landing Zones. The only transformation that should be taking place at this step is the removal of PII.

&#x200B;

# Linting, Testing, and QA

# Tools Used:

* [dbt](https://www.getdbt.com/)
* [SQLFluff](https://github.com/sqlfluff/sqlfluff)
* [pylint](https://pylint.org/)
* [black](https://pypi.org/project/black/)
* [Great Expectations](https://greatexpectations.io/)

For linting, things are pretty straightforward on the python front. We use pylint and black to enforce a standard. From a SQL point of view, [SQLFluff is a new player to the field filling a much-needed space](https://www.youtube.com/watch?v=veYB9uh0RCM).

For testing, we use Great Expectations for data coming from source and arriving in the Lake House. Once in the Lake House we leverage a couple of dbt's packages to keep the workflow standardized. Firstly is a new port of Great Expectations syntax to dbt in [dbt\_expectations](https://github.com/calogica/dbt-expectations). This allows you to bake in tests as easily as

    models:
        - name:table_a
          description: Slightly modified copy of table_123
          tests:
              - dbt_expectations.expect_table_column_count_to_equal:
                  value: 27
              - dbt_expectations.expect_table_row_count_to_equal_other_table:
                  compare_model: source('raw_schema_a','table_123')
          columns:
            - name: month_value
              description: Month number [01 -> Jan, 02 -> Feb, etc.]
              tests:
                  - dbt_expectations.expect_column_to_exist    

We also use [dbt\_meta\_testing](https://github.com/tnightengale/dbt-meta-testing). This checks all your models to ensure all tables and columns have descriptions and that all required tests are defined. This is important because we then leverage `dbt docs generate && dbt docs serve`  which auto-generates our documentation and data lineage.

&#x200B;

# Infrastructure

Tools Used:

* [Terraform](https://www.terraform.io/)

We use Terraform of Infrastructure as Code (IaC). All of our infra is terraform'd, including our Snowflake instance. Terraform offers a [Snowflake provider](https://github.com/chanzuckerberg/terraform-provider-snowflake) which allows you to create tables, users, roles, schemas, etc. Given all of our managed infrastructure resides in AWS and Snowflake, everything is Terraformed.

&#x200B;

# Data Workflow Automation

Tools Used:

* [Prefect](https://www.prefect.io/)

There is a litany of tools out there for this (Airflow, Luigi, etc.) but we settled on Prefect for its simplicity. It focuses on data orchestration and does it very well. It also operates on what they call a hybrid model. This means that, if you are using their cloud UI, all they see is meta-data. None of your actual data is processed outside of your infrastructure.

You still need to have an agent running somewhere in-house though. For that, we are using [AWS ECS](https://aws.amazon.com/ecs/). Again this is Terraformed.

Prefect comes with tons of integrations, including Great Expectations and AWS. It is written in python so it works with our choice of analytics language. And it's Jupyter notebook plugin allows you to leverage the power of [papermill](https://github.com/nteract/papermill) to orchestrate, schedule, and run parameterized notebooks. Very handy for deploying models with all the benefits that come with notebooks.

# Development

**I give all credit for this idea to the GitLab data team**

All of our actual data modeling, analysis, ml work is containerized in [Docker](https://www.docker.com/). Depending on the work you are doing you spin up the appropriate image.  This has the benefit of not only being able to jump into an environment that has everything you need to do your work but also ensures that everyone and everything is running on the same baseline.

We also take advantage of Docker's volume bind so that changes made to the directory outside of the container are immediately reflected inside the container. This is important because it allows you to use your code editor ([VSCode](https://code.visualstudio.com/) in our case) and not have to operate in something like vim.

**- Analytics -**

We have an in-house `data-image` that comes pre-loaded with the familiar cast. Pandas, scypi, Jupyter, they are all installed and ready to be used. We also built our own helper libraries that make connecting to and extracting data easier. The goal here is to reduce all friction that exists when you want to start an analysis. Get it all out of the way so you can just do your work.

**- Engineering -**

Likewise, we have `dbt_image` and `prefect-image` which act in much the same way. They are all wired up so you can focus on the actual work instead of the technology.

&#x200B;

# CI

Tools Used:

* [Github Actions](https://github.com/features/actions)

Great so we have all of these tools - what about the dev-op-y stuff?

For that, we use GitHub Actions (but Gitlab is super powerful as well and I use it for my personal projects). As you may have noticed, everything we do has been codified. That allows us to create Actions that can handle:

* All of our testing and linting by defining steps accordingly
* Updating and provisioning our infrastructure with Terraform. Need to add a user? Cut a branch, add then to the `users.yml` file with the access they need and open a Merge Request.
* Need to create a replicated environment to try something new in AWS? Easy as changing the name.

But the real power comes in the use of the Docker images mentioned earlier. Because all of our workflows are containerized, we can then use those exact containers in both CI pipelines and production deployments. I know that if it works locally it should work all the way through.

Finally, using the concept of infinite data warehouses, you can have a Github action that goes up and replicates Lake House prod into a dev database for you. You can do whatever you want and it won't harm production. And then when you are done we can tear it down.

&#x200B;

You've made it this far. I am happy to answer questions. I am sure I have forgotten something. And I am positive that this will evolve and there are better solutions waiting to be uncovered.

&#x200B;

# Edit 1:

**Code Testing**

In regards to code testing, there isn't much to it. unittest for python is as simple as it gets.

For terraform our structure looks like below:

    .
    ├── main.tf
    ├── modules
    │   ├── module1
    │   │   └── main.tf
    │   └── module2
    │       └── main.tf
    ├── prod
    │    └── main.tf
    └── staging
        ├── main.tf
        └── test.sh

I know it violates some of the structure best practices but it makes it simple. The `test.sh`  file has commands to `plan`, `apply`, and `destroy` and the staging [`main.tf`](https://main.tf) has our staging environment variables in it. So GitHub Actions can just run that script.

**Data Governance**

We haven't done much in the way of Governance. But I am more of the mindset of Data Discovery than Governance. That is the next chunk I'm likely going to be tackling.

**Data Versioning**

Going back to the concept of Infinite Warehouses. Snowflake has an awesome `clone database` feature. And so every morning before the jobs run you can clone the database and name it something like `backup_raw_source_2021-02-01.` And then its up to you how many days you want to be backed up. Easily automated to create a lifecycle.

**BI Tools**

We use Looker. I am a huge fan of their explore feature but it also fits so well with our stack. Everything boils down to LookML which, as you may have guessed, is versioned. You then have the ability to create templates for reuse. And they also have what they call Blocks which are essentially just pre-made LookML files relating to a specific source (Youtube, Google Analytics, etc).

&#x200B;

# Edit 2:

Speaking of code testing, I literally just came across a Terraforming library called [Terratest](https://github.com/gruntwork-io/terratest/). I know nothing about it yet but it looks way more robust than what I had laid out above.

&#x200B;

# Edit 3:

**Why Looker?**

Looker was chosen for a couple reasons.

1. It sits directly on top of Snowflake. Most BI tools will extract the data onto their servers (on-prem or cloud) and then run the reports there. You are essentially paying for storage twice in this case. Looker leverages Snowflake entirely and so you are only paying once.
2. It is fully version controlled. Because everything is done in LookML we have it all available in our GitHub repo
3. It has an amazing explore feature which allows you to drill down as far as you are comfortable. This was critical because it added another access point for end-users. A focus has been on providing these access points where someone can get to the data and depth that they need depending on their capability. It isn't a - send me requirements - I build a report - you ingest. We really encourage other teams to get in, build, and explore. This gets them the Level 1 and Level 2 answers that would normally consume our time and instead frees us up to do more valuable and deep work.

**How Long and How Many People Did This Take?**

It took about a year from the start to where we are. A lot of that also included going down an Airflow route for a while before pivoting to Prefect, converting from Domo to Looker, and I had a baby.

Given how small we are I was wholly responsible for the architecture design and setup while the other members of my team focused on the analytics. The design was built specifically so that they didn't really have to learn any new technology.

Having said that, they did have to learn framework (dbt, prefect, terraform) syntax but that is reduced to SQL and YAML files for them.

&#x200B;

# Edit 4:

**How do you handle data that fails validations defined in dbt tests or great expectations?**

When something fails we have notifications in a slack channel dedicated to that tool. The reason for separate channels is minor, but it means we know exactly what tool and where in the pipeline the issue is.

We then create an issue for the error and include the lineage graph from dbt (looks like below) so that we know all the downstream potential impacts.

&#x200B;

Then, to fix the issue, we cut a new branch from our repo and go through our standard workflow. You spin up your container, create a dev DB, and fix the issue. You then push to Github and open a PR for CI and review.

On my list is to automate the issue creation and include that in the slack message.

**Once these failed records are corrected, how do you re-integrate them into the destination tables?**

Once everything is corrected, we just re-run the jobs. Dbt and prefect pull from the master branch of our `pipeline` repository. So once the PR is merged, the new code is available and the job will just pick it up.

**What is the turnaround time?**

It all depends on the scope and complexity of the failure. But it can be between minutes or until the end of the day (this is the case if we have to do a source -> Lake House data backfill so that we don't impact actual production during business hours)

&#x200B;

# Edit 5:

**Positions and Personelle**

What this framework does for the team is it eliminates the need for Data Scientists, BI analysts, and Data Engineers and replaces them with [Analytics Engineers](https://blog.getdbt.com/what-is-an-analytics-engineer/). At its root, this enables the analyst to build, maintain, and enhance our pipeline

A concept of Dev Ops that I love is the idea that waste is created in the system anytime there is a hand-off. Having 'Analytics Engineers' removes all the hand-off waste I've endured in the past when managing separate functional teams.

Eventually, there will be value in havering separate functions, but given how small we are, this allows us to hit all the needs.",115,41,Jwelch25,2021-02-19 02:35:12,https://www.reddit.com/r/dataengineering/comments/ln4fi7/dataopsish_design_and_workflow_walkthrough/,False,False,False,False
1b43j0z,Are we entering a boom time for Data Engineering?,"**Bull case:** AI is taking off. There will be new tools all over the place. Companies will need to port data around to use it in new ways not possible today. Data will become a big differentiator for products driving its value up. The need for data migrations is going to increase and the opportunity to streamline data management tools will increase accordingly.

**To discuss:** Do we agree with this bull case? Are the golden days of data engineering ahead of us? What can we do to capitalize on this opportunity? If we disagree then what am I missing?",115,62,jmack_startups,2024-03-01 19:47:19,https://www.reddit.com/r/dataengineering/comments/1b43j0z/are_we_entering_a_boom_time_for_data_engineering/,False,False,False,False
181mou1,"A takedown of Alteryx, no-code data as a concept and the people who force it on talented data folks.",,112,87,UCOVINed,2023-11-22 23:11:59,https://ucovi-data.com/BlogLatest.html,False,False,False,False
15oc8z5,Why is dbt popular for the transformation step?,"I did not work extensively with dbt, but it always seemed less appealing than having a databricks based pyspark/spark sql workflow + some orchestrator and some internal python libs to make things smooth. Sure data models and lineage are alright with dbt, but they can get out of hand with analysts chiming in. Maybe my DWH/etl usecases were not big enough for them not to be managed via an a priori sketched out data model, decent PKs and FKs, good naming conventions and job structure.

Making things performant and cost effective just seem more straight forward in databricks.

Maybe I just did not get the vibe. dbt cloud also kinda feels like the myriad of other scammy ""modern data tools"".

Curious about other perspectives!",116,73,None,2023-08-11 15:39:29,https://www.reddit.com/r/dataengineering/comments/15oc8z5/why_is_dbt_popular_for_the_transformation_step/,False,False,False,False
152yp2h,"Free copy of ""Fundamentals of Data Engineering"" to learn DE",,109,18,TemporaryPoorFrench,2023-07-18 13:36:06,https://go.redpanda.com/fundamentals-of-data-engineering,False,False,False,False
zgtrnk,"Dates are hard—we can relate to that, can't we r/dataengineering?","&#x200B;

https://preview.redd.it/s8llls4you4a1.png?width=460&format=png&auto=webp&s=0e7cccdde22e2da0be5d14dff0e0f7f02d664628

I love the irony of this :D 

&#x200B;

(and probably also the meta-paradox of being a jerk by posting this thus violating the very rule I'm citing 😉 )",115,30,rmoff,2022-12-09 10:38:02,https://www.reddit.com/r/dataengineering/comments/zgtrnk/dates_are_hardwe_can_relate_to_that_cant_we/,False,False,False,False
wo7cd7,FAANG Interview question styles for DEs,"When I check on the web, people usually suggest LeetCode for studying interviews for FAANG companies. That means it is mainly about data structures and algorihms. Is that valid for the data engineering field?

Although it is always good to know data structures, algorithms, etc., I don't think that this is the fundamental job of a data engineer.

**TL.DR**: As a data engineer who is targeting FAANG, do I start studying LeetCode? What kind of interview questions are asked by FAANG to data engineers?",117,38,Disastrous-State-503,2022-08-14 14:13:07,https://www.reddit.com/r/dataengineering/comments/wo7cd7/faang_interview_question_styles_for_des/,False,False,False,False
q37zfu,Does anyone know where to find jobs for companies doing good in the world?,I’ve never been one to really care much for money and I’d much rather just do some good in the world. How do you find these places?,115,67,Fatal_Conceit,2021-10-07 12:27:15,https://www.reddit.com/r/dataengineering/comments/q37zfu/does_anyone_know_where_to_find_jobs_for_companies/,False,False,False,False
pbd9gm,Recruiter reached out to me about a potential Data Engineer position. 1 hour later she sent her Robinhood affiliate link?,,114,29,mac-0,2021-08-25 15:13:22,https://i.imgur.com/GJzAso4.png,False,False,False,False
nac4do,How to present your data engineering work to 300+ persons from the whole company,,115,5,AndreSionek,2021-05-12 00:33:41,https://medium.com/gousto-engineering-techbrunch/from-bullet-points-to-data-storytelling-6b0db47ce1e6,False,False,False,False
1basru9,‘Data’ is essentially a consulting field in most companies ? Thoughts?,"I have been working as a DE for about a year and a half the following is my opinion on basis for my experience and looking at mu friends’ experiences. 90% of the roles in data are usually analytics, BI, data science. Even if it is a DE role it usually falls into one of the above. These roles typically exist in orgs which are not mature with and in data and execs work on excel. If this is the case, then the ‘data’ team’s priority is making a case for itself /selling itself with its initiatives add value to the execs. In my opinion this is very close to consulting. This causes a de-prioritization of DE work which can be lack of data modeling, no focus on data infra, data quality sucks etc. This makes DE a support role and a visibility lacking role. On the other hand, orgs which are mature with data, say Netflix, few mid sized startups and maybe few companies actually have real DE roles where focus is equal on infra, data quality, analytics, DS. If I want to get into these roles, it makes it tougher as there are so few of these. Would like to know thoughts of DEs/Senior DEs here who have been thru this/navigated/transitioned into something else from DE",111,35,cyamnihc,2024-03-09 21:29:56,https://www.reddit.com/r/dataengineering/comments/1basru9/data_is_essentially_a_consulting_field_in_most/,False,False,False,False
1b3zatv,Quarterly Salary Discussion - Mar 2024,"https://preview.redd.it/ia7kdykk8dlb1.png?width=500&format=png&auto=webp&s=5cbb667f30e089119bae1fcb2922ffac0700aecd

This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering.

# [Submit your salary here](https://tally.so/r/nraYkN)

You can view and analyze all of the data on our [DE salary page](https://dataengineering.wiki/Community/Salaries) and get involved with this open-source project [here](https://github.com/data-engineering-community/data-engineering-salaries).

&#x200B;

If you'd like to share publicly as well you can comment on this thread using the template below but it will not be reflected in the dataset:

1. Current title
2. Years of experience (YOE)
3. Location
4. Base salary & currency (dollars, euro, pesos, etc.)
5. Bonuses/Equity (optional)
6. Industry (optional)
7. Tech stack (optional)",113,191,AutoModerator,2024-03-01 17:00:46,https://www.reddit.com/r/dataengineering/comments/1b3zatv/quarterly_salary_discussion_mar_2024/,False,False,False,True
16rxj7v,Is there a great book on design patterns in data engineering?,"I've read ""Fundamentals of Data Engineering"" by Reis. However, as the name says that book covers the fundamentals. There are loads of books on software engineering design patterns in general. Not for data engineering in my knowledge.

I'm looking for a great book that goes through the popular data architecture patterns end-to-end. With code samples. Googled, but didn't find anything particularly exciting. Just a few blog posts. Which books do you folks suggest as essential reading for a data engineer?",114,29,newplayer12345,2023-09-25 16:58:49,https://www.reddit.com/r/dataengineering/comments/16rxj7v/is_there_a_great_book_on_design_patterns_in_data/,False,False,False,False
112l0h5,"Thoughts on ""Databricks ❤️ IDEs""?",,114,14,BoiElroy,2023-02-15 00:39:26,https://www.databricks.com/blog/2023/02/14/announcing-a-native-visual-studio-code-experience-for-databricks.html,False,False,False,False
10z1ft9,Big Data is Dead,Clickbaity title but this former Google BigQuery engineer has some really interesting things to say about why most companies do not or should not utilize big data.,111,68,FortunOfficial,2023-02-10 20:01:42,https://motherduck.com/blog/big-data-is-dead/,False,False,False,False
v0emaj,Background conversation in a CIA safehouse in the last Jason Bourne Movie 😆 Does the CIA pay well? Asking for a friend…,,111,10,leopkoo,2022-05-29 16:20:15,https://i.redd.it/o2umtypixf291.jpg,False,False,False,False
su180u,One thing that irks me about the software field: Many companies are not willing to give a candidate a chance if they do not know their specific tech.,"As you all know there are many versions of the same types of technology, and once you know one, you can fairly quickly pick up a new tech as long as you have a good baseline knowledge. This is even true for entire languages (Java, C#, etc)

I had a company I was working recently on a role ask me if I’ve used Airflow before in a professional environment. I said no but I know Python and I’ve used pipeline management platforms like Dagster and understand the concepts of it.

They got back to me and declined me. When I asked why (to see how I can improve), they told me they looking for people with more experience with the tools they use.

I don’t get this, a lot of DE tools need to be used in an enterprise environment and the only way someone ever learns in that scenario is by getting hired into it without prior knowledge of the exact tool.

Why is it that so many companies are unwilling to provide training or take a chance on candidates who have used other variations of the same tools they use? Have the people here experienced this?",113,76,ijpck,2022-02-16 17:18:18,https://www.reddit.com/r/dataengineering/comments/su180u/one_thing_that_irks_me_about_the_software_field/,False,False,False,False
ru62kw,Please suggest a book for Data Engineering concepts.,"I think it would be a good idea to grasp more knowledge about DE concepts, terms and data pipelines. 

I am interviewing to be a DE (I was a SDE for 5 years) and I have worked with Relational and Non-relational DBs in the past. I have knowledge of NLP and ML concepts too. 

I can prepare for the interviews through google articles but it does not give me satisfactory wisdom with DE. In interviews, I get lost when they ask me to create a data model from start to end. I need to learn more. 

 Can you please suggest a book ? If not book, then some series of articles or anything else?",112,73,GreedyCourse3116,2022-01-02 09:18:47,https://www.reddit.com/r/dataengineering/comments/ru62kw/please_suggest_a_book_for_data_engineering/,False,False,False,False
18420vg,What DBT hacks do you wish you knew sooner?,As per post title. Anything from macros to project config. Keen to hear and learn.,112,53,casematta,2023-11-26 04:00:36,https://www.reddit.com/r/dataengineering/comments/18420vg/what_dbt_hacks_do_you_wish_you_knew_sooner/,False,False,False,False
1792sm4,How to make these diagrams,,113,20,mommylovesme2,2023-10-16 10:23:23,https://i.redd.it/8u1jldiojjub1.png,False,False,False,False
135470k,Which ETL/ELT tools do you think have future in data engineering space?,"I'm looking to learn a tool but confused which to learn because there are a lot of tools and most of them are very expensive. I'm reasearching on Informatica, Ab Initio, Stitich. Any new tools you suggest which you think will have a future help would be much appreciated. 

I already know Spark, Databricks, Snowflake etc this question is for my friend who wants to get into data engineering who doesn't know coding.",111,104,digithat,2023-05-01 23:01:26,https://www.reddit.com/r/dataengineering/comments/135470k/which_etlelt_tools_do_you_think_have_future_in/,False,False,False,False
12hj52u,What problems does the “modern data stack” actually solve that have not been solved already?,"Help an old guy out here! I have been working in the data warehousing / business intelligence field for the last 25 years. I have probably been exposed to every traditional technology under the sun. I am honestly having a really hard time understanding what the big deal is with the whole data lake / Serverless X / spark / airflow / metric store / etc. 

It seems to me that these technologies are geared towards use cases that either involve data volumes that most companies will never reach or reinvent stuff that already exists (how is a metric store any different than an OLAP cube with calculations?)

What is it that’s so great that’s not solved by a file system, traditional ETL and a RDBMS?",114,72,orru75,2023-04-10 13:40:12,https://www.reddit.com/r/dataengineering/comments/12hj52u/what_problems_does_the_modern_data_stack_actually/,False,False,False,False
1c42ckw,Databricks SQL Warehouse is too expensive (for leadership),"Our team is paying around $5000/month for all querying/dashboards across the business and we are getting heat from senior leadership.

* Databricks SQL engine ($2500)
* Corresponding AWS costs for EC2 ($1900)
* GET requests from S3 (around $700)

Cluster Details:

* **Type:** Classic
* **Cluster size:** Small
* **Auto stop:** Off
* **Scaling:** Cluster count: Active *1* Min *1* Max *8*
* **Channel:** Current (v 2024.15)
* **Spot instance policy:** Cost optimized
* running 24/7 cost $2.64/h
* unity catalogue

Are these prices reasonable? Should I push back on senior leadership? Or are there any optimizations we could perform?

We are a company of 90 employees and need dashboards live 24/7 for oversees clients.

I've been thinking of syncing the data to Athena or Redshift and using one of them as the query engine. But it's very hard to calculate how much that would cost as its based on MB scanned for Athena.

Edit: I guess my main question is did any of you have any success using Athena/Redshift as a query engine on top of Databricks? ",109,87,the_underfitter,2024-04-14 19:48:42,https://www.reddit.com/r/dataengineering/comments/1c42ckw/databricks_sql_warehouse_is_too_expensive_for/,False,False,False,False
1c2bcq1,"Job Market Slightly Better, but Employers are SO PICKY and Hiring Process TOO LONG!!","Senior data engineer here with about 6 yoe. I've been casually applying for the past 4 years to keep a pulse on the market. These applications were for decent jobs at Tier 1-3 tech companies, or high growth startups. 

2021/2022 - 1 response for every 5 applications. Multiple recruiters hounding me every day. Process were expedited, I could expect to go from application to offer in 20 days. Job searcher rating: 10/10

2023 - 1 response for every 50 applications, if I'm lucky. 1 to 2 recruiter messages a month for jobs I would not consider. I could expect to get ghosted every time. Job searcher rating: 0/10

2024 - 1 response for every 25 applications. 3 to 4 recruiter messages a week, with maybe 1 I'd consider. I could expect 2 to 3 weeks between each stage, 4-6 stages, ghosting, 1 to 2 hour technicals, and take homes. Job searcher rating: 2/10

Overall: 

* Entry level is f'd.
* 1-4 years, it's possible.
* 5+ years, might as well be an acrobat, cause you're gonna be jumping through so many hoops and hurdles. 

At this point, if you're employed, it's so hard commit that much time to job searching. Thoughts?

&#x200B;",110,52,Capable-Jicama2155,2024-04-12 15:01:09,https://www.reddit.com/r/dataengineering/comments/1c2bcq1/job_market_slightly_better_but_employers_are_so/,False,False,False,False
ypfb1m,How much software engineering knowledge should a data engineer know?,"I’m trying to create a curriculum for the DE path to help other programmers prepare for Data Engineering. Some SE topics are required to learn because they are generally useful when developing software. I want to hand-pick some important topics for someone who is just getting started, It’s important to only pick topics that will be useful in DE.  
If someone wants to study data engineering, how much of the following SE-related stuff do you think is required?  


* Design patterns & Principles like KISS and YAGNI: books like pragmatic programmer have a handful of these.
* Testing: Mostly unit tests.
* APIs: I assume only basic REST knowledge is enough to get the concept, and people can learn other methods like Grpc and GraphQL on the job.
* Errors & Monitoring & Tracing: Being able to log correctly, send metrics where needed and write code that can be debugged if it crashes with traces & logs.
* System Design: Keep in mind the simple techniques like caching and scaling, Plus some other examples like how you design X(Do you know any good resource on this?)
* RDBMS: I never used more than SQL and ORM knowledge myself. Please suggest which areas you think are important. Is RDBMS internals relevant? Is ACID relevant?",111,79,glyphack,2022-11-08 08:14:29,https://www.reddit.com/r/dataengineering/comments/ypfb1m/how_much_software_engineering_knowledge_should_a/,False,False,False,False
ybc8ml,Anyone currently using an Operational Data Store?,,110,44,DrRedmondNYC,2022-10-23 08:30:47,https://i.redd.it/1sl9v1aa5kv91.png,False,False,False,False
xgq4dh,What is ideal or perfect ETL/ELT pipeline in data engineering? An interviewer ask this questions and here is my answer,"1.) Self-Heal
2.) should have auditing/error handing
3.) Self explanatory
4.) Properly documented
5.) De-couple
6.) Scale to handle any amount of the data
7.) Repeatable with same result. 
8.) Modular
9.) Cost efficient and should not incur any charge  while not using. 
10.) 100% availability
11.) Should have all the types of connectors for source and destination
12.) Easy to Deploy with CI/CD or any other process
13.) Granular production support.

What else Am I missing?",113,53,boogie_woogie_100,2022-09-17 15:55:46,https://www.reddit.com/r/dataengineering/comments/xgq4dh/what_is_ideal_or_perfect_etlelt_pipeline_in_data/,False,False,False,False
t28jy7,"Introducing Kestra, infinitely scalable open source orchestration and scheduling platform.","Today, our team is proud to announce a first public release of [Kestra](https://github.com/kestra-io/kestra), an open-source platform to orchestrate & schedule any kinds of workflow at scale.

## What is Kestra?

Kestra is :

* **an orchestrator**: Build a complex pipeline in couple of minutes.
* **a scheduler**: Launch your flows whatever your need!
* **a rich ui**: Create, run, and monitor all your flows with a real-time user interface.
* **a data orchestrator**: With its many plugins, build your data orchestration directly.
* **cloud native & scalable**: Scale to millions of executions without stress or hassle.
* **an all-in-one platform**: No need to use multiple tools to deliver a complete pipeline.
* **a pluggable platform** with the option to choose from several plugins or to build your own.

As you can see, Kestra will handle **all your pipelines** !

## The History of Kestra!

Kestra started in 2019 with this [initial commit](https://github.com/kestra-io/kestra/commit/d57e30c0c0d450590a1eaac5df0e82e1ea94e562). At this time, Kestra was at the proof-of-concept stage.

[Initial commit of Kestra](https://preview.redd.it/nzevvwt339k81.png?width=779&format=png&auto=webp&s=92f58459a6db4d64367582590854590dd1d63577)

To provide a bit of a background: I was working for Leroy Merlin as a consultant. We needed to build a new cloud-based data platform from scratch (destination: mostly Google Cloud Platform). We tried a [lot of things](https://kestra.io/blogs/2022-02-22-leroy-merlin-usage-kestra) and failed with some of our attempts. The **biggest setback was the orchestration** software that we tried to deliver with Apache Airflow: a lot of instability (tasks that failed simply due to the Airflow scheduler), performance issues (unable to handle a light workload), and a lack of features (scaling, data processing). After many tests (Google Composer, Open source Airflow on Kubernetes), the decision was final: **Airflow was rejected by Leroy Merlin**.

I did some research on the orchestrator ecosystem; most are **proprietary and license based** (far from my mindset), some are open source (at this time, only Apache Airflow seemed to be active — and it was rejected). I was really surprised by this discovery and faced this challenge from a co-worker:

>If you think Airflow is bad, do better!

It was decided: I set myself the task of producing a proof of concept for our own open-source workflow management system. It took a lot of time to build this software, and the task seemed to be never ending; but I continued to work on it for several months by:

* Choosing [Kafka as database and queue](https://github.com/kestra-io/kestra/commit/b4d026574c2fb141a3c7dd5b7f1481a31063acb2)
* Implementing [storage](https://github.com/kestra-io/kestra/commit/bcc5798d7fdcbe3afe95c019c41ddc546b24f62d) for task processing
* Choosing [ElasticSearch as a repository for UI](https://github.com/kestra-io/kestra/commit/2ede1e692be50999bc16f011f6a4796ffbbb9e1a)
* Adding some dynamic templating with [HandleBar](https://github.com/kestra-io/kestra/commit/05f1e20a3cb1e9a623024f5674144b3934cd5874) and changing it later to Peeble
* Starting some [Google Cloud](https://github.com/kestra-io/kestra/commit/14e3384be2144a2bf6698439b5ae22106ac83914) plugins
* Introducing [the UI](https://github.com/kestra-io/kestra/commit/1fef7509bb2d04b24bf66fce19b35dd01411a1db) — built with [Vue.js](https://vuejs.org/)

&#x200B;

[Kestra user interface](https://i.redd.it/ymdkj8lu89k81.gif)

And so on !

During a thirty-month period I built a variety of features, numerous plugins, and countless bug fixes — mostly during the night as I was still working as a full-time consultant for Leroy Merlin. It took a lot of effort, investment, and time that I could have spent with my family.

But now we are really proud of what we’ve achieved!

## Kestra is Open Source!

I'm a real open-source enthusiast. As an architect, I’ve been interested in open source solutions in IT for twenty years. I started as an open source consumer (using it without adding contributions, as is the case with most users). I then decided that the time was right to start out with the permissive [Apache License](https://github.com/kestra-io/kestra/blob/develop/LICENSE).

Three years ago, I started another open source project, [AKHQ](https://github.com/tchiotludo/akhq), with the same license. Working with a successful project was an invaluable experience for me as I was able to learn how to build a community around a project. I've also learnt that an open source system won't pay the bills on its own. AKHQ required a lot of personal investment; Kestra has required a lot more and will continue to do so in the future! This means you will have to ensure that you have the financial resources in place to enable your project to be viable and sustainable — we decided to create a company alongside Kestra in order to raise the required funds to support the development of the open source software.

The open source license is not limited and allows you to install and run it as you want on your server on premise or your cloud. We have also built our **Enterprise Edition** , bringing added security and productivity to your Kestra clusters. In addition, we plan to deliver Kestra in the form of software as a service in the near future (don't hesitate to [contact us](https://kestra.io/company/contact) for more information).

## Kestra Plugins are also Open Source!

[Kestra plugins already available](https://preview.redd.it/qnhl80en39k81.png?width=1141&format=png&auto=webp&s=ab3fae64526502d52c953efe412866fc6d4d7f08)

When implementing the deep integration of the tools and databases you are using, the connectors (what we call “plugins”) can present the biggest challenge. Most orchestrators (even proprietary and licensed based) only talk bash or cmd. You have to manage all of your needs with simple commands, often requiring you to use another tool in order to have access to the underlying resource (such as Talend). With Kestra, we want to have a deep integration with your tools and let [bash](https://kestra.io/plugins/core/tasks/scripts/io.kestra.core.tasks.scripts.Bash) deal solely with edge cases a plugin can't cover.

An example for a query to Google BigQuery:

>with Bash

    DATE=$(date --iso-8601=seconds)
    bq --format=json query 'SELECT name FROM \`project.dataset.table\` WHERE shippedDate=${DATE} AND shippedCountry = \'FR\'' > /tmp/query.json
    jq -r '.name' /tmp/query.json

>with Kestra

    - id: query
      type: io.kestra.plugin.gcp.bigquery.Query
      fetchOne: true
      sql: |
        SELECT name
        FROM `kestra-prd.demo.salesOrder` AS s
        WHERE shippedDate = '{{ now() }}'
        AND shippedCountry = 'FR'
    - id: ""return""
      type: ""io.kestra.core.tasks.debugs.Return""
      format: ""{{ outputs.query.row.name }}""

Kestra avoids the rigmarole of installing the software on the system, handling dependencies and conflicts, dealing with Python, etc. — just install a plugin (a simple jar) and speak directly with your database.

We have a [number of plugins](https://kestra.io/plugins/) and the process of [developing your own](https://kestra.io/docs/plugin-developer-guide/) is very simple. We also hope that a community will help us to maintain new plugins/connectors ([contact us](https://kestra.io/company/contact) if you require help or support).

## First Public Release and Production Ready!

First public release doesn't mean that Kestra is not production ready. In fact, it has been **used in production since August 2020 at Leroy Merlin** — take a deeper look at the [case study](https://kestra.io/blogs/2022-02-22-leroy-merlin-usage-kestra) if you want more detail. Here are some figures to give a picture of Kestra’s credentials:

* **4 clusters** one for every environment
* **200+ users/developers**
* **2000+ flows** in production
* **350,000 executions** every month
* **3,000,000 tasks** every month
* **Equivalent of 1,500 days of task processing time** every month (yeah, that’s the equivalent of fifty days of task processing every single day)

So, your next question is: **why are you waiting so long for the first public release?**

The answer is simple: we want to deliver the first impression as best as possible and this led to a lot of work: missing features, missing plugins, new UI design, polish of documentation and website. Now we are proud and confident enough in our product to display the result of our labor.

The road is not finished; we still have a lot to do. Stay tuned for the journey.

Stay connected and follow us on [GitHub](https://github.com/kestra-io/kestra), [Twitter](https://twitter.com/kestra_io) or [Slack](https://api.kestra.io/v1/communities/slack/redirect).",112,44,tchiotludo,2022-02-26 22:15:44,https://www.reddit.com/r/dataengineering/comments/t28jy7/introducing_kestra_infinitely_scalable_open/,False,False,False,False
q8zdlk,changing your attitude,"I just got back from a 1.5 week vacation trip two days ago. I immediately blew through all my project backlog which I had let sit for the past three months (really). The next day and today was me going back to old projects that didn't work out and fixing them. And responding to requests that I had not seen before (with a little creativity). It's a great feeling. 

But if i had skipped the trip and just kept ""attending"" work none of that would have been accomplished.

Something to think about.",110,23,chaoticalheavy,2021-10-15 22:47:29,https://www.reddit.com/r/dataengineering/comments/q8zdlk/changing_your_attitude/,False,False,False,False
1bl3n3r,Writing effective SQL,"Hi, r/dataengineering!   

Over the last ten years, I've written tons of SQL and learned a few lessons. I [summarize them in a blog post.](https://ploomber.io/blog/sql/)

A few things I discuss:   

* When should I use Python/R over SQL? (and vice versa)   
* How to write clean SQL queries   
* How to document queries   
* Auto-formatting   
* Debugging   
* Templating   
* Testing   

I hope you enjoy it!   ",109,38,databot_,2024-03-22 16:38:00,https://www.reddit.com/r/dataengineering/comments/1bl3n3r/writing_effective_sql/,False,False,False,False
1b0up0e,Read/Filter a 1.7 TB CSV File in Python,"I'm reaching a mental breaking point.

I have a 1.7TB csv file that I need to filter and store two columns from as a new csv based on if column 'ID' is in a predetermined set of ID's (roughly 135,000,000) . I've tried playing around with Dask to speed up the process but set the blocksize to 50MB and just had it run for 8+ days without converging.

I really don't know what to do at this point or if it is possible to make an efficient script to do this.",111,89,The-Salamander-Fan,2024-02-26 22:20:31,https://www.reddit.com/r/dataengineering/comments/1b0up0e/readfilter_a_17_tb_csv_file_in_python/,False,False,False,False
1asc1gj,PySpark Tutorial in Jupyter Notebook,"I created this Jupyter Notebook when I started learning PySpark, intended as a cheat sheet for me when working with it. Since I started learning PySpark with the book ""Data Analysis with Python and PySpark"", this notebook can be seen as my learning notes focused on practical coding. If you want to fully understand PySpark, I highly recommend reading the book. Originally, I put it on Kaggle. I rediscovered it recently and want to share it. It can be a good starting point for beginners who want to learn PySpark.

Link to notebook: [https://gist.github.com/ThaiDat/81c3662801aa8410a65b94f3c993c377](https://gist.github.com/ThaiDat/81c3662801aa8410a65b94f3c993c377)

Some related posts that expand/explain the notebook:

[PySpark common operations](https://note.datengineer.dev/posts/a-practical-pyspark-tutorial-for-beginners-in-jupyter-notebook/)

[PySpark UDFs](https://note.datengineer.dev/posts/pyspark-udfs-a-comprehensive-guide-to-unlock-pyspark-potential/)",109,10,ImportantA,2024-02-16 15:58:48,https://www.reddit.com/r/dataengineering/comments/1asc1gj/pyspark_tutorial_in_jupyter_notebook/,False,False,False,False
17j3bt3,Why did DE go the vendor tooling (hell) route for most things vs SWE where the solution is language frameworks/libraries and do less (or more) code.,"Title. This is more of a disappointment with expectation vs reality more than a rant really.  When I first started in Data, I was an analyst and immediately found out that ""deriving trends and insights"" from data wasn't for me. I realized I liked and wanted to be a builder, making tangible things in production. A SWE fits the bill but I still wanted to be in data so Data Engineering felt like the best route. But I was honestly disappointed that left and right vendor tools are the prescribed solution to everything.

It also made me sad that wanting to build anything on your own was ""reinventing the wheel"" apparently. So maybe people can give insights here how a DE is just as technical as a SWE because I see people here become adamant that a DE can even be more technical as a SWE? For me a technical solution is making your API or your own server with Go or something as a backend engineer. Using vendor A product that has a connector between OLTP and OLAP databases aren't as exciting honestly. Custom built solutions are what I want vs throwing money at the problem.

 And how did all this happen in the first place? Is Data Engineer just too broad a spectrum? A good technical DE isn't worth the ROI for pipelines vs a SWE building applications?",110,88,codeejen,2023-10-29 13:49:09,https://www.reddit.com/r/dataengineering/comments/17j3bt3/why_did_de_go_the_vendor_tooling_hell_route_for/,False,False,False,False
16l4r9l,Every other post in this sub seems to be “going from DA to DE?” or “should I learn X?”.,"It’s the same shit over and over, and it’s getting a little tiring. There is some great content and interesting discussions from time to time — I look forward to occasional article or post with lots of comments, usually get inspired by or learn something in there. But these newbie/career advice are the same thing over and over and over and over again. Do people not search prior posts? Do they just need some validation or words of encouragement for **their** particular situation? 

That’s it, rant over.",109,47,EarthGoddessDude,2023-09-17 16:14:08,https://www.reddit.com/r/dataengineering/comments/16l4r9l/every_other_post_in_this_sub_seems_to_be_going/,False,False,False,False
13vrzlt,What does dbt Labs get wrong about dbt best practices?,"I've seen a bunch of scattered criticism of how [dbt's official docs](https://docs.getdbt.com/guides/best-practices) describe best practices for the tool, but I haven't come across anything centralized here or elsewhere, so I thought this would be useful as a discussion topic where people could make their points about specific flaws and propose alternatives (or say which parts they agree with).

The two overarching points that I see come up are that their best practices:

* Encourage lock-in
* Lead to a large proliferation in models that become difficult to maintain and expensive to run

Do people agree with that premise?

EDIT: To clarify, I am more interested in issues with suggested best practices than I am issues with dbt itself - they're obviously related but I think it makes sense to separate those discussions.",110,59,PaginatedSalmon,2023-05-30 14:35:04,https://www.reddit.com/r/dataengineering/comments/13vrzlt/what_does_dbt_labs_get_wrong_about_dbt_best/,False,False,False,False
u5pbql,"Is it me, or does blockchain have zero use cases in data infrastructure?","Blockchain has been a buzzword for years, but I still don't see the big deal.  The update/write times makes it unusable in production.  



Also, I hear a lot of people say blockchain can be used to keep a record of transactions.  That pretty much sounds like any data warehouse I worked with.",109,58,None,2022-04-17 15:27:12,https://www.reddit.com/r/dataengineering/comments/u5pbql/is_it_me_or_does_blockchain_have_zero_use_cases/,False,False,False,False
suvukx,r/DataEngineering Buzzwords - Details in comments,,108,26,JEs4,2022-02-17 18:35:11,https://i.redd.it/467kafwtqfi81.png,False,False,False,False
oquubj,What are some intermediate to advanced level SQL and Python topics that someone starting in Data Engineering should know?,"Someone being me, having good understanding and experience with most basics of SQL and Python, and some intermediate topics as well. Want to know what are the must-have intermediate to advanced level SQL & Python topics I should know when applying for Data Engineering jobs?",112,35,marshr9523,2021-07-24 17:55:14,https://www.reddit.com/r/dataengineering/comments/oquubj/what_are_some_intermediate_to_advanced_level_sql/,False,False,False,False
1b04b8j,"Marry, F, kill… databricks, snowflake, ms fabric?","Curious what you guys see as the romantic market force and best platform. If you had to marry just one? Which is it and why? What does your company use? 

Thanks. You are deciding my life and future right now. ",107,121,JamesGarrison,2024-02-26 00:48:29,https://www.reddit.com/r/dataengineering/comments/1b04b8j/marry_f_kill_databricks_snowflake_ms_fabric/,False,False,False,False
1aldl7r,How do these companies make money?,"I am currently a student of data engineering, and I just built my first pipelines using terraform, airflow, dbt, cosmos, snowflake, bigquery etc..

But all the tools I used were free... How the heck does Hashicorp (for terraform), Apache (for airflow), DBT labs (for dbt), and Astronomer (for cosmos) make any money? 

Sorry just one of those embarrasingly basic questions but I still don't get it",108,48,KimchiFitness,2024-02-07 21:03:09,https://www.reddit.com/r/dataengineering/comments/1aldl7r/how_do_these_companies_make_money/,False,False,False,False
15bs36m,What's the coolest data you've worked with,I know we're all cool and blase and here for the money (except for us euro folks that are chronically underpayed) but i'm curious: what's the coolest data you've ever had to deal with in your projects? Maybe something bio related? space image data? massive network traffic data? ,108,105,None,2023-07-28 09:03:44,https://www.reddit.com/r/dataengineering/comments/15bs36m/whats_the_coolest_data_youve_worked_with/,False,False,False,False
14kra4i,Are these terms irrelevant in the industry anymore?,"I am having interviews to hire someone who will work for me. I interviewed two people so far. Neither of them answered on questions:

&#x200B;

1. OLAP and OLTP systems
2. Star Schema vs. Cube
3. ETL vs. ELT
4. Window function SQL question

&#x200B;

It is a position for 3+ years in data analytics, business intelligence, or a related field and I didn't expect to get the full extent of complete answers. Am I asking too difficult questions? or am I becoming out of touch and those aren't relevant anymore?

&#x200B;

Edit: I didn't really make it clear what the role is for. The role is BI Engineer, but the candidates that the head hunter sent to our HR manager happened to have a data analyst background. ",107,116,Bloodylime,2023-06-27 22:13:05,https://www.reddit.com/r/dataengineering/comments/14kra4i/are_these_terms_irrelevant_in_the_industry_anymore/,False,False,False,False
1449ezb,Most companies are rushing to build or incorporate #gpt in their value chain. #genai. Do you agree?,,108,30,de4all,2023-06-08 13:32:17,https://i.redd.it/228ijxavqs4b1.jpg,False,False,False,False
12qldg7,Does data engineering not have as much interesting career progression as other areas of engineering?,"I've been a data engineer for almost 5 years, and I noticed that of the many data engineers I have worked with, almost all of them have stayed in data engineering.  They have either moved into more senior data engineering positions or became data engineering managers.




My friends who are software engineers have had more interesting career progressions.  For example, one engineer started out as a backend developer, moved into full stack development to pick up frontend skills, and is currently in devops.  I'm surprised my data engineering colleagues have not seen similar career progression since a lot of companies see us as specialized backend engineers, but for data.




I enjoy data engineering, but I don't see myself doing this my whole career.  I would eventually enjoy going into devops or becoming a software reliability engineer.",110,55,level_126_programmer,2023-04-18 12:26:42,https://www.reddit.com/r/dataengineering/comments/12qldg7/does_data_engineering_not_have_as_much/,False,False,False,False
v4xtu9,Remote data engineers deserve salaries based on the market where the company performs primary operations,"I am a data engineer who moved from SF to Seattle. Since I went fully remote, my salary has flatlined.

I have grown my team, improved productivity, improved output, and even fly down once a month to give in-person trainings.

I am a manager of a team of 5. I feel that I should continue to be paid a San Francisco-based engineering salary, including objectively-deserved pay raises, even while being fully remote.

I continue to hear about this push by employers to reduce the pay of engineers who work remotely full-time, citing reduced cost of living as the primary reason.

Excuse me?  My personal cost of living is my own business and not the business of my employer. That should not be leveraged against me.

The value delivered to the marketplace is roughly the same whether the code and management are written and performed from SF, Seattle, or the other side of the planet.

Using my cost of living as an excuse to pay me less results in more profit for the company at my full expense and nothing else changes. That is not a fair shake.

If they want they can hire someone else from Seattle for 50% below market rate, but it's not going to work for me anymore.

I have seen 1.5 years of no increases after requesting salary increases twice over (2) 6-month periods and showing expected deliverables were achieved for both periods. Still getting stiffed.

I think some employers just don't understand what data engineering is valued at. Before our systems were in place, seeing the problems their employees were dealing with was like eating glass and staring into the abyss.

* Hour long report refreshes that had a 30% reliability rate.
* Users having to manually integrate 2 ERPs in Excel each month for budget vs cost analysis.
* Never ending data schlepping for monthly, quarterly, and annual financial reporting requiring 2 dedicated FTE to manage both using 100% of their time.

4 years later our business has grown 2x even through the pandemic. If these systems weren't here and new ones not currently being planned and delivered, the business would crumble.

But they would rather lose millions than pay their data engineering team manager an extra $10k per year.

I have an interview on Monday for a job that pays $30k more, with $135-175k RSUs, unlimited PTO and 6-week paid sabbatical every 4 years. Currently I only get 3 weeks PTO. If I get this offer I will be jumping ship *immediately*.

Grateful for the experience at prior company, but they lost touch with my needs and made decisions that were ignorant and offensive to me. They said I was lucky I had my remote contract signed when I did, because soon thereafter new remote employees had their salaries cut (albeit on other teams w/ different operating models).

**But I reserve that I should earn the fair market rate geolocated to the company's primary operations, and commensurate with the value my role delivers to that market.**

Nothing about my cost of living should impact my salary.

What does Reddit think tho?",109,67,FactMuncher,2022-06-04 20:52:58,https://www.reddit.com/r/dataengineering/comments/v4xtu9/remote_data_engineers_deserve_salaries_based_on/,False,False,False,False
q8mbpt,"If leetcode is for software engineers, what's it then for data engineers?","To get a high tc for software engineers they must grind leetcode extremely, but what is thd counterpart for this for data engineers",107,65,izner82,2021-10-15 11:19:01,https://www.reddit.com/r/dataengineering/comments/q8mbpt/if_leetcode_is_for_software_engineers_whats_it/,False,False,False,False
1bit81p,"Stretching the truth about being a ""Data Engineer""","Has anyone ever stretched the truth about them being a DE? I was reading a post on Reddit where someone was a Data Analyst, and just did cloud work and projects on his own. He said he put his job title down as ""Data Engineer"" and ended up getting a DE job.  


Does that sound like something common in the job field? I have heard horror stories of people being hired in and not showing competencies. ",107,60,DarkPaladin67,2024-03-19 19:42:15,https://www.reddit.com/r/dataengineering/comments/1bit81p/stretching_the_truth_about_being_a_data_engineer/,False,False,False,False
19difxp,Is the Data Space really this Complicated or am I just overthinking?,"For some reason, everytime I try to learn I see new tools and how they ease the existing work. And I end up wasting more time where if I spent that on actually learning, I would be way ahead. How do you know which tool to pick and choose(from the noise in the market) ?

https://preview.redd.it/ji5thy5f05ec1.png?width=2013&format=png&auto=webp&s=167f4e2afce621cc135d5a0ff7d5c484fedaa032",104,88,_areebpasha,2024-01-23 06:55:02,https://www.reddit.com/r/dataengineering/comments/19difxp/is_the_data_space_really_this_complicated_or_am_i/,False,False,False,False
167b3ep,Quarterly Salary Discussion - Sep 2023,"https://preview.redd.it/ia7kdykk8dlb1.png?width=500&format=png&auto=webp&s=5cbb667f30e089119bae1fcb2922ffac0700aecd

This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering.

# [Submit your salary here](https://tally.so/r/nraYkN)

&#x200B;

If you'd like to share publicly as well you can optionally comment below and include the following:

1. Current title
2. Years of experience (YOE)
3. Location
4. Base salary & currency (dollars, euro, pesos, etc.)
5. Bonuses/Equity (optional)
6. Industry (optional)
7. Tech stack (optional)",105,250,AutoModerator,2023-09-01 16:01:00,https://www.reddit.com/r/dataengineering/comments/167b3ep/quarterly_salary_discussion_sep_2023/,False,False,False,False
10njfnd,(RANT) I think I'll die trying to setup and run Spark with Python in my local environment,"For the last 4 hours, one error or the other keeps popping up. Never faced something like this in any other software.

Update: I'm making do with Pyspark in my Jupyter Notebook. Had to configure some things but things working fine as of now.",109,59,riderx65,2023-01-28 16:45:05,https://www.reddit.com/r/dataengineering/comments/10njfnd/rant_i_think_ill_die_trying_to_setup_and_run/,False,False,False,False
rvel59,"2021, will it change in 2022?",,108,91,randomusicjunkie,2022-01-03 22:57:42,https://i.redd.it/uh7ss39czj981.jpg,False,False,False,False
mmrgpo,Looking for open source projects that use data pipelines and big data flows,"As a software developer planning to move into data engineering, I'm looking for open source projects where I can learn from and contribute to the actual building & maintenance of data pipelines.

I don't mean developing open source data engineering tools like Kafka or RabbitMQ. I mean projects that are actually \*using\* those kinds of tools to consume big data and build out data pipelines. I can and will learn on my own by building out pipelines on my own personal projects. But I'm trying to find projects where I can collaborate with others on production-like pipelines using real-world data and tools.

I know there are plenty of projects that use open government/open data/etc. types of sources. I figure a lot of them must have solid data architectures using pipelines, data lakes, ELT/ETL processes, data warehouses, etc. Do you know of any where people can contribute to those processes? Thanks!",109,31,devon_b,2021-04-08 13:11:29,https://www.reddit.com/r/dataengineering/comments/mmrgpo/looking_for_open_source_projects_that_use_data/,False,False,False,False
18gfpn9,Why do job descriptions demand skills that are not at all needed?,"Just accepted a data engineering job. Talked about graph databases, unstructured data, python, DAGs, etc during the interview. 

Day one I'm told the job will be making dashboards in tableau under the direction of senior data scientist. 

Why?",104,29,FisterAct,2023-12-12 06:36:39,https://www.reddit.com/r/dataengineering/comments/18gfpn9/why_do_job_descriptions_demand_skills_that_are/,False,False,False,False
168p757,Will Airflow become obsolete in coming years?,"I see a lot of new orchestration tools popping up. Especially in the last 2 years. A few prominent ones are:

1) Prefect

2) Mage

3) Dagster

All three projects look solid, and all of them cover the most common use cases of data engineering. Which revolve mainly around orchestrating and error handling batch data jobs.

Having personally used Airflow, I know how quirky it is. With its idiosyncrasies and an awkward learning curve. Not to mention a nightmare to manage, if you're handling infrastructure yourself.

Are we witnessing a Hadoop v Spark battle in the orchestration world?

Apart from legacy systems, are there any good enough reasons in 2023 to still pick up Airflow over other tools, if I'm starting a new project?",108,126,newplayer12345,2023-09-03 06:08:34,https://www.reddit.com/r/dataengineering/comments/168p757/will_airflow_become_obsolete_in_coming_years/,False,False,False,False
13pvcss,What’s your opinion on today’s release of Microsoft fabric?,,106,64,boogie_woogie_100,2023-05-23 17:42:58,https://i.redd.it/1urjzbw1tn1b1.jpg,False,False,False,False
137ij6s,What kind of work can a data engineer do if they can't find employment as a data engineer?,"I'm currently employed as a data engineer, with about 5 years of experience across this field and devops.  I decided to give applying in this job market a try, and was surprised to see how competitive everything is even at smaller/less well-known companies.  Companies whose interview process 2 years ago involved asking candidates to find the most frequently occurring element in an array are now asking Leetcode hard questions and exact experience with certain technologies (even though it can be learned quickly!).




Even ignoring my work experience, I had a much easier time as a very average new grad engineer.  How can a data engineer get employed as soon as possible, if data and devops engineering jobs have gotten so competitive even at smaller companies?  I am mostly talking about any job that is tech/IT related.",105,60,level_126_programmer,2023-05-04 12:01:59,https://www.reddit.com/r/dataengineering/comments/137ij6s/what_kind_of_work_can_a_data_engineer_do_if_they/,False,False,False,False
y5n0oj,Has anyone built a data warehouse primarily using Databricks?,"I hear many cases of Snowflake and Azure Synapse serving as several company's main data warehousing platform, but interestingly I haven't heard much on Databricks serving as the central data warehouse platform even though Databricks themselves claim you can certainly do so. 

Has anyone been in a situation where you've built or come across data warehouses with Databricks used primarily?",105,85,No_Stick_8227,2022-10-16 17:48:32,https://www.reddit.com/r/dataengineering/comments/y5n0oj/has_anyone_built_a_data_warehouse_primarily_using/,False,False,False,False
odgm3w,I've built online resource where aspiring data engineers can learn modern tools in production like env. What are your thoughts?,"Hi. I’ve noticed that most online tutoring platforms are focused on specific cloud vendors (AWS, GCP, etc.) and their data tools. There is no online platform where you can try vanilla Airflow or Kafka, you can only install them on your local machine, and I believe that’s a hindrance. That’s why I decided to build a platform where aspiring DEs can learn various tools without installing anything on their local machines.

Here how it works inside (in case of Airflow):

I manually created few servers that Airflow interacts with. One server acts as RBDMS, second server acts as REST API, third server acts as HDFS with preloaded data. Every 5 minutes servers receive new data.

User story:

1. User registers and selects tools.
2. User receives hands-on tutorials that explain how to setup working Airflow pipeline.
3. User works inside hosted jupyter environment where she writes code and has access to shell.
4. User builds real pipeline that ingests raw data from several servers and performs transformations according to business requirements.
5. User can schedule DAG to run every ten minutes and every run will produce new results.

What user gets:

1. Immediate access to cloud sandbox environment (which is quite comfortable)
2. Every tutorial has its own business quest and interacts with many preconfigured servers
3. In no time user understands how different data tools are used
4. Preconfigured servers emulate production environment, so that student knows what to expect in real life
5. Experience with Airflow, Kafka, PySpark, Hadoop (HDFS & Hive), Cassandra, Mongo, Postres, MySQL, DBT, Neo4j

Note, every tool has its own business quest.

Experienced data engineers please share your thoughts.",106,18,SerialBussy,2021-07-04 08:58:50,https://www.reddit.com/r/dataengineering/comments/odgm3w/ive_built_online_resource_where_aspiring_data/,False,False,False,False
ms33t0,Open source contributions for a Data Engineer?,"What are some good git projects that a Data Engineer can target to increase their skills? Contributing to which git projects have helped you so far?

Edit:

Listing down all the repos mentioned in the comments below -

* [Quinn](https://github.com/MrPowers/quinn)
* [Chispa](https://github.com/MrPowers/chispa)
* [Spark-daria](https://github.com/MrPowers/spark-daria)
* [Spark-fast-tests](https://github.com/MrPowers/spark-fast-tests)
* [Soda SQL](https://github.com/sodadata/soda-sql)
* [Spark-rapids](https://github.com/NVIDIA/spark-rapids)
* [Airbyte](https://github.com/airbytehq/airbyte)
* [Singer](https://github.com/singer-io)
* [Meltano](https://gitlab.com/meltano/meltano)
* [SQLfluff](https://github.com/sqlfluff/sqlfluff)
* [DataGristle](https://github.com/kenfar/DataGristle)
* [Perfect](https://github.com/PrefectHQ/prefect)
* [Metabase](https://github.com/metabase/metabase)
* [Superset](https://github.com/apache/superset)
* [Streamlit](https://github.com/streamlit/streamlit)
* [Skytrax Data Warehouse](https://github.com/iam-mhaseeb/Skytrax-Data-Warehouse)
*  [Dagster](https://github.com/dagster-io/dagster) 
* [Top 20 list by Data Council](https://petesoder.medium.com/what-are-the-most-popular-oss-data-projects-of-2021-84ef021bb5a2)",106,55,porcelainsmile,2021-04-16 13:26:06,https://www.reddit.com/r/dataengineering/comments/ms33t0/open_source_contributions_for_a_data_engineer/,False,False,False,False
17qge22,Is SELECT DISTINCT really that bad?,"I have been pushing back on DBT (sql on Sowflake) Pull Requests that use SELECT DISTINCT and instead ask people to create a surrogate key and aggregate/de-dupe explicitly on the keys they want to define uniqueness by. This is a lot more work. Yet, we all know the urge to SELECT DISTINCT “just in case” to avoid the dreaded duplicates test error or find by a stakeholder. 

I find myself wondering lately if my blanket rule against SELECT DISTINCT and blocking people’s work because of it, is outdated and misguided, Am I unnecessarily asking more work without enough evidence to the value add or risk mitigation? Because I’ve been so strict on this my whole career (14 yrs) and my teammates along the way have also agreed, major issues resulting from its use has not come up in recent memory. 

But I am now in the process of transferring the ownership of these models and reviewer pool admin to a new group who disagrees with me on this point. 

Does anyone have any horror stories of how SELECT DISTINCT caused problems? Query runtimes, cost, troubleshooting issues? Or the opposite—Has using SELECT DISTINCT consistently, made your life easier? I of course can do some testing myself and query plan analyses but I’m also looking for anecdotes and others’ experiences.",105,115,mrp4434,2023-11-08 07:23:54,https://www.reddit.com/r/dataengineering/comments/17qge22/is_select_distinct_really_that_bad/,False,False,False,False
1454ghq,Have you ever seen a successful “self-serve” implementation?,"My employer has spent sickening amounts of money on various “self serve” analytics/bi/de tools. I have been successful on some small projects like setting up tools to generate commonly requested excel files and dashboards to let people filter and play around with stuff that been largely defined I.e return on sales…. But that’s akin to letting somebody drive the car. You can drive it however you want, but we built the roads so you can only go where we say you can go and if you want to drive somewhere new, the engineering team is going to have to build you a new road.  

Have you ever witnessed the ever elusive “self serve” where entry level hourly workers are solving complex problems and generating value for companies? Or is somebody doing AVG/SUM in some carefully curated DB view in power bi and calling it ML the absolute ceiling?",102,48,None,2023-06-09 12:52:41,https://www.reddit.com/r/dataengineering/comments/1454ghq/have_you_ever_seen_a_successful_selfserve/,False,False,False,False
v5yp7l,Building a simple ETL for personal projects,"Hi, DE newbie here.

I want to build a simple ETL with AWS for a personal project and I'm a bit overwhelmed by all the tools that are out here. 

Basically, I have a few functions that get data from a blockchain via an API. I want to run them on a daily basis and store the data in a database that I can query with SQL for analytics purposes.

I did some research into architectures but it's unclear to me which stack is most suitable for my use case. Specifically, I have the following doubts:

1. Which tool to use for scheduling? Argo, Airflow, Prefect or simply a AWS Lambda function?
2. How to store the data? Directly append a Redshift table? Store the data files in S3? Or maybe first S3 and then have a AWS Glue job ingest data to Redshift periodically?
3. How to query the data? Do I even need a relational DB service or could I just query S3 directly? 

How would you design this architecture? Any help is highly appreciated! :)

Thanks a lot!",103,38,2PLEXX,2022-06-06 09:14:33,https://www.reddit.com/r/dataengineering/comments/v5yp7l/building_a_simple_etl_for_personal_projects/,False,False,False,False
jwh0qv,New courses on distributed systems and elliptic curve cryptography (by Martin Kleppmann),,103,7,nfrankel,2020-11-18 14:45:38,https://martin.kleppmann.com/2020/11/18/distributed-systems-and-elliptic-curves.html,False,False,False,False
1axyooe,Talend is no longer free," Now that Talend is no longer free, what other ETL tool would you recommend that has data transformation capabilities as powerful as the tMap component?

[https://www.talend.com/products/talend-open-studio/](https://www.talend.com/products/talend-open-studio/)

Thanks!

Edit: We need to deploy each ETL in client environments, which is why Talend was good for us, it generate the .jar files and a ready-to-run .bat file",104,112,Comfortable-Bug9572,2024-02-23 11:55:21,https://www.reddit.com/r/dataengineering/comments/1axyooe/talend_is_no_longer_free/,False,False,False,False
19cak0s,what is it that you do for work again?,"&#x200B;

https://i.redd.it/zhg215lraudc1.gif",106,26,mesirmysir,2024-01-21 18:51:36,https://www.reddit.com/r/dataengineering/comments/19cak0s/what_is_it_that_you_do_for_work_again/,False,False,False,False
14lfqfu,Delta Lake 3.0,"Just announced, Delta Lake 3.0 now compatible with Hudi and Iceberg.

My life just got more interesting.",104,79,rexicusmaximus,2023-06-28 17:33:34,https://www.reddit.com/r/dataengineering/comments/14lfqfu/delta_lake_30/,False,False,False,False
10tjhve,"What did ETL look like before the ""modern data stack"" was a thing?","This is kind of a noob question: When I started working in this field I was already being bombarded with blog posts, ads and tutorials about dbt, Fivetran (or the like), Snowflake etc...

But I'm curious to know, what did ETL look like before this?

What did engineers have to do, before Fivetran or dbt were a thing, to move data from OLTP sytems to OLAP ones? And to model it?

Sometimes I find tutorials like ""Build an ETL pipeline with Airflow and Pandas"" and I think ""Pandas, Really?"".

In other words: If all services like Fivetran and dbt disappeared tomorrow, what would I need to learn / use to extract and model data in a production system?

Please do share your experience!",106,220,wtfzambo,2023-02-04 15:48:44,https://www.reddit.com/r/dataengineering/comments/10tjhve/what_did_etl_look_like_before_the_modern_data/,False,False,False,False
ycbwv0,"What do you consider ""advanced"" SQL","Well it's Monday morning back to work. I'm finishing up some QA queries from last week. In my opinion QA is one of the most tedious parts of Data Engineering because it's rather time consuming and many times it seems like your ETLs or Pipelines are working just fine but they might be missing a key data element.

In an attempt to automate some of this I am creating stored procedures that can dynamically iterate through tables and check for specific data points (record count, columns with null values where there should be something, ratio of nulls to non nulls).

Got a little bit of everything in there, temp tables, variables, a while loop, dynamic SQL. The only thing missing is a cursor or any XML functions.

At what point do you consider SQL transitioning from basic to advanced. For me I consider that line when you start using the programmability features like making stored procedures or functions that accept parameters for inputs or can store result sets into variables. However some people still consider this basic SQL and don't think of it as being advanced until you start getting down and dirty with some of the features like CURSORS, Dynamic SQL, and all of that XML PATH stuff.

Personally I've only used the XML functions for string concatenation and manipulation but I've seen entire queries written with those XML commands and they are pretty complex. I am sure if I had a better understanding of XML they would appear much more simple but I never use XML for anything. Especially with JSON being widely used now.

 Anyway where does everyone draw the line on what they considered advanced SQL. I expect the responses to vary widely.",107,77,DrRedmondNYC,2022-10-24 13:55:36,https://www.reddit.com/r/dataengineering/comments/ycbwv0/what_do_you_consider_advanced_sql/,False,False,False,False
qupb5w,Databricks responds to Snowflake (TPC-DS),,104,66,Blayzovich,2021-11-15 20:15:26,https://databricks.com/blog/2021/11/15/snowflake-claims-similar-price-performance-to-databricks-but-not-so-fast.html,False,False,False,False
pved42,Data Engineering Mentor Network,"We've noticed a trend in members who are interested in mentorship opportunities because mentorship is one of the best ways to get personalized advice to help you reach your goals.

https://preview.redd.it/4jikhbxn2pp71.png?width=847&format=png&auto=webp&s=7d41763b890168227c48a0b8e45e939adcf48f30

That's why over the next few months we are creating a network of verified expert mentors.

**As a mentee**, you will be able to search our directory of mentors and schedule time to get 1:1 personalized advice, join a live event, or join a cohort-based course.

**As a mentor**, you will be able to engage directly with the community and offer mentorship as well as be able to set your own rates and earn revenue for paid content you create.

If you're interested in finding a mentor, [please fill out this form](https://airtable.com/shrVdtALd6aZkvMMQ).

If you're interested in becoming a mentor, [please fill out this form](https://airtable.com/shr7dzDz37OvrskX5).

&#x200B;

**What is the #1 topic you would like to discuss with a mentor? Tell us in the comments 👇**",104,31,AutoModerator,2021-09-25 20:00:22,https://www.reddit.com/r/dataengineering/comments/pved42/data_engineering_mentor_network/,False,False,False,False
k7xo0q,Useful resources for Data Engineers,,104,11,oleg_agapov,2020-12-06 17:16:39,https://github.com/oleg-agapov/data-engineering-book/blob/master/book/1-introduction-to-data-engineering/1.3-useful-resources.md,False,False,False,False
1b9ba5c,Data architecture diagram tools,"Hi! I've found gifs with this style on LinkedIn a couple of times and I wonder which tool was it made on. In some posts they say it's Excalidraw, but I can't find the moving arrows style.

But let's not make it so specific. Which visual tools do you use to diagram your data architectures/solutions? I almost always use **draw.io**

https://i.redd.it/wwy0b4byi0nc1.gif",102,42,thehelptea,2024-03-08 01:27:24,https://www.reddit.com/r/dataengineering/comments/1b9ba5c/data_architecture_diagram_tools/,False,False,False,False
16snd4x,What whitepapers should every data engineer read?,Looking for suggestions that you consider fundamental/useful to any DE.,107,22,theporterhaus,2023-09-26 13:02:14,https://www.reddit.com/r/dataengineering/comments/16snd4x/what_whitepapers_should_every_data_engineer_read/,False,False,False,False
11uiemx,Books that made you become a better engineer,As the title suggest I’d love to know what books you felt made you become a better data / software engineer. Ones that helped you either advance your career or changed the way you thought about data / programming in general.,103,28,GC_invests,2023-03-18 08:27:18,https://www.reddit.com/r/dataengineering/comments/11uiemx/books_that_made_you_become_a_better_engineer/,False,False,False,False
yhhl2y,What skill would you learn after Python and SQL?,"Hey everyone!

I already know Python and SQL and I’m wondering what would be a good skill to learn next for becoming more experienced with data engineering.

Thank you in advance!",101,71,The-Fourth-Hokage,2022-10-30 15:49:54,https://www.reddit.com/r/dataengineering/comments/yhhl2y/what_skill_would_you_learn_after_python_and_sql/,False,False,False,False
1935ykj,Reality check: How good are you at the skills in your tech stack?,"So let's start by saying this is by no means a post to complain but just to get a reality check and understand what you guys mean when you name a technology in your tech stack. I was navigating the sub and found the posts like the salary discussion or newbies asking for help, and you've got the usual comments throwing around a bunch of technologies like it's fresh water. Oh sure just learn aws, sql, python, powerbi, airflow, terraform and docker and you're good to go, took me a year. Obviously started questioning if I'm dumb since I've been at this job 4 years and I'm currently mastering SQL after spending 3 years on PowerBI only

So I want to understand when you name these tools and put them in your tech stack, how good are you actually at this and how much is it just ""I understand the basics and I can Google/ChatGPT the rest"" ?

Let's take SQL for example. There's a huge difference between ""Udemy course"" level of knowledge (you got the basic idea, can use SQL up to subqueries and Window Functions) and that one colleague that can write 1000+ lines of stored procedure from scratch and model JSON into tables level of knowledge. Or PowerBI: again there's a difference between ""I can drag and drop objects on the canvas and create a visualization, yaay let me add it to the CV"" and having read The definitive guide to DAX 700+ pages on PowerBI's programming language, understanding how Vertipaq engine works internally and so on. You might say it is overkill to invest that much time in one single tech but that's another topic I don't want to tackle now.

For example, I don't use Python daily at my job, but I can do some stuff with it with the help of Google and Chatgpt. I know the basics of programming, I've done a couple Udemy courses out of curiosity, I know what sets and dictionaries are, I can query an API, do some stuff with the common libraries for data manipulation and return the data. If I have to touch a Python script written by another DEV to modify something specific, I can do that. But I don't have profound knowledge of the internals, I wouldn't know how to optimize code, I probably couldn't do heavy tasks on Python-built infrastructure unless the task was very clear or build something enterprise-level from scratch myself. Do you think I should name Python in my tech stack? Is this an acceptable level of knowledge for you to name in the tech stack? 

So yeah I just need to know what's the idea of this sub on this topic because there's one of two possible outcomes:

1. I'm studying in the wrong way, and it's taking me a lot more than a normal person to really understand these tools
2. I am underselling myself and suffering from imposter syndrome or something like that

Cheers ",98,73,schizo_coder,2024-01-10 11:15:25,https://www.reddit.com/r/dataengineering/comments/1935ykj/reality_check_how_good_are_you_at_the_skills_in/,False,False,False,False
17b6wdl,PyGWalker: a Python library for data engineer that turns your dataframe into tableau-like data app.,"PyGWalker is a python library that turns your dataframe (or a database connection) to an embeddable tableau-like user interface for visual analysis.

It can be used to explore and visualize your data in juypter notebook without switching between different tools. It can also be used with streamlit to host and share an interactive data app on web.

PyGWalker Github: [https://github.com/Kanaries/pygwalker](https://github.com/Kanaries/pygwalker)

[pygwalker in juypter lab](https://preview.redd.it/lor544wm82vb1.png?width=3002&format=png&auto=webp&s=993a09ffb21075b1a4a213e3988c09b9a2be1bdd)

A simple example of how to use pygwalker, you can also check more information at official doc of pygwalker: https://docs.kanaries.net/pygwalker

    import pygwalker as pyg
    import pandas as pd
    
    df = pd.read_csv(""you_data"")
    
    # then pass it to pygwalker
    pyg.walk(df)",101,20,Sudden_Beginning_597,2023-10-19 01:16:44,https://www.reddit.com/r/dataengineering/comments/17b6wdl/pygwalker_a_python_library_for_data_engineer_that/,False,False,False,False
1460qft,"Streaming data engineering project: Apache Flink, Apache Kafka, Prometheus, & Graphana running on Docker.","
Hello everyone,

  I've seen (& written) a lot of articles explaining batch data processing. However, not much content explains the concepts to be aware of when designing streaming data pipelines. 

  With that in mind, I wrote an article that covers the fundamental concepts to know when building a streaming data pipeline. It covers concepts such as 

  1. State
  2. Watermarking
  3. Backpressure
  4. Joins in streaming systems and their caveats
  5. Monitoring

The concepts are explained as you build a streaming data pipeline that does [clickstream attribution](https://www.shopify.com/blog/marketing-attribution#3) (which is very common in marketing).

  Post: https://www.startdataengineering.com/post/data-engineering-project-for-beginners-stream-edition/

  Code: https://github.com/josephmachado/beginner_de_project_stream

  Appreciate any questions, feedback, or comments. I hope this helps someone.",101,12,joseph_machado,2023-06-10 13:47:50,https://www.reddit.com/r/dataengineering/comments/1460qft/streaming_data_engineering_project_apache_flink/,False,False,False,False
12w49y1,Is it a data structure and algorithm a must for data engineering roles,"Background

I am a 2 YOE data analyst from a finance background but I am doing lots of data engineering work, so I want to make a move on to a data engineer role. 

I am certified with AWS SAA and CCP, and I am going to get the Data Analytics Specialty certification. I have built some data pipelines with AWS services with python scripting.

My question is: Is it a data structure and algorithm a must for data engineering roles?  
I had a few interviews and there were coding interviews, but I failed so badly due to the data structure and algorithm questions. It sometimes feels like a software engineer role - perhaps data engineers are really software engineers who focus on data ?

Additionally, some role require ML knowledge in the coding interview, and again I did it not very well. 

I am not so sure if I should slightly leave AWS certifications and focus more on data structure and algorithm knowledge.   
It would be highly appreciated if anyone could share your experience, please?

p.s. I am seeking ways to improve myself or succeed in the interview.   
Thank you so much",103,81,uk_dataguy,2023-04-23 10:37:00,https://www.reddit.com/r/dataengineering/comments/12w49y1/is_it_a_data_structure_and_algorithm_a_must_for/,False,False,False,False
w0dz7f,Build Awesome Data Engineering Portfolio from Scratch in 2022 | Complete Guide,"Hi Data People,

📣 Data Engineering Project Alert 📣

👨🏻‍💻 The best way to learn anything is by doing hands-on practice and doing projects is one of the ways  


💻 Having done so many projects on my YouTube channel, I have come up with a new project challenge  


🤩 Build Awesome Data Engineering Portfolio from Scratch in 2022  


👉🏻 In this challenge, you will spend 45 days executing an end-to-end data engineering project  


What You Will Learn? 👇🏻  
✅ Researching problems and building dataset  
✅ Data Modelling  
✅ Working with Database (PostgreSQL/MySQL)  
✅ Cloud Data Services (AWS/Azure/GCP)  
✅ Dimension Modelling  
✅ Data Warehouse (Redshift/BigQuery/Snowflake)  
✅ ETL (Spark/Python/PySpark)  
✅ Dashboarding(QuickSight/Tableau/DataStudio)  


📈 No only this but you will also learn about Content Creation and Learning in public  


🕵🏻‍♂️ I have given a detailed explanation in the video

[https://youtu.be/UIZdjAKadc8](https://youtu.be/UIZdjAKadc8)",99,6,darshill,2022-07-16 10:55:54,https://www.reddit.com/r/dataengineering/comments/w0dz7f/build_awesome_data_engineering_portfolio_from/,False,False,False,False
qm6jpa,"CMV: Data Engineers should code, not build ad-hoc dashboards",Or call it Data Analyst.,103,56,rckahuna,2021-11-03 22:53:29,https://www.reddit.com/r/dataengineering/comments/qm6jpa/cmv_data_engineers_should_code_not_build_adhoc/,False,False,False,False
opt99w,Is it me or are beginner-friendly ETL pipeline guides that explain from the ground-up how to incorporate the use of various technologies notoriously difficult to find.,"So this is something I've been struggling with for a couple of weeks. 

I have a small windows EC2 instance (1gb RAM, so it's tiny) where I'm running a simple python script that does some ETL. 

It reads files in an S3 bucket, and then saves some data in a MySQL db. Really simple code that a beginner could have put together. 

However this little set-up we have going clearly is neither sustainable nor scalable, as sometimes it ends up crashing the EC2 instance it's running on. A better solution would incorporate the use of technologies able to perform auto-scaling, but we haven't had much luck finding guides about this that teach you how to incorporate those things from the ground up. Most of what we found assume a lot of background knowledge.

So my question would be this - given that the only things I have familiarity with are Python and MySQL, what would be a good resource that would hold my hand through setting up an alien process like Airflow/Docker/Spark/etc to perform this more efficiently.",103,19,Lostwhispers05,2021-07-23 02:49:31,https://www.reddit.com/r/dataengineering/comments/opt99w/is_it_me_or_are_beginnerfriendly_etl_pipeline/,False,False,False,False
f9l209,Want to learn Data Engineering? Here are some Example Projects to get your hands dirty.,,99,5,sanchit089,2020-02-26 01:19:45,https://github.com/san089/Udacity-Data-Engineering-Projects,False,False,False,False
18dvjbf,Rockstar Data Engineers making big bucks: what are you doing exactly?,"I'm wondering what is the responsibility and daily work like.

Edit: not Rockstar Games.",101,95,mackbenc,2023-12-08 20:14:08,https://www.reddit.com/r/dataengineering/comments/18dvjbf/rockstar_data_engineers_making_big_bucks_what_are/,False,False,False,False
184cqy2,What are your favourite data buzzwords? I.e. Terms or words or sayings that make you want to barf or roll your eyes every time you hear it.,What are your favourite data buzzwords? I.e. Terms or words or sayings that make you want to barf or roll your eyes every time you hear it.,101,224,TheDataGentleman,2023-11-26 15:10:53,https://www.reddit.com/r/dataengineering/comments/184cqy2/what_are_your_favourite_data_buzzwords_ie_terms/,False,False,False,False
17asnwh,"Have you seen any examples of “serious” companies using anything other than Power BI or Tableau for their data viz, including customer facing analytics? Example: pro-code tools like Shiny, Python Dash, or D3.","
I get the (false?) impression that the visual end of the data stack is always Power BI or Tableau, but is that true?

Would love to hear from other DEs that serve data to **pro-code** visualization tools like Shiny, Dash, or D3.js.

Trying to get a sense of how common these pro-code tools are in an enterprise, and/or customer facing analytics, or if it’s just hobbyists and companies that can’t afford Tableau/PBI.",101,187,icysandstone,2023-10-18 14:52:12,https://www.reddit.com/r/dataengineering/comments/17asnwh/have_you_seen_any_examples_of_serious_companies/,False,False,False,False
110gth0,What do you personally do to improve as a DE?,"Side projects? Reading? Courses?

As someone with no formal background in SWE/tech, most of my improvement has come from the desire to learn more about things. Not the most structured approach though, just curious to know how everyone else keeps their skills sharp!",100,41,rlyply,2023-02-12 14:21:44,https://www.reddit.com/r/dataengineering/comments/110gth0/what_do_you_personally_do_to_improve_as_a_de/,False,False,False,False
10qdxjr,Feel a bit burned from a take-home assignment I completed during an interview,"Hey everyone, I'm sorry if this post is long...there's a **tldr** at the bottom. I just want a place to vent and get feedback - I feel discouraged and frustrated. About three weeks ago, I had an initial screening with a recruiter from a tech start-up based in Europe. It led to a technical interview for a remote data engineering role (US) with them. The first technical interview went well, and I was informed that the next step would be a take-home assignment. The assignment asked me to do pretty much what I would be doing if I received the position:

1. Write an API that connects to one of their datasets
2. include multiple parameters for aggregation, querying, and filtering
3. put it on the cloud
4. write documentation for it
5. Write a report of why you chose the tools you did and what you would do differently.

The time limit on the assignment was approximately three hours. This struck me as a red flag because I felt like this was a lot of work to accomplish in less than half a work day (I would love your thoughts on that, actually. Maybe I'm just slow.)? The assignment was also very open-ended. There was no mention of which dataset to connect to, how to connect to it, which tools to use, or really any expectations on their end.

I'm not a fan of take-home assignments because I've heard horror stories of people getting used for their free work. Since it was so open-ended, only three hours long, and the interviewer was super chill when talking about the assignment (""We just need something to get a feel for your Python skills"") I assumed it was ok to build a small application showcasing the fundamentals of API development.

I used one of their smaller datasets, coded my API in Python using Django and Pandas, hosted it on an EC2 instance, and wrote markdown documentation. In my report, I mention how I would use Spark, tokenization (I was working with text data), and a proper database if I were to make this app production grade. I included all the drawbacks of my current application and how I would fix them - because of course - the assignment was only supposed to be three hours long, and there was only so much I was willing to do for a take-home assignment. I felt like this was a lot of work as it is, and it took me roughly 6.5 hours to complete.

After submitting my assignment, I got an email the next day telling me I did a great job and they would like to proceed with the second technical interview. The second interview ended well, with the interviewer telling me they would schedule a final interview with the CTO. Instead, I received another email three days later stating that the company decided to terminate my interview process. They said it was because my app was not using a large enough dataset, my code wasn't efficient enough, pretty much telling me that it wasn't applicable for a large-scale system. ????????. Of course not? This is a take-home assignment that was supposed to take 3 hours to finish. I am honestly baffled. Is this normal? I understand there are people out there who probably can write an enterprise-level API in 3 hours, but even so, why do it for free during the interview process? At this point, I feel a bit sketched out and like I dodged a bullet. It's still discouraging to make it so far in the interview process and not get an offer, though.

&#x200B;

**TLDR**; *I completed a take-home assignment, and I feel kind of shitty about it. Considering avoiding companies that ask me to do this in the future...*",101,108,LusciousPigeon,2023-01-31 23:25:33,https://www.reddit.com/r/dataengineering/comments/10qdxjr/feel_a_bit_burned_from_a_takehome_assignment_i/,False,False,False,False
103j5cq,"What parts of ""the data warehouse toolkit"" and ""designing data intensive applications"" are important to read?","The two books are regularly mentioned as must-reads for DE's ; but they're pretty long, and some parts are outdated. What parts are still important to read for someone who wants to get into the field, has a technical data background, but doesn't want to read 1000 odd pages?

&#x200B;

Also, would you recommend any other books? specifically any with practice problems?",100,27,Particular-Bet-1828,2023-01-04 23:45:12,https://www.reddit.com/r/dataengineering/comments/103j5cq/what_parts_of_the_data_warehouse_toolkit_and/,False,False,False,False
yy2qmg,Snowflake syntax now supports EXCLUDE & RENAME syntax,"[https://select.dev/posts/exclude-rename](https://select.dev/posts/exclude-rename)

https://preview.redd.it/2inizkec1l0a1.png?width=1626&format=png&auto=webp&s=1513a820971b80f542a1a75b6a46d3ab9cefb140",102,39,ian-whitestone,2022-11-17 21:52:31,https://www.reddit.com/r/dataengineering/comments/yy2qmg/snowflake_syntax_now_supports_exclude_rename/,False,False,False,False
w1z2at,"I start my first day as a Data Engineer next Monday, any tips?","Hey Everyone, I’m overly excited that I achieved this role.

I live here in the USA, MCOL (Atlanta, GA). My degree is in Bio-Chemistry and I’m all self-taught in programming/Data Science. I have roughly 1 1/2 YOE in Data Science, I was a Inventory Analyst my first year and Systems Analyst in the second position at my most recent company.

Total Compensation: $120,000/yr

I already work heavily in SQL and light to medium in Python. I’ve been studying AWS (S3, Heavy on Lambda, DynamoDB, & etc) and Spark jobs, as the that’s the platform the new company uses. I’ve been using Udemy courses heavily and they have helped a lot.

I would like some advice on what to focus on: 
- How to not be nervous with my first Engineering job
- Did I get a good salary for my first Engineer position
- How can I grow into a better Engineer or to even a Architect?
- Then any questions or advice you all can give.

Please and thank you for all help.",102,93,Scales25,2022-07-18 13:13:11,https://www.reddit.com/r/dataengineering/comments/w1z2at/i_start_my_first_day_as_a_data_engineer_next/,False,False,False,False
1bf4aft,How do I future proof my career as a Data Engineer?,"AI at this point is inevitable and it’s become quite clear to me that the roles and responsibilities of a data engineer today will significantly change as AI tools become more common place. At this point it’s  all speculative but my questions are
A) what does the data engineer of tomorrow look like
B) how can I adapt to a changing landscape and essentially future proof my career

Any advice will be greatly appreciated!


EDIT:

Thanks for all the helpful advice and comments (even the neuralink suggestion haha). I think my biggest takeaway is that AI is a tool, and like any other tool will still need humans to apply it. But the biggest thing I can do to develop my career is to enhance my soft skills i.e. stakeholder management, communication etc… as well as keeping up to date with the latest trends and developments in the industry. Thanks everyone, I’m glad to be part of such an awesome subreddit!",99,86,Ill-Advisor-8235,2024-03-15 03:19:05,https://www.reddit.com/r/dataengineering/comments/1bf4aft/how_do_i_future_proof_my_career_as_a_data_engineer/,False,False,False,False
11f0ckj,What is the role of Kubernetes in Data Engineering?,"The title basically. I am trying to upskill myself and Kubernetes is in my list to learn. Before I dive in I need to first understand how k8s is used in data engg. 
Can you share how you are using k8s in your etl pipeline? Anything is appreciated. Thanks!",97,50,_barnuts,2023-03-01 10:27:57,https://www.reddit.com/r/dataengineering/comments/11f0ckj/what_is_the_role_of_kubernetes_in_data_engineering/,False,False,False,False
wimm53,best books on data engineering? (not necessarily technical ones),"I'm looking for more general books on data/data engineering (i.e. not a technical book on Apache Spark for example) - if there are more general technical books, or technical books you feel are transferable enough (or simply just must reads) then feel free to fire them in, just try be clear if it's technical or not

I'm keen to learn more from the product/platform/use case aspect, hence the more generalist request.",100,37,tea_horse,2022-08-07 18:34:46,https://www.reddit.com/r/dataengineering/comments/wimm53/best_books_on_data_engineering_not_necessarily/,False,False,False,False
vrucvw,ETL Pipeline Testing,"I'm new to data engineering trying to learn from diffrent ressources and tutorials online what i dont find easily is Testing pipelines  do you have any  good ressources  where to learn and practice ETL testing ? 

what do you exactly  test in an ETL  and what does those tests include  ?",99,17,Significant-Ad-1712,2022-07-05 10:01:33,https://www.reddit.com/r/dataengineering/comments/vrucvw/etl_pipeline_testing/,False,False,False,False
tj81in,Wrote a post about how we implemented The Modern Data Stack at ManyPets,"Wrote [this post](https://medium.com/data-manypets/how-manypets-implemented-the-modern-data-stack-35877715c0da) about our data architecture. We use a cloud data warehouse and the group of low/no-code tools which is now getting called The Modern Data Stack.  


Not everyone should copy this stack but it does work well for us. It's come up on this subreddit before if you can use these tools for higher volume work. So to give some context on that, we've hundreds of thousands of customers and our warehouse tables are typically in the 10s of GB scale. There's a couple in the 100s of GB but few over 1TB.",100,26,today_is_tuesday,2022-03-21 09:46:00,https://www.reddit.com/r/dataengineering/comments/tj81in/wrote_a_post_about_how_we_implemented_the_modern/,False,False,False,False
iwzjm8,Data Engineering from the Ground Up - Part 2: Better Pipelines with Python and Idempotency,,96,26,None,2020-09-21 12:35:07,https://peterdannemann.com/better-pipelines-with-python-and-idempotency/,False,False,False,False
1b6dgsp,I created an open-source microsite to help analysts and SQL-heavy devs get started with Spark,"I help companies build and scale machine learning and analytics applications, with Spark being a core part of our data processing toolkit. While Python is generally the go-to language for data processing, libraries like Pandas & NumPy come with a LOT of sharp edges. This is especially true if you're trying to read someone else's code. 

IMO Spark (& PySpark) has an edge over other data processing tools for a few reasons: it reads more like SQL (making it a easier to understand without digging through obscure documentation), it's well-supported across cloud platforms, and it's dead simple to scale to handle any size data you need to throw at it.

I wanted to create a resource that anyone with basic Python and SQL knowledge can use to get up and running quickly with Spark (you can probably learn 80% of what you need in a day or two). I also wanted to include some suggestions around code conventions to help with readability and avoiding gotchas that trip a lot of folks up (e.g. duplicating columns when doing joins).

You can get started with either a web notebook or locally with a batteries-included Docker container. 

You can access it at [SparkMadeEasy.com](https://sparkmadeeasy.com/).

This is still a work-in-progress, with more topics to come (e.g. Spark-ML). Happy to hear any feedback!",102,16,zchtsk,2024-03-04 15:19:58,https://www.reddit.com/r/dataengineering/comments/1b6dgsp/i_created_an_opensource_microsite_to_help/,False,False,False,False
19501yg,My whole team hates DLTs and I don't blame them.,"We have been using databricks(aws) close to a year now and have started working with DLTs \[Delta Live Tables\]. I personally don't hate them as much as my teammates but I don't blame them, a lot of DLT limitations are in direct contradiction with the Databricks vision. Reasons listed below:

* You have to be on shared compute, this complicates reading data and writing back out to s3 if you need to drop a file (need to be in single user mode)
* BIGGEST COMPLAINT: You cannot ""hop"" catalogs or even schemas. This is so weird to me. They are rolling out DLT and UC \[Unity Catalog\] and are pushing customers hard on both, but DLTs directly contradict the medallion architecture. You want to have data land in a bronze catalog, then move it to a silver, gold, etc. Great, create a pipeline for each and kill your job runtime because you now have to spin up 3 different computes. They had a private preview that allowed you to write to multiple schemas but they killed it. Why?
* DLTs have to run from workspace notebooks, because GIT providers can only point to a users specific repo, not a config'd repo and branch like a notebook job. Luckily we have DABs to control our deployment and skirt this issue but it seems so odd to us. The DLT documentation recommends setting up a repo per pipeline, thats insane!
* Cannot share compute across multiple pipelines. To my second point, the limitation of not being able to hop schemas/catalogs wouldn't matter if I could specify DLT compute to use in the three pipelines processing the data. That solves a huge problem.
* Documentation, community support is still weak.

I do love the automation DLT brings with ingesting data, specifically CDC data. But there are some pain-points that make absolutely no sense. I think Databricks is doing too much too fast and needs to refocus on what they were initially, a data platform that provided one place to do everything.",98,70,DataDoyle,2024-01-12 17:06:52,https://www.reddit.com/r/dataengineering/comments/19501yg/my_whole_team_hates_dlts_and_i_dont_blame_them/,False,False,False,False
11i4n2h,Why would I choose Snowflake over BigQuery?,"Apologies if this is already covered somewhere as it seems like a simple question, but I'm not clear why I'd ever choose Snowflake.

The costs seem to be higher, and the integrated tooling seems to lock me into a vendor in the way that using 3rd party or open source tools would not.

I have nowhere near enough data to come close to hitting size or performance limits on either.

What am I missing?",99,73,Far_Deer_8686,2023-03-04 16:32:02,https://www.reddit.com/r/dataengineering/comments/11i4n2h/why_would_i_choose_snowflake_over_bigquery/,False,False,False,False
113x4cb,Data Engineering Competition!,"Inspired by this [post](https://www.reddit.com/r/dataengineering/comments/11349lf/is_there_anything_like_kaggle_for_data_engineering/) and this [comment](https://www.reddit.com/r/dataengineering/comments/11349lf/comment/j8spjrg/), would r/dataengineering be interested in a project based competition? (mainly for learning purposes)

To keep things simple, we could use reddit polls to host it. We can decide on the project (and the winner) using votes.

We can hash out the details if there's enough interest, but I'd be willing to chip in the first $500 to the winning pot. My personal preference is to donate the winnings but will also defer this decision to a poll.

**Open questions:**

1. What should the scope of the project be? Data Engineering is a very broad field.
2. Do you see any downside to deciding the project using a reddit poll?
3. Do you see any downside to deciding the winner using a reddit poll?
4. How long should the competition run? 4 weeks should be max for building a production-ready project on the side (to account for DEs with full time jobs and give new DEs time to learn)

Let me know what you think :)",102,48,datain30,2023-02-16 18:16:19,https://www.reddit.com/r/dataengineering/comments/113x4cb/data_engineering_competition/,False,False,False,False
10wcbp7,Leetcode for Spark and PySpark at Zillacode.com,,99,15,dmage5000,2023-02-07 20:34:07,https://i.redd.it/h3tyyo81utga1.png,False,False,False,False
10uw3rn,End to End Pyspark Testing CI/CD Example Repo,"Over the past while I've been battling with some local pyspark and docker-compose setups to have better tests for CI/CD.   
To path to solve this problem was to try get the same level of depth of environment that runs in production in the company I work at (databricks, AWS EMR, S3, etc.) as locally  


Figured I'd share out some of wrapped up knowledge of battling with specific Jars and hive-metastore (which you don't actually need, just a SQL database set up in certain way!) 

The repo has the following features in it  
 

* Quick and Easy setup for local testing and development environment and abstracting the complexity of configuring up a pyspark environment in this way
* Full Pyspark Implementation
* Full S3 like implementation with Minio
* Read and write data to S3 and access them as tables and databases in Spark through metastore
* Ability to run pytest on the pyspark container
* Read and write with delta format
* Example of testing pyspark code with either unittest or pytest
* Consistent environment for testing and development with docker-compose and poetry
* Ability to run tests on push with github actions  


If I've missed anything please feel free to let me know! Hope a few of you find this useful

[https://github.com/emmc15/pyspark-testing-env](https://github.com/emmc15/pyspark-testing-env)",100,5,Oct8-Danger,2023-02-06 03:15:18,https://www.reddit.com/r/dataengineering/comments/10uw3rn/end_to_end_pyspark_testing_cicd_example_repo/,False,False,False,False
zfv38r,Rust for Data Engineering—what's the hype about? 🦀,"This is a great article that tries to explain [why people everywhere are speaking about Rust](https://www.adventofdata.com/rust-for-data-engineering/). To be honest it feels more like a under the hood thing rather than a full switch to the language and the article depicts this super well.

Mainly it says:

* Why Rust: because Rust compiler is strict, easier to use than C/C++ out of JVM
* When Rust should be used: when you want speed and performance with data, Rust and Arrow are well integrated and with security about your data types
* When Rust should not be used: sometimes Rust type safety is too rigid for data (hf read a CSV), when you want to go fast or on a new project because the learning curve is steep

*🎄 PS: if you did not see this article is day 8 in the Advent of Data. Advent of Data is a advent calendar, everyday in December a new article about data is published.*",101,40,blef__,2022-12-08 10:18:28,https://www.reddit.com/r/dataengineering/comments/zfv38r/rust_for_data_engineeringwhats_the_hype_about/,False,False,False,False
xr1bk7,Databricks Vs Snowflake - User Experience,"Hello all,

We are currently evaluating **Databricks (on AWS)** and Snowflake for our company, can fellow DE's share some experience/thoughts for the below questions

Consider the below scenario where there are **no cluster/SQL warehouses** running on both platforms, a data analyst comes in and starts hitting a table with some queries and this is what happens on both the platforms (correct me if I am wrong with this)

**Databricks** \- The compute cluster / SQL warehouse takes 3-4 mins to spin up, and the first query that hits it takes considerable time to return some results.

**Snowflake -** The SQL warehouse startup time is within seconds and even the first query returns results fast

I know in Databricks we can create cluster pools and keep some clusters idle (warm) which can reduce the startup time but still are we not paying more money to keep servers inactive 24/7.

The question is how teams  are managing Databricks to be always up all the time and at the same time maintaining the costs, doesn't Snowflake has an edge over this as you are not always keeping the cluster/warehouse active

One of our main evaluation criteria is User experience (at the same time maintaining low costs), we don't want people using these platforms needs to wait for a considerable amount of time running their queries.

Also, for people using Databricks are there any guidelines for choosing the instance families for the cluster, as there are quite a lot of them, it would be great if you guys can provide some tips on choosing it as we feel the jobs are running slow because we are not choosing the right instance family for the cluster and do you guys recommend using SQL warehouses as they are spinning up quite large machines (which can cost more) even for a smaller warehouse.

Just trying to understand real-life examples from teams who are using Databricks now

Any help on the above is highly appreciated. Thanks in advance",99,77,psgharen,2022-09-29 07:28:30,https://www.reddit.com/r/dataengineering/comments/xr1bk7/databricks_vs_snowflake_user_experience/,False,False,False,False
r58r49,The Data Stack Show: A Podcast on Data Engineering,"Hey everyone, 

I'm one of the hosts of a podcast show, called the data stack show. We are focusing on data engineering related topics but we never say no to a guest who has something interesting to say around technology. We even had a guest where we chatted about communities.

We've been running the show for about a year now and never shared its existence with any community.

 I'd love to get feedback and suggestions from you. What questions you might have and what people you would like to listen to. 

Of course any constructive criticism is more than welcome!

&#x200B;

You can check the show here: [The Data Stack Show](https://datastackshow.com/)",99,19,cpardl,2021-11-29 23:29:43,https://www.reddit.com/r/dataengineering/comments/r58r49/the_data_stack_show_a_podcast_on_data_engineering/,False,False,False,False
qbyml5,Some Production SQL Database schemas for a few different businesses. (inc Airbnb and Twitter) as an example of some real world data models.,,99,14,tdatas,2021-10-20 11:26:02,https://drawsql.app/templates?page=1,False,False,False,False
1bfevg2,"Flat file with over 5,000 columns…","I recently received an export from a client’s previous vendor which contained 5,463 columns of Un-normalized data… I was also given a timeframe of less than a week to build tooling for and migrate this data. 

Does anyone have any tools they’ve used in the past to process this kind of thing? I mainly use Python, pandas, SQLite, Google sheets to extract and transform data (we don’t have infrastructure built yet for streamlined migrations). So far, I’ve removed empty columns and split it into two data frames in order to meet the limit of SQLite 2,000 column max. Still, the data is a mess… each record, it seems ,was flattened from several tables into a single row for each unique case. 

Sometimes this isn’t fun anymore lol",97,119,iambatmanman,2024-03-15 14:12:01,https://www.reddit.com/r/dataengineering/comments/1bfevg2/flat_file_with_over_5000_columns/,False,False,False,False
15umpgh,Where does the career go from Data Engineering?,"I've been a Data Engineer for some years now and wondering what kind of career possibilities there are from here onwards.

The way I am thinking myself is that I have two possible paths, (1) go towards a more architect role in a consulting company, meaning more of a technical sales role including tech selection, drawing archtecture blueprints and not participating so much in the actual coding/implementation. The other path (2) I see is to transition into a more business-focused inhouse product owner role including working closer with business to identify and suggest new use-cases for analytics and linking those investments and activities to actual business value.

What experiences does others have and would you rather go with option 1 or 2? Or are there other options I am not considering?",98,53,EzPzData,2023-08-18 14:52:16,https://www.reddit.com/r/dataengineering/comments/15umpgh/where_does_the_career_go_from_data_engineering/,False,False,False,False
14r4v10,I attempted to create the Ultimate Guide to dbt,"When I was first learning dbt I found a lot of really great resources (dbt docs, blog posts, slack threads etc.) and wanted to try to put that all together in one place for anyone else new to dbt, or anyone wanting to learn a bit more about it.   
The guide covers everything from ‘what is dbt?’ to advanced topics like model refactoring best practices.  
I hope it's useful! **And if you spot anything missing, or ways to make it better, please let me know!**  


Desktop link: [https://count.co/canvas/JpkaYdqr9oN](https://count.co/canvas/JpkaYdqr9oN)  
Mobile link: [https://taylor-count.medium.com/the-ultimate-guide-to-dbt-bad192ab4914](https://taylor-count.medium.com/the-ultimate-guide-to-dbt-bad192ab4914)  


Full disclosure: I do work for [count.co](https://count.co), the canvas in which the guide was built. ",98,7,tbrownlow,2023-07-05 08:50:09,https://www.reddit.com/r/dataengineering/comments/14r4v10/i_attempted_to_create_the_ultimate_guide_to_dbt/,False,False,False,False
14hy0vb,What did data engineering teach you about companies and businesses that most people don’t know?,What’s the “insider knowledge” that you now have?,97,77,SeriouslySally36,2023-06-24 17:05:16,https://www.reddit.com/r/dataengineering/comments/14hy0vb/what_did_data_engineering_teach_you_about/,False,False,False,False
11c06oj,Any examples of DE projects that you feel are the gold standard for how DE projects should be organized?,"I know this can vary significantly case by case, but mainly thinking in terms of the best structured DE projects that you’ve seen on GitHub.",101,20,wild_bill34,2023-02-26 00:14:23,https://www.reddit.com/r/dataengineering/comments/11c06oj/any_examples_of_de_projects_that_you_feel_are_the/,False,False,False,False
1014khn,Why does everyone on this sub hate to use Azure?,Are there any major pitfalls in azure that turns you off..i would like to hear why Azure is not that cool compared to AWS or gcp,94,123,johnyjohnyespappa,2023-01-02 05:38:16,https://www.reddit.com/r/dataengineering/comments/1014khn/why_does_everyone_on_this_sub_hate_to_use_azure/,False,False,False,False
z31651,How are you incrementally testing your data pipelines as you develop them?,,96,8,tchungry,2022-11-23 21:30:46,https://v.redd.it/qqe4f9hwhr1a1,False,False,False,False
uim1tl,How to not be exhausted after work?,"This post really isn’t a DE specific discussion. But, I’m wondering if anyone has developed strategies to not feel mentally exhausted after work. Usually after I log off, my attention span is shot and I don’t have the energy to do much except watch TV.

I noticed that not touching my phone all day helps, as well as taking periodic breaks. It’s easier said than done though

Edit: wow, so much great advice! I WFH and blocked out two 15 minute time blocks to lay down and close my eyes (not look at my phone). I also exercised this morning. It’s the end of the work day and I feel great! Thank you all!",101,80,None,2022-05-05 01:33:06,https://www.reddit.com/r/dataengineering/comments/uim1tl/how_to_not_be_exhausted_after_work/,False,False,False,False
lwb26q,What is your salary and where are you from?,"I’m from San Fran area, I get paid 77,000 base with about 11k in bonuses/benefits. 

Your seniority/years of experience would also provide further Insight

Would love to contrast with other data engineers to figure out a median/average salary.",98,260,The_Alpacas,2021-03-02 19:59:45,https://www.reddit.com/r/dataengineering/comments/lwb26q/what_is_your_salary_and_where_are_you_from/,False,False,False,False
163522r,Data teams right now,,97,56,audiologician,2023-08-27 23:02:12,https://i.redd.it/mtcerpyghqkb1.png,False,False,False,False
149g5zk,why is Apache Pyspark documentation so...sparse?,"Just curious, whenever I look for examples and syntax, Apache has these one-liners like ""this is what it is and don't ask anymore questions"" lol.

[https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.row\_number.html?highlight=row\_number#pyspark.sql.functions.row\_number](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.row_number.html?highlight=row_number#pyspark.sql.functions.row_number)

Compared to Pandas docs, for example, which are more descriptive and useful. Thoughts?",100,39,rotterdamn8,2023-06-14 19:06:16,https://www.reddit.com/r/dataengineering/comments/149g5zk/why_is_apache_pyspark_documentation_sosparse/,False,False,False,False
12db1ol,Dozer: The Future of Data APIs,"Hey r/dataengineering,

I'm Matteo, and, over the last few months, I have been working with my co-founder and other folks from Goldman Sachs, Netflix, Palantir, and DBS Bank to simplify building data APIs. I have personally faced this problem myself multiple times, but, the inspiration to create a company out of it really came from this [Netflix article](https://netflixtechblog.com/bulldozer-batch-data-moving-from-data-warehouse-to-online-key-value-stores-41bac13863f8).

You know the story: you have tons of data locked in your data platform and RDBMS and suddenly,  a PM asks to integrate this data with your customer-facing app. Obviously, all in real-time. And the pain begins! You have to set up infrastructure to move and process the data in real-time (Kafka, Spark, Flink), provision a solid caching/serving layer, build APIs on top and, only at the end of all this, you can start integrating data with your mobile or web app! As if all this is not enough, because you are now serving data to customers, you have to put in place all the monitoring and recovery tools, just in case something goes wrong.

There must be an easier way !!!!!

That is what drove us to build Dozer. Dozer is a simple open-source Data APIs backend that allows you to source data in real-time from databases, data warehouses, files, etc., process it using SQL, store all the results in a caching layer, and automatically provide gRPC and REST APIs. Everything with just a bunch of SQL and YAML files. 

In Dozer everything happens in real-time: we subscribe to CDC sources (i.e. Postgres CDC, Snowflake table streams, etc.), process all events using our Reactive SQL engine, and store the results in the cache. The advantage is that data in the serving layer is always pre-aggregated, and fresh, which helps us to guarantee constant low latency.

We are at a very early stage, but Dozer can already be downloaded from our [GitHub repo](https://github.com/getdozer/dozer). We have taken the decision to build it entirely in Rust, which gives us the ridiculous performance and the beauty of a self-contained binary.

We are now working on several features like cloud deployment, blue/green deployment of caches, data actions (aka real-time triggers in Typescript/Python), a nice UI, and many others.

Please try it out and let us know your feedback. We have set up a [samples-repository](https://github.com/getdozer/dozer-samples) for testing it out and a [Discord channel](https://discord.com/invite/3eWXBgJaEQ) in case you need help or would like to contribute ideas!

Thanks  
Matteo",94,44,matteopelati76,2023-04-06 06:12:06,https://www.reddit.com/r/dataengineering/comments/12db1ol/dozer_the_future_of_data_apis/,False,False,False,False
11izyfy,The SQL Murder Mystery,,95,6,theporterhaus,2023-03-05 15:38:09,https://mystery.knightlab.com/,False,False,False,False
10wd2e0,What is the hype around duckDB that I don’t seem to understand?,"When we have moved to data lakes and data lake house based architecture, why should I care about an OLAP DB? At this point in the data eco system?

I seem to have missed the memo on this one lol",101,106,money_noob_007,2023-02-07 21:03:03,https://www.reddit.com/r/dataengineering/comments/10wd2e0/what_is_the_hype_around_duckdb_that_i_dont_seem/,False,False,False,False
10awbwj,What are the things that you want to know but it’s too late now to ask…?,"I will go first, how debugging works in IDE and how to use it? 
I’ve been coding for awhile but never debug anything or never thought of doing it…

Edit: what I was trying to mean is that I don’t get why it’s used and I didn’t mean that I don’t understand it…",95,47,Tumbleweed-Afraid,2023-01-13 14:48:43,https://www.reddit.com/r/dataengineering/comments/10awbwj/what_are_the_things_that_you_want_to_know_but_its/,False,False,False,False
ucacvc,Do you think this AWS based personal project would be suitable and complex enough for a resume?,"Hi. I'm looking to add a 2nd personal project to my resume.

I recently created one using Airflow, Docker, dbt, S3, Redshift, and PowerBI. It's not perfect, and totally overkill with regards to the tools I used, but you can find it here: [https://github.com/ABZ-Aaron/Reddit-API-Pipeline](https://github.com/ABZ-Aaron/Reddit-API-Pipeline)

For the next one, I thought I'd take someone else's advice and follow a process like below:

1. Lambda function to extract API data and load into S3 (daily)
2. Lambda function to transform and copy S3 data into Redshift or use Athena to query data without loading into warehouse (daily). 
3. Visualise data with QuickSight or setup an EC2 Instance with Metabase so I can link to the dashboard from my resume (QuickSight only has a 30 day free trial).
4. Use Step Functions or CloudWatch to orchestrate the two Lambda functions
5. Use a bash script or CloudFormation to setup and tear down the infrastructure

Does this make sense? 

I've chosen AWS mainly because I recently passed my Cloud Practitioner Cert and it would be good to further develop my practical knowledge here.

Athena looks interesting, but not sure if running Athena daily to update a dashboard with new data is really its use case. Seem like it's more for occasional runs to analyse log files and that sort of thing, so maybe loading into Redshift and linking it to a BI tool is the smarter decision.

I've looked into services like AWS Glue and EMR. Don't know if anyone on here has experience with these. EMR caught my eye as it might help me develop some Spark knowledge.

All in all I'd like a project that'll help me develop some core data engineering skills and hopefully get my resume past the first stage (I'm still looking for a junior/enter level DE position).",98,40,None,2022-04-26 11:29:34,https://www.reddit.com/r/dataengineering/comments/ucacvc/do_you_think_this_aws_based_personal_project/,False,False,False,False
rnmumx,Kimball vs. Inmon vs. Vault,"This post does a good job explaining the nuances of the three methodologies

https://link.medium.com/Arq8VTkzfmb

TLDR:

Inmon: a stable warehousing strategy where data consistency is the highest priority. All user-facing data marts are built on top of a robust and normalized data warehouse.

Kimball: a dynamic warehouse strategy where quick development of useful data structures is the highest priority. All user-facing data are built on top of a star schema which is housed in a dimensional data warehouse.

Data Vault: a fast and asynchronous warehousing strategy where speed of both development and run time is the highest priority. All user-facing tables are built directly from untransformed source data.

I would love to hear your experience with using any of the three.",96,51,dawarravi,2021-12-24 13:57:23,https://www.reddit.com/r/dataengineering/comments/rnmumx/kimball_vs_inmon_vs_vault/,False,False,False,False
pru6r1,"Data Engineering course - UC Berkeley, Spring 2021",,97,12,tfyz,2021-09-20 12:49:16,https://cal-data-eng.github.io/,False,False,False,False
nolaf8,How to gain practical experience in Data engineering ?,I am fairly proficient in Python and know a fair bit of SQL. How do I go on from here. How do I gain some practical experience that I can showcase in my resume?,98,34,vivek9191,2021-05-30 21:39:55,https://www.reddit.com/r/dataengineering/comments/nolaf8/how_to_gain_practical_experience_in_data/,False,False,False,False
m7v2tf,Azure Data Factory sucks,"A simple two-step process to hit a REST API, extract the JSON payload, and land it into a data lake takes like 3 hours of meticulous debugging through the illegible, buggy, half-baked mess of a GUI. I swear I need to do a special chant and sacrifice my pet cat to have any hope of getting it to work. Why the hell do businesses use this crap?",98,65,None,2021-03-18 16:46:59,https://www.reddit.com/r/dataengineering/comments/m7v2tf/azure_data_factory_sucks/,False,False,False,False
1905tj2,Which tools are worth learning for an aspiring data engineer?,"I'm currently working as a data analyst, but in the near future I'd like to move into a more technical role and delve into data engineering. 

At the moment, I have quite good knowledge of Python and SQL as I work with these languages in my day-to-day job. I also have basic knowledge of general database concepts like normalisation/views/stored procedures etc.

But even looking at the entry-level job descriptions, I feel that I'm still a long way from getting a data engineering job, because the amount of tools required is insane.

And my question is, which of these tools are actually worth my time to learn? For example, are SQL Server tools like SSIS/SSAS important for data engineering? Or is it better to learn some cloud computing concepts? Or maybe something else?",96,42,Mokebe13,2024-01-06 18:01:08,https://www.reddit.com/r/dataengineering/comments/1905tj2/which_tools_are_worth_learning_for_an_aspiring/,False,False,False,False
17qlnmi,What are the skills of an advance Data Engineer?,"What would u consider are the skills of an advance data engineer? This could be a technical or non technical skills.

For me it would be
- CI/CD on data pipelines
- Implementing tests
- Data Quality Checks
- Writing maintable SQL

Share your opinions",95,31,Kindly-Screen-2557,2023-11-08 13:29:03,https://www.reddit.com/r/dataengineering/comments/17qlnmi/what_are_the_skills_of_an_advance_data_engineer/,False,False,False,False
17m0ioz,LeetCode for Data Engineers?,"I've been thinking about it for quite a while now.   


What is the alternate for Data Engineers when it comes to upskilling and showcasing their skills.   
Like, Developers usually have coding questions like Leetcode, Codeforces etc. 

What do the DEs have to practice or work on?   


I've seen few companies ask LC questions as well in interviews for DE, Analyst etc and these companies are legit Fortune 500 ones. ",96,48,Key_Consideration385,2023-11-02 10:35:06,https://www.reddit.com/r/dataengineering/comments/17m0ioz/leetcode_for_data_engineers/,False,False,False,False
164jb05,Catching cheaters in CS:GO using data,"TLDR, no you can't.   


Sorry for the baiting title, but my colleague wrote an article about extracting data from Faceit API and looking at the data to see if you can get any insights into who's cheating based on the data from Faceit. 

Thought some people might find it interesting. 

[https://medium.com/@twkuhn/analysing-faceit-cs-go-data-14847a3cd2a9](https://medium.com/@twkuhn/analysing-faceit-cs-go-data-14847a3cd2a9) 

&#x200B;

If you're more interested in learning how Valve works to catch cheaters in game, you can check out these two videos:   
[https://www.youtube.com/watch?v=hI7V60r7Jco](https://www.youtube.com/watch?v=hI7V60r7Jco) \- Anti-Cheat for Multiplayer Games

[https://www.youtube.com/watch?v=kTiP0zKF9bc](https://www.youtube.com/watch?v=kTiP0zKF9bc) \- Robocalypse Now: Using Deep Learning to Combat Cheating in Counter-Strike: Global Offensive",99,29,CalleKeboola,2023-08-29 14:00:40,https://www.reddit.com/r/dataengineering/comments/164jb05/catching_cheaters_in_csgo_using_data/,False,False,False,False
14zpfq8,Do you backup your S3 data?,,96,12,Affectionate-Day-240,2023-07-14 19:19:43,https://i.redd.it/om6ea88jdzbb1.jpg,False,False,False,False
144u8fv,I have a final job interview in a few days and I’m scared.,That’s all.,97,111,Emosk8rboi42969,2023-06-09 03:41:15,https://www.reddit.com/r/dataengineering/comments/144u8fv/i_have_a_final_job_interview_in_a_few_days_and_im/,False,False,False,False
135mhee,I'm a one man data team (pretty much). How can I be successful and as future proof as possible?,"In my organization, 1/2 of my responsibilities include pulling data from our financial system via APIs and on-prem SQL Server databases with Python scripts, making transformations if needed, then loading it in Power BI to build table relationships and create dashboards. There is another team member who also helps with just the dashboarding in Power BI. The infrastructure is all me.

Me being on my own, there is no one to bounce ideas off of or for someone to check me if what I'm implementing makes sense but here is where I'm currently at:

* Implementing version control via GitLab to store the Python scripts.
* Going to start creating documentation and an overview of the architecture.
* I'd like to implement a database to store the data then have Power BI read from there. Currently, the financial web APIs are being connected directly to Power BI and gives me no vision on whether a pull is working or if some are failing. It's also making the refreshes take a lot longer with the more connections we make because it's pulling all the data every single time. Still need to test this theory. This would also make me want to implement Airflow for orchestration.
* We deploy the dashboards via the Power BI Service and the file is stored in SharePoint so all we have to do is replace the current file with an updated one and it'll pick up the changes and re-deploy on its own.

... and that's it. Not sure if I am missing anything. I'm trying to learn as much as I can on my own but not sure what I could be missing as I simply might not know. I'd like for someone to come into this role in the future and be able to pick up where I left off.

I'm not a data engineer by the way and these responsibilities fell on me but am grateful to have this opportunity.",97,61,digitalghost-dev,2023-05-02 14:16:43,https://www.reddit.com/r/dataengineering/comments/135mhee/im_a_one_man_data_team_pretty_much_how_can_i_be/,False,False,False,False
11rm2wy,What has been your career path?,"I know everyone is different but I’m interested to see what jobs most of the Data Engineers in this sub have stopped at along the way to the posit hey are in now.

Example: Help desk -> ? -> ? -> Data engineer(junior/senior/etc…)",96,143,AJohnM_IT,2023-03-15 04:31:13,https://www.reddit.com/r/dataengineering/comments/11rm2wy/what_has_been_your_career_path/,False,False,False,False
11q6ouz,"Ideal Data Stack - architecture , whats your view?",,97,68,de4all,2023-03-13 11:06:00,https://i.redd.it/q5mn1rnwnhna1.png,False,False,False,False
urbsr3,Why there are few data engineering blogs/bloggers out there?,I am wondering why there are few data engineering blogs? Unlike data science for example. (Ps: share your favorite blogs/bloggers),92,49,HighlyIllogicall,2022-05-17 01:57:39,https://www.reddit.com/r/dataengineering/comments/urbsr3/why_there_are_few_data_engineering_blogsbloggers/,False,False,False,False
ryg4e7,"a curated list of docker-compose files prepared for testing data engineering tools, databases, and open-source libraries.",,97,5,smbanaie,2022-01-07 19:50:09,https://github.com/irbigdata/data-dockerfiles,False,False,False,False
qsaxbi,How the fuck did I land this?,"So for context I am a senior in college and was a analytics intern at a Fortune 15 company in my junior year summer. Currently I am taking a graduate level databases class where I have learned a lot but not necessarily what one would consider to be data engineering. I interviewed for one of Microsoft’s consulting company for a data engineer and got the job at an insane 115k a year salary. The position is entry level and in new york and the first 5 weeks will just be training me before I take on my first project. Can anybody give me advice if I should just fake it till I make it? And point me to resources like textbooks and whatnot as to how one becomes a data engineer?

Edit: Thank you, truly for the amazing feedback, will look out into the sources all of you mentioned as well as be as active as I can in asking question!",96,49,Mauliklm10,2021-11-12 13:14:47,https://www.reddit.com/r/dataengineering/comments/qsaxbi/how_the_fuck_did_i_land_this/,False,False,False,False
n12a23,Have you been able to use your Data Engineering skills to successfully start your own business?,"If so, please share your story.",94,40,Carnegie118,2021-04-29 11:28:02,https://www.reddit.com/r/dataengineering/comments/n12a23/have_you_been_able_to_use_your_data_engineering/,False,False,False,False
mcwast,A short and clean example of how to create memory efficient data pipelines with basic Python generators,,99,17,sib_n,2021-03-25 11:43:53,https://realpython.com/introduction-to-python-generators/#creating-data-pipelines-with-generators,False,False,False,False
k3ygzc,What exactly does being proficient in SQL entail?,"Hello there. Pretty sure this is kind of a noob question, sorry about that! I recently transitioned to a role that requires me to be more hands on with data from a role that was more management oriented. Since then I've been beefing up my technical capabilities further - a few months of python, some ETL and SQL.

I keep reading on here about proficiency in SQL, and considering how that in itself comes with a lot of other DB related skills - I was wondering when I can decide that I am in fact, proficient? What exactly does proficiency in SQL entail?

Thanks v much! :)",96,40,None,2020-11-30 15:56:24,https://www.reddit.com/r/dataengineering/comments/k3ygzc/what_exactly_does_being_proficient_in_sql_entail/,False,False,False,False
1br2c2e,What would you learn if you were to start all  DE again ?,"What would you start learning with, what skills , technology , certifications and toolset would you learn ? Would really love your input as I am starting all over 

I am someone who has internship DA experience and AWS CCP and Tableau Experience 
 ",96,45,bhumik98,2024-03-29 23:06:04,https://www.reddit.com/r/dataengineering/comments/1br2c2e/what_would_you_learn_if_you_were_to_start_all_de/,False,False,False,False
1bpjpcj,Follow up to last post on Management shaming me for not knowing Data engineering,"Crux of last post: I am a data analyst with skillsets on descriptive stats and dashboards, Management is shaming me for not knowing Data engineering.

 **Latest Update: The customer insists on a synapse setup, So my  manager tried to sweet talk me to accept to do the work within a very  short deadline, while masking the fact from the customer that I dont  have any experience in this. I explicitly told the customer that I dont  have any hands on in Synapse, they were shocked. I gave an ultimatum to  my manager that I will build a PoC to try this out and will implement  the whole setup within 4 weeks, while a data engineer will be guiding me  for an hour/day.  If they want to get this done within the given  deadline ( 6 days) they have to bring in a Data engineer, I am not  management and I dont care whether they get billing or not. I told my  manager that if If they dont accept to my proposal, they can release me  from the project.** ",92,24,urbanguy22,2024-03-28 01:51:19,https://www.reddit.com/r/dataengineering/comments/1bpjpcj/follow_up_to_last_post_on_management_shaming_me/,False,False,False,False
1amq8hs,What is the hardest interview question you got asked?,Drop the hardest interview questions you had,96,96,soviet69er,2024-02-09 15:08:14,https://www.reddit.com/r/dataengineering/comments/1amq8hs/what_is_the_hardest_interview_question_you_got/,False,False,False,False
13lppxu,Any Data Engineering Podcasts?,"Want to ask if there are any data engineering podcasts to listen while working :) 

I am willing to listen more opinions and discussions about emerging trends on Data & AI as well ! 

Thanks <3",95,18,paolapardo,2023-05-19 08:50:38,https://www.reddit.com/r/dataengineering/comments/13lppxu/any_data_engineering_podcasts/,False,False,False,False
138l4yc,How Do You Bring Your DE Team Out of The Shadows?,"Asking this question to open up a general discussion about how do we bring the DE team under the spotlight. The work done by DE team is critical to support the dashboards/ ML Models but rarely do we see recognition for the DEs. 
What are some of the ways through which you are handling this at your org?",97,72,booyahtech,2023-05-05 13:04:49,https://www.reddit.com/r/dataengineering/comments/138l4yc/how_do_you_bring_your_de_team_out_of_the_shadows/,False,False,False,False
12gdiok,Anything else to read,,97,31,young-dumb-and_broke,2023-04-09 08:36:13,https://i.redd.it/p304kn2e3vsa1.png,False,False,False,False
11s8x0q,I'm not getting it...what's the point of DBT?,"I've got the below synopsis of dbt...

 

1. Modularity and Reusability: DBT allows for the creation of reusable and modular data models, which can be easily shared across different projects and teams. This makes it easier to maintain consistency across the organization and reduces the duplication of effort.
2. Version Control: DBT integrates with version control systems like Git, allowing data engineers to track changes to the data pipeline over time. This feature is crucial for maintaining data integrity and auditability.
3. Testing and Validation: DBT includes built-in testing and validation features, which enable data engineers to test data pipelines and ensure that the output is accurate and consistent with the input data.
4. Collaboration: DBT's collaborative features allow multiple team members to work on the same project simultaneously, which increases productivity and reduces the likelihood of errors.

&#x200B;

...but I still don't get what the point of it is. What am I missing?",92,108,mister_patience,2023-03-15 20:53:15,https://www.reddit.com/r/dataengineering/comments/11s8x0q/im_not_getting_itwhats_the_point_of_dbt/,False,False,False,False
zlvtte,"Recently laid off, please review my Resume and throw me some hard truths! Interested in Data Engineer/Science roles. Thanks! (~1 Year of FT experience)",,94,46,throw_me_away_2424,2022-12-14 16:46:15,https://i.redd.it/wlb5dn0k7w5a1.png,False,False,False,False
yq3nwa,Hot Take : QA is the most tedious part of Data Engineering,"I've noticed with most big Data Engineering projects that a whole multitude of things can go wrong in the development phase. Data types don't match, tables get renamed or disappear all together , new columns get added so the bad Data Engineers before you who used SELECT * instead of listing out every column explicitly inadvertently break your pipelines months later. And many many other things.

The QA processes for data work seem a bit more tedious than other types of development I've done before. Right now we are in the process of fine tubing our QA methods , I've made quite a few helpful views and stored procedures that print out key information on certain tables but it's still very much a manual process. I'm trying to use dynamic SQL with input parameters to automate as much of it as I can but in general I try to avoid dynamic SQL due to performance issues particularly with large data sets.

How does everyone else feel about the QA process ? Love it, hate it, equate it to the 7th circle of hell?",96,38,DrRedmondNYC,2022-11-09 00:37:10,https://www.reddit.com/r/dataengineering/comments/yq3nwa/hot_take_qa_is_the_most_tedious_part_of_data/,False,False,False,False
xxuodv,Do programmers like it when someone asks you to code on spot while they are watching?,"I had an interview today via zoom and out of the blue the interviewer asked me solve a coding problem suddenly on the spot while sharing my screen to him. I found it very uncomfortable. The question wasn't that hard but I couldn't focus because I was being watched and judged by someone. He wouldn't even let me Google until I said I need to Google! Anyways, I didn't solve it because I was pissed off already and didn't feel like. What do you coders think?",97,63,codejunkiemonkie,2022-10-07 09:37:53,https://www.reddit.com/r/dataengineering/comments/xxuodv/do_programmers_like_it_when_someone_asks_you_to/,False,False,False,False
k1tqbs,Thinking of starting a Youtube channel on Data Engineering. Ideas?,"I've been working for more than a year in a data engineering team, and have been learning a lot of stuff in this domain - primarily Hadoop, Spark, data pipelines, etc. Recently I've been thinking of starting a Youtube channel wherein I'll stream while making a project or while how to learn some technology or something like that.

Suggestions and ideas are welcome.",95,22,stym06,2020-11-27 03:46:24,https://www.reddit.com/r/dataengineering/comments/k1tqbs/thinking_of_starting_a_youtube_channel_on_data/,False,False,False,False
1cbx9ki,Mastering the Spark UI,"I’m a specialist solutions architect at Databricks focusing on optimization, and I just published a [guide](https://docs.databricks.com/en/optimizations/spark-ui-guide/index.html) on how to use the Spark UI. It’s published as part of the official Databricks documentation.  I felt what was out there wasn’t that approachable, so hopefully this helps some. It doesn’t assume you know anything (or at least that’s the intent), and it takes you through to a diagnosis. It is written with Databricks in mind, but it should be helpful for any distribution of Spark. Let me know what you think!",94,10,peterst28,2024-04-24 12:46:13,https://www.reddit.com/r/dataengineering/comments/1cbx9ki/mastering_the_spark_ui/,False,False,False,False
1bvw19c,Polars — Accelerating Polars DataFrames. Polars and NVIDIA engineers are collaborating to bring GPU acceleration to Polars DataFrames in the near future.,,94,32,Fit_Yesterday_5955,2024-04-04 19:18:26,https://pola.rs/posts/polars-on-gpu/,False,False,False,False
1at7mdg,Update to interview posts,"After careful consideration and listening to your feedback, we've decided to no longer allow interview-related posts because they take away focus from our community's main purpose.

In the past, although they usually weren't directly related to data engineering we've allowed interview posts like ""What are interviews like at XYZ company?"" or ""What should I prepare/study for XYZ position?""

These questions are more often than not either too difficult to meaningfully answer or have already been answered many times. Similarly to resume reviews, we will no longer be allowing these types of posts and instead point users to other resources that are better suited and focused on answering those questions like Glassdoor and Blind.

Thank you again to everyone who has been providing constructive feedback on this topic. We know it may feel frustrating to see the same type of content and it may not feel like progress is happening but it just takes time to carefully review these changes and hear all opinions. We appreciate your patience and for helping shape this community.",92,13,AutoModerator,2024-02-17 17:44:48,https://www.reddit.com/r/dataengineering/comments/1at7mdg/update_to_interview_posts/,False,False,False,False
16p1v2j,Anyone migrating away from Snowflake and back to AWS?,When I migrated to Snowflake my costs went up like 5x. Maybe more.,94,125,Fitbot5000,2023-09-22 05:21:39,https://www.reddit.com/r/dataengineering/comments/16p1v2j/anyone_migrating_away_from_snowflake_and_back_to/,False,False,False,False
149fhb7,1 year since I started data engineer and I found the job of my dreams while you guys were gone 😭,"Started a year ago with microsoft exams, started a minimum wage job doing DE and have been for 10 months and I've been offered an amazing job actually helping people and also exploring analytics/datascience and other stuff too. 

Complete DE freedom to engineer and explore and find ways to help people, bonus is its 1.5x my salary and offers senior level relatively quickly. 

I've always struggled and felt like an imposter but in such a short time I've come far and I can't wait to learn more. 

I suck at doing off job projects, I prefer having  data shoved in my face and told to fix or do something with it!

Had a rough year but this has worked out amazingly and I'm thankful for everyone's support!

(Sorry if it's slight brag)",95,16,anonymous6156,2023-06-14 18:39:01,https://www.reddit.com/r/dataengineering/comments/149fhb7/1_year_since_i_started_data_engineer_and_i_found/,False,False,False,False
138vov9,Is it me or is the AWS Glue documentation kind of bad,"I'm going through the AWS Glue scripts of my company and I see the module ""awsglue"". What's inside it? I go in circles in their documentation and I search for it and I cannot find any details on it. I google ""awsglue documentation aws glue"" and find nothing. It's only when I look at the pip page do I see a link to the Github which lists the classes in it. 

&#x200B;

I see a ""DynamicFrame"" class in the Github page, so I go to the API reference in the documentation. Its  table of contents is massive and impossible to scroll through. I Ctrl-F ""DynamicFrame"". It doesn't exist. I search ""DynamicFrame"" in the search bar, and the documentation is put in some separate area called ""PySpark Extensions"" Why is this not in a central location?

&#x200B;

So now I want to find details on the ""Job"" class because its in the script im studying. Where do I go? There's no central documentation. I have no idea which section of the website to click to. I search ""Job"" in the search bar, and ofc, I can't find any details because the word ""Job"" is in EVERY SINGLE PAGE

&#x200B;

Like, i've been having issues finding stuff on documentation my whole life so part of this is probably my fault but still ahhhh im frustrated",92,39,DoubleDual63,2023-05-05 17:35:14,https://www.reddit.com/r/dataengineering/comments/138vov9/is_it_me_or_is_the_aws_glue_documentation_kind_of/,False,False,False,False
zc6lvq,What are some of the big problems Data Engineering currently faces / will face in the next 5-10 years?,"Basically title - I've been thinking about this a lot lately and am curious to hear others' thoughts.

So far my main thought patterns have been along the lines of:

1) Ways of working. In organisations I've been exposed to the ways of working for data teams have been all over the shop. Some are trending towards working more like SWE teams, some act as project teams, and some (most common IME) seem to exist on their own island inside an organisation. Does this create a risk that teams will by and large be working non-optimally?

2) Businesses not knowing what they want out of their data. This I feel is pretty self-explanatory - if the business doesn't know what they want is there risk of decision-makers opting not to utilise the data, making data work redundant?

3) Technology crossover and ambiguity. There is seemingly an unlimited number of technologies that can be utilized for various functions, with no clear use cases for which is best in what situations. Also, as technologies have expanded they have sought to expand functionality into other use cases, which further increases the permutations possible. Per examplur, for data orchestration there is ADF, Step Functions, Airflow, DBT, Flyte, Cloud Functions, Luigi, etc., etc. How can a business case be made for any of them without thorough testing, which is expensive and time-consuming?",94,47,elevatebi,2022-12-04 10:45:17,https://www.reddit.com/r/dataengineering/comments/zc6lvq/what_are_some_of_the_big_problems_data/,False,False,False,False
wgefn6,"Thoughts on the resume? Been looking for 6 months & no offers I'm happy with, yet.",,95,91,DaddyDock,2022-08-04 22:38:34,https://i.redd.it/1sbo98eoxrf91.png,False,False,False,False
w7qzs6,"""The best"" Data Engineering BootCamp","Hi all, looking to transition from automation engineering into data engineering and was hopping to borrow wisdom selecting ""the best"" data engineering bootcamp.

There is a lot info online and frankly it feels a bit overwhelming to select the top tools that are being used or the ""right"" skillset, therefore was hoping that a bootcamp will help me out to direct/guide me through  the process of transition.

At the moment following this roudmap:

[https://c-nemri.medium.com/your-2022-data-engineering-roadmap-3bfe6691ec50](https://c-nemri.medium.com/your-2022-data-engineering-roadmap-3bfe6691ec50)

[https://dataengineering.wiki/FAQ/What+skills+do+I+need+to+become+a+Data+Engineer](https://dataengineering.wiki/FAQ/What+skills+do+I+need+to+become+a+Data+Engineer)

&#x200B;

Apologize if this topic has been covered and thank you all for taking your time,",93,59,liutmantas,2022-07-25 14:54:27,https://www.reddit.com/r/dataengineering/comments/w7qzs6/the_best_data_engineering_bootcamp/,False,False,False,False
po7imu,Is Python + SQL + Spark enough to enter DE role?,"I am coming from database role and wondering what skills I need to learn, thank you in advance",98,47,stani76,2021-09-14 17:34:09,https://www.reddit.com/r/dataengineering/comments/po7imu/is_python_sql_spark_enough_to_enter_de_role/,False,False,False,False
p2i0po,Loading 261GB of reddit comments into Snowflake,,92,47,fhoffa,2021-08-11 17:40:10,https://hoffa.medium.com/loading-reddit-comments-into-snowflake-44b9f2072a84,False,False,False,False
1bwggc7,8 Billion Files,"How would you process 8 billion files sitting in an S3 bucket? Each file is about 300 bytes. They're txt files that will need a few small transformations.

I need to:

1. Perform some minor transformations on values
2. Load into a permanent queryable datastore

Each file holds a single check-in update for an IoT device and holds about 20 key value pairs that look like this:

    KeyA: ValA
    KeyB: ValB
    KeyC: ValC

Each file will eventually represent a row in my final, single ""table"" (probably a groups of parquets in S3 with spectrum as the query engine)

**What's a good way to iterate in manageable chunks?** ",93,94,__mifflinPaper,2024-04-05 12:11:50,https://www.reddit.com/r/dataengineering/comments/1bwggc7/8_billion_files/,False,False,False,False
18rjrrf,"Are data engineers, architects and programmers in general getting older?","Maybe it's the recent lay offs and hiring freeze for graduates but I've had two different jobs at 2 different companies in the last 2 years were I was easily the only person under 40 in the team.

Has anyone else noticed this?",92,65,PureLavishness8654,2023-12-26 22:12:53,https://www.reddit.com/r/dataengineering/comments/18rjrrf/are_data_engineers_architects_and_programmers_in/,False,False,False,False
170otm0,Microsoft Fabric: Should Databricks be Worried?,,95,92,include_stdio_h,2023-10-05 18:06:29,https://www.vantage.sh/blog/databricks-vs-microsoft-fabric-pricing-analysis,False,False,False,False
10iq2pk,Layoffs for Data Engineers,"Its been a blood bath of layoffs for tech companies as of late and it clear we are headed towards a recession. 

I'm curious to hear various thoughts and the subreddits observations as it pertains to Data Engineers / Data Engineering. Have you been affected by the recent layoffs? Have you observed DE teams being affected? My first instinct is that Data Engineers are such an important role to any company that they are largely insulated from these types of events but then again, you honestly never know. 

Please share your experiences",89,77,BJJaddicy,2023-01-22 18:13:31,https://www.reddit.com/r/dataengineering/comments/10iq2pk/layoffs_for_data_engineers/,False,False,False,False
y2r6ml,"Celebrating my first Data Engineering Project -- Fitbit data with PySpark, GCP, prefect, and terraform!","Hello!

I've been trying to learn about data engineering concepts recently through the help of this subreddit and the data engineering Zoom-Camp. I'm really happy to say I finished putting together my first functioning DE project (really my first project ever :) ) and wanted to share to celebrate/ get feedback!

[Fit-pipe DE Project](https://github.com/rickyriled/data_engineering_project_1)

The goal of this project was to just get the various technologies I was learning about interconnected, and to pull in/transform some simple data that I found interesting with them -- specifically, my fit-bit heart rate data!

In short, terraform was used to build a data lake in GCS, and then I scheduled regular batch jobs through a prefect DAG to pull in my fitbit data, transform it with PySpark, and then push the updated data to the cloud. From there I just made a really simple visualization to test if things were working on google data studios.

https://preview.redd.it/24wnijf2mit91.png?width=1566&format=png&auto=webp&s=b9b48e19ac442b5070c0fe07e1dd4fb17b20f3d1

Ultimately there were a few things I left out due to issues with my local environment/ a lack of computing power; e.g. airflow running in docker was too computationally heavy for my MacBook air, so I switched to prefect; and various python dependency issues held me back from connecting to big query and developing a data warehouse to pull from.

In the future, I wan't to try and more appropriately use PySpark for data transforming, as I ultimately used very little of what the tool has to offer. Additionally, though I didn't use it, the various difficulties I had setting up my environment taught me the value of docker containers.

&#x200B;

&#x200B;

I wanted to give a shout out to some of the repos that I found help in/ drew inspiration from too:

[MarcosMJD Global Historical Climatology Pipeline](https://github.com/MarcosMJD/ghcn-d)

[ris-tlp adiophile-e2e-pipeline](https://github.com/ris-tlp/audiophile-e2e-pipeline)

[Data Engineering Zoom Camp](https://github.com/DataTalksClub/data-engineering-zoomcamp)

&#x200B;

Cheers!",95,15,Particular-Bet-1828,2022-10-13 06:16:27,https://www.reddit.com/r/dataengineering/comments/y2r6ml/celebrating_my_first_data_engineering_project/,False,False,False,False
svx88j,Data Engineer Salaries - Did they suddenly jump up?,"I'm starting to wonder if I'm under payed. I truly don't know so wanted to get everyone's opinion.

First of all: our team lost alot of people in 2021. When we attempted to hire replacements we had a very very difficult time. Candidates were asking for the same salary that our senior director was making, even with only a few years of experience. 

True senior level engineers were almost nowhere to be found.

I also was contacted by a former manager and was told he just got a 40% pay increase moving to a new company.

I did some googling just now on average salary for senior data engineer and found the following:

1. Glassdoor - 125k
2. Builtin - 131.5k + 14.5k bonus
3. Zip recruiter - 134k
 
These numbers appear to be about what id expect for a senior data engineer working in an average cost of living area.

However, I wouldn't expect websites such as these to keep up with sudden rapid wage inflation for such a specific role.

So. What do you think? Is there crazy wage inflation for data engineers right now? Or do these websites have it about right?",93,111,idiotlog,2022-02-19 00:50:17,https://www.reddit.com/r/dataengineering/comments/svx88j/data_engineer_salaries_did_they_suddenly_jump_up/,False,False,False,False
m0mx2g,Ask Martin Kleppmann - the author of Designing Data-Intensive Applications,"Martin is answering our questions! We've already asked:

* Should a start-up immediately plan applications for data-intense use cases?
* Should we read the book if the cloud providers already implement everything?
* Are relational databases suitable for banking systems?

And other things.

Join us and ask Marting questions! More info: [https://datatalks.club/books/20210308-designing-data-intensive-applications.html](https://datatalks.club/books/20210308-designing-data-intensive-applications.html)",94,15,stolzen,2021-03-08 18:52:39,https://www.reddit.com/r/dataengineering/comments/m0mx2g/ask_martin_kleppmann_the_author_of_designing/,False,False,False,False
l9anov,Building a personal data warehouse in Snowflake for fun and no profit.,,91,10,thomasdziedzic0,2021-01-31 11:35:27,https://www.thomasdziedzic0.com/blog/building-a-personal-data-warehouse-in-snowflake-for-fun-and-no-profit,False,False,False,False
k0faxp,Introducing Amazon Managed Workflows for Apache Airflow,,93,23,None,2020-11-24 22:27:27,https://aws.amazon.com/blogs/aws/introducing-amazon-managed-workflows-for-apache-airflow-mwaa/,False,False,False,False
1cf6vob,I recorded a Python PySpark Big Data Course and uploaded it on YouTube,"Hello everyone, I uploaded a PySpark course to my YouTube channel. I tried to cover wide range of topics including SparkContext and SparkSession, Resilient Distributed Datasets (RDDs), DataFrame and Dataset APIs, Data Cleaning and Preprocessing, Exploratory Data Analysis, Data Transformation and Manipulation, Group By and Window ,User Defined Functions and Machine Learning with Spark MLlib. I am leaving the link to this post, have a great day!



[https://www.youtube.com/watch?v=jWZ9K1agm5Y&list=PLTsu3dft3CWiow7L7WrCd27ohlra\_5PGH&index=9&t=1s](https://www.youtube.com/watch?v=jWZ9K1agm5Y&list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&index=9&t=1s)",94,10,onurbaltaci,2024-04-28 13:51:21,https://www.reddit.com/r/dataengineering/comments/1cf6vob/i_recorded_a_python_pyspark_big_data_course_and/,False,False,False,False
1c2wm0o,Is it true that DEs don't get much recognition for their work as other roles do?,"Recently I got to know that -

```
Data engineering is not a core business
function. In many companies, especially
product-based companies, data engineers are seen as supporting the core business, rather than being part of it. This means that data engineers may not get as much recognition for their work as other roles, like product managers or data scientists.
```

Is it true?",92,83,Discharged_Pikachu,2024-04-13 07:51:13,https://www.reddit.com/r/dataengineering/comments/1c2wm0o/is_it_true_that_des_dont_get_much_recognition_for/,False,False,False,False
18tjgp8,Zendesk Moves from DynamoDB to MySQL and S3 to Save over 80% in Costs,,92,11,rgancarz,2023-12-29 09:43:54,https://www.infoq.com/news/2023/12/zendesk-dynamodb-mysql-s3-cost/,False,False,False,False
180lef1,How is Airflow really used in real practice?,"Hello everyone, I'm currently learning about orchestrating data pipeline with Airflow. With all the free resources I have seen, it seems that a majority of the demos are showing developers to save the python code in the `dags/` folder and then call PythonOperator to run the task. Note that the python code would not only include the Dags and tasks code, but they also have the actual ETL codes in there. To be honest, I am not usually comfortable in jamming codes for different purpose in one .py file because I prefer separating codes for ETL from codes that defines a DAG and tasks in other places.

So, some questions I have, maybe a stupid question, are:

In real practice, are we really saving the codes in the `dags` folder?  

Say that I have a function that does simple data cleansing with pandas. in real practice, should I just define a function in a .py and then also define the dags and tasks in the same python file? Would the `dags` folder be flooded? Is there any ways to just save the dags and tasks codes in the dags folder, but import the actual ETL codes from other .py files saved in another folder?

I feel the question may be leaning towards a 'no', but what I'm looking to learn is the \`why\` and \`how is that being done in real practice\`.

&#x200B;

Thanks!!!",92,65,sevkw,2023-11-21 16:47:37,https://www.reddit.com/r/dataengineering/comments/180lef1/how_is_airflow_really_used_in_real_practice/,False,False,False,False
1570ujm,What are some common data engineering mistakes you’ve seen in your career?,Related to any stage of the process. Or anything at all?,92,103,SeriouslySally36,2023-07-23 01:05:20,https://www.reddit.com/r/dataengineering/comments/1570ujm/what_are_some_common_data_engineering_mistakes/,False,False,False,False
14ufjuc,Why are there no junior or regular data engineer positions?,"Current data analyst and getting into engineering for the first time ever. Have never worked as a DBA or engineer in any capacity, but extensive experience working with data, SQL, data sets, etc. Would like to learn within my current company how data engineering works and learn the ropes. But there is no regular data engineer position, no junior positions. The only ones I have ever seen are senior data engineer. Why is this?",91,82,None,2023-07-08 21:11:30,https://www.reddit.com/r/dataengineering/comments/14ufjuc/why_are_there_no_junior_or_regular_data_engineer/,False,False,False,False
14e7vaz,Polars cookbook (Jupyter),"I went through an old pandas cookbook and repurposed it to use polars instead. I found it pretty useful when I was learning pandas years ago, so I hope that someone learning polars might find this useful as well.

[https://github.com/escobar-west/polars-cookbook](https://github.com/escobar-west/polars-cookbook)",92,20,None,2023-06-20 10:53:05,https://www.reddit.com/r/dataengineering/comments/14e7vaz/polars_cookbook_jupyter/,False,False,False,False
13d8rlr,What is that one skill you would wanna master at work?,"Just got my first job as a DE, am super pumped!! I am trying to learn as much as I can before I start the job. Wanting to start this discussion:

If there are only one/two most important skills you could master at DE work, what would they be?",92,88,Fasthandman,2023-05-09 22:47:42,https://www.reddit.com/r/dataengineering/comments/13d8rlr/what_is_that_one_skill_you_would_wanna_master_at/,False,False,False,False
1191ia8,what does your company's current data landscape look like? Which tools and technologies did you go for and why?,"We are currently on Azure datafactory (orcestration)  + Azure SQL database (ETL done using procedures + presentation layer). We tested databricks and liked the functionality so are utilizing that for newer ETL development. The company has decided to go to AWS so now we are exploring options there. 

So my question to you would be which orcestration tools, databases/data warehouses, CICD tools are you using and why?",92,65,fancyshamancy,2023-02-22 14:48:21,https://www.reddit.com/r/dataengineering/comments/1191ia8/what_does_your_companys_current_data_landscape/,False,False,False,False
10dr0r4,"For engineers who primarily work in a data warehouse environment mainly using SQL (and very little python), what are your career goals?","I'm an analytics engineer who is confused on what to learn next or where to take my career, so I am curious about like-kind professionals who are in the same boat. Have used python but 95% of my day is spent performing transformations using dbt in a cloud data warehouse environment.",89,92,Tender_Figs,2023-01-16 20:41:16,https://www.reddit.com/r/dataengineering/comments/10dr0r4/for_engineers_who_primarily_work_in_a_data/,False,False,False,False
zy8lrb,AWS,"Hi, I got a job as a data engineer. I am comfortable in SQL and python. But, no aws exposure. My upcoming team lead emphasised on learning cloud services. From where do I start? and what is the scope?",92,43,Tousif_11,2022-12-29 15:52:46,https://www.reddit.com/r/dataengineering/comments/zy8lrb/aws/,False,False,False,False
up583y,Imposter Syndrome,"I recently joined a company as a Data Engineer, and I constantly feel like I don't know enough weather it's fundamentals of Python, Data Structures, Cloud Computing Services, Linux, and most importantly hands on experience. I'm going through many courses and not retaining much. Is it normal to feel like you don't know enough or you will not be able to do the projects or tasks given to you? Any personal experience stories?",92,35,Gagan_Ku2905,2022-05-14 00:19:19,https://www.reddit.com/r/dataengineering/comments/up583y/imposter_syndrome/,False,False,False,False
q9o6aj,Offered DE Job... Feeling under-qualified,"A recruiter reach out to me over Linkedin a couple weeks ago for a Data Engineer position at a startup. I wen through the interview process which is pretty short, only 2 interviews and one short take home assessment. They tested some basics stuff like SQL and a bit of Pandas. I apparently did well on those and now they offered me the job. The problem is that I feel super under-qualified for job. 

I have been working as data analyst for 2 years but I have mainly used SQL and a small amount of python. I have no experience with things like Airflow, Spark and I am pretty clueless about data structures and algorithm. 

Do you guys think I should take this job? Why am I even offered this job when there are so many qualified data engineers out there?",92,61,sillythebunny,2021-10-17 00:35:31,https://www.reddit.com/r/dataengineering/comments/q9o6aj/offered_de_job_feeling_underqualified/,False,False,False,False
isl3a6,Diary of a Data Engineer,,96,8,ozzyboy,2020-09-14 13:43:10,https://lakefs.io/2020/09/14/diary-of-a-data-engineer/,False,False,False,False
1c0qaxv,A year into building a data project,"Last year, I set out to build a data project to start learning about data engineering. Since then, I’ve had some positive return on investment. 

By building my project, I was able to learn enough about building data pipelines to take on building pipelines at work. My current boss saw what I could do and asked me to build out a data infrastructure with the end goal of building Power BI dashboards.

Now, I’m solely responsible for managing several pipelines that read from different sources, applying transformations, and loading into a data warehouse for Power BI to read from. 30+ people now rely on me to keep the dashboards up to date. 

I’ve also took a further dive into Microsoft’s Power Platform with Power Apps and Power Automate. 

My personal project and the Power BI dashboards have caught the attention of the CIO and VP of Finance, respectively. I’m putting my name on the map! 

The CIO wants me to start taking AWS courses with a trainer and to learn more about our company’s IT infrastructure. He liked my passion and motivation. 

Funny thing is, I wasn’t originally hired for this type of work but is what I’ve been studying for and it just fell into my lap!

All this is to say that I’m proud of myself and am making a name for myself here. 
",92,27,digitalghost-dev,2024-04-10 16:50:11,https://www.reddit.com/r/dataengineering/comments/1c0qaxv/a_year_into_building_a_data_project/,False,False,False,False
18x2214,Optimizing a One Billion Row Challenge in with Rust and Python with Polars,"I posted this on /rust and I thought /dataengineering might find it interesting! 

I saw this [Blog Post](https://www.morling.dev/blog/one-billion-row-challenge/) on a Billion Row challenge for Java so naturally I tried implementing a solution in Rust using mainly polars.[Code/Gist here](https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8)

Running the code on my laptop, which is equipped with an i7-1185G7 @ 3.00GHz and 32GB of RAM, but it is limited to 16GB of RAM because I developed in a Dev Container.  Using Polars I was able to get a solution that only takes around 39 seconds.


|Implementation|Time|Code/Gist Link|
|:-|:-|:-|
|Rust + Polars|39s|[https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8](https://gist.github.com/Butch78/702944427d78da6727a277e1f54d65c8)|
|Rust STD Libray|19s|[Coriolinus Solution](https://github.com/coriolinus/1brc)|
|Python + Polars|61.41 sec|[https://github.com/Butch78/1BillionRowChallenge/blob/main/python\_1brc/main.py](https://github.com/Butch78/1BillionRowChallenge/blob/main/python_1brc/main.py)|
|Java [royvanrijn](https://github.com/gunnarmorling/1brc/blob/main/calculate_average_royvanrijn.sh)'s Solution | 23.366sec on the (8 core, 32 GB RAM) |[https://github.com/gunnarmorling/1brc/blob/main/calculate\_average\_royvanrijn.sh](https://github.com/gunnarmorling/1brc/blob/main/calculate_average_royvanrijn.sh)|

Thanks to @[coriolinus](https://www.reddit.com/user/coriolinus/) and his code, I was able to get a better implementation with the Rust STD library implementation.  Also thanks to @[ritchie46](https://www.reddit.com/user/ritchie46/) for the Polars recommendations and the great library!",91,29,matt78whoop,2024-01-02 22:13:42,https://www.reddit.com/r/dataengineering/comments/18x2214/optimizing_a_one_billion_row_challenge_in_with/,False,False,False,False
18hgf31,What are some things that have radically improved your data engineering skills?,"Other than ""creating data pipelines over, and over again"" or ""practicing"", what are some things that have radically improved your data engineering skills?

e.g.

* Making each discrete phase of a given pipeline dumber (i.e. simpler, easier to deliver, easier to replace)
* Starting with a dumb Python script almost every time, and making it more robust over time
* Just getting started, and making things pretty later
* Using a particular data pipeline orchestrator because <x>
* Analyzing data a particular way before getting started
* Creating documentation <y> before getting started
* Sketching data composition and lineage out with pen and paper
* Sketching out your data pipeline with pen and paper
* Rendering your data pipeline using GraphViz and DOT markup, etc. so you can better visualize the structure of your data pipelines

EDIT: if this is too vague, or a bad fit, I'll delete it, just let me know - AFAIK how you build things and the process by which you decide to build things is just as important as what tools you use to build those things - i.e., software architecture matters when developing software - but, I don't know - just, let me know.",90,68,Fun-Importance-1605,2023-12-13 13:07:42,https://www.reddit.com/r/dataengineering/comments/18hgf31/what_are_some_things_that_have_radically_improved/,False,False,False,False
15hhvj3,Can washing machines be used for parallel processing?,"Seriously, Spark is built on top of Scala, which runs on the JVM. JVM compiles code to byte code which is readable to anything that has a processor in it(?). Washing machines do have a processor of some sort.

So, can I theoretically use my washing machine to do some work besides, well, washing.",93,32,ultrachad420,2023-08-03 22:16:43,https://www.reddit.com/r/dataengineering/comments/15hhvj3/can_washing_machines_be_used_for_parallel/,False,False,False,False
13xldpd,Quarterly Salary Discussion - Jun 2023,"This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering. Please comment below and include the following:

1. Current title

2. Years of experience (YOE)

3. Location

4. Base salary & currency (dollars, euro, pesos, etc.)

5. Bonuses/Equity (optional)

6. Industry (optional)

7. Tech stack (optional)",90,230,AutoModerator,2023-06-01 16:00:29,https://www.reddit.com/r/dataengineering/comments/13xldpd/quarterly_salary_discussion_jun_2023/,False,False,False,False
13u5lly,Are SQL query optimizations skills important for an exceptional data engineer?,I am sure most people in DE do have sql knowledge. But how important is to know how to optimize an sql query? Is it something exceptional with very few people with those skills or those skills are as widespread and normal as other sql staff?,93,76,andrey1736,2023-05-28 17:09:37,https://www.reddit.com/r/dataengineering/comments/13u5lly/are_sql_query_optimizations_skills_important_for/,False,False,False,False
12knal1,Interviewer wants me to go into detail about current company's architecture,"As the title says. 

I'm based in the UK and interviewing for a well known company.

I've been provided an outline of the interview and in it they want me to prepare a diagram of my current company's data architecture and spend 10-20 mins explaining it.

I don't know if it's an odd request or not - I understand wanting to test my knowledge around architecture but it still feels odd.",92,67,iamanoob38,2023-04-13 12:23:49,https://www.reddit.com/r/dataengineering/comments/12knal1/interviewer_wants_me_to_go_into_detail_about/,False,False,False,False
119s7yv,Is dbt really necessary?,"dbt is getting pretty popular recently, but is it really that “necessary”? I mean what are the added benefit of introducing new tool when you can do all transformations using python (polars, duckDB…) + in python you can also do the “extract” step so basically you are able to cover entire ETL lifecycle with one tool? Also you can unit test your code better. As python disadvantage I see the dependency management. The only advantage of dbt I can see is you do not have to explicitly create tables as it creates it for you.",91,95,romanzdk,2023-02-23 08:02:08,https://www.reddit.com/r/dataengineering/comments/119s7yv/is_dbt_really_necessary/,False,False,False,False
10seqay,Using Polars over Pandas or PySpark,"I've been refactoring some code lately from using Pandas to Polars and I'm just blown away by the speed increases on locally run tasks (I'm running python 3.11.1 too which probably helps a wee bit as well).

I'm yet to employ it for work but it looks like the perfect library for AWS Lambdas or GCP Cloud Functions where billing is by the millisecond.

I've also noticed that Polars is much more similar to PySpark than Pandas with some of the naming conventions, lazy execution function-chaining etc, and can't help but think that's the benchmark or 'target' for lack of a better term.

The documentation is really coming together well and I'm just looking for stories or experiences of polars being used by DE's in industry.

I think it's incredible but of course, in our game adoption is everything.",89,50,None,2023-02-03 07:52:53,https://www.reddit.com/r/dataengineering/comments/10seqay/using_polars_over_pandas_or_pyspark/,False,False,False,False
yzmsyw,How do you keep yourself updated about latest DE news?,"Do you have websites you check daily like you'd read a newspaper? Do you have a strategy to keep yourself updated? I can't really find an efficient way to keep myself documented on new technologies, major bugs on tools we use daily etc.",91,23,barbapapalone,2022-11-19 21:36:41,https://www.reddit.com/r/dataengineering/comments/yzmsyw/how_do_you_keep_yourself_updated_about_latest_de/,False,False,False,False
y6n37d,"SQL Workshop recording: Making SQL more efficient, readable, and easier to debug","[Ergest Xheblati](https://www.linkedin.com/in/ergestxheblati/) is the author of [*Minimum Viable SQL Patterns*](https://ergestx.gumroad.com/l/sqlpatterns), and he's spent the last 15 years mastering SQL.  

In this [SQL workshop](https://youtu.be/UFiZx5NlzL4), Ergest teaches various SQL principles (patters) to help you take your SQL skills from intermediate to expert. More specifically, you'll learn about:   
🎯 Query composition patterns - How to make your complex queries shorter, more legible, and more performant   
🎯 Query maintainability patterns - Constructing CTEs that can be reused. In software engineering, it's called the DRY principle (don't repeat yourself)  
🎯 Query robustness patterns - Constructing queries that don't break when the underlying data changes in unpredictable ways  
🎯 Query performance patterns - Make your queries faster (and cheaper) regardless of specific database you’re using.

Toward the end of the workshop, Ergest answers over a dozen questions from SQL professionals all over the world.  

Watch the full workshop 👉 [**here**](https://www.youtube.com/watch?v=UFiZx5NlzL4) 👈. It's free, and don't worry, you're not being sold on a SaaS product 🤣  


If you enjoyed this video, please join Ergest and +1,000 like minded data professionals in our Slack Community, the [OA Club](https://www.operationalanalytics.club/)! We're creating content like this all the time!",89,11,JParkerRogers,2022-10-17 21:20:34,https://www.reddit.com/r/dataengineering/comments/y6n37d/sql_workshop_recording_making_sql_more_efficient/,False,False,False,False
tmdqlo,Data Lake vs Data Warehouse,,94,25,luminoumen,2022-03-24 14:28:22,https://luminousmen.com/post/data-lake-vs-data-warehouse,False,False,False,False
rja8s6,What is Kubernetes used for in data engineering?,Im curious as do why people use kubernetes in this field.,91,40,SeaworthinessFit7893,2021-12-18 15:54:03,https://www.reddit.com/r/dataengineering/comments/rja8s6/what_is_kubernetes_used_for_in_data_engineering/,False,False,False,False
laibru,Databricks raises $1B at $28B valuation as it reaches $425M ARR,,89,32,Bazencourt,2021-02-02 00:06:17,https://techcrunch.com/2021/02/01/databricks-raises-1b-at-28b-valuation-as-it-reaches-425m-arr/?guccounter=1,False,False,False,False
16cnpvq,"Accepted Data Eng offer, seems more like BI Engineer","My company is a large enterprise that sells automotive data. I recently accepted a role as DE and have sat around for 6 weeks without training or understanding what my role is.

I’m now learning that they want me to be the admin for their BI tool (Microstrategy) and that I will not be doing any work on any cloud platforms and I will just be doing reporting and sql. Zero Python, infrastructure, data modelling, or scripting.

Is this common? I feel like I’m being trained on a useless, outdated tool that won’t progress res my career whatsoever.",90,107,Dallaluce,2023-09-07 19:23:48,https://www.reddit.com/r/dataengineering/comments/16cnpvq/accepted_data_eng_offer_seems_more_like_bi/,False,False,False,False
12ctygq,Spark/databricks seems amazing?,"Looking at some foundational components for a new data platform. Looks like data lake + Spark is just all you need. Spark as a compute engine with its different APIs seems extremely powerful. 

What are the drawbacks? If self hosting I guess it's what comes with self hosting anything. If using a managed service like databricks I guess it's cost? 

Any insights?

EDIT: Looks like there's a DBT connector for Spark. Has anyone used the two in combination? I've used dbt with RS, BQ, and SF but not Spark.",91,97,alex_o_h,2023-04-05 18:47:33,https://www.reddit.com/r/dataengineering/comments/12ctygq/sparkdatabricks_seems_amazing/,False,False,False,False
x90yf4,How do i become a data engineer?,"Hi everyone!

I'm looking to get some advice on how best to go from my current situation (zero experience and skills in data engineering) to getting a junior position as a data engineer. I've been doing my best to understand the industry/job, and would like to take the leap as it seems that data engineering is 1. in high demand, 2. growing quickly, 3. sounds like a challenging and interesting job and 4. is better paid than almost every other computer science field.

I have a computer science degree and a modest amount of professional experience in front end web development and game development, but zero experience/skills in backend and data engineering. I'm not entirely hopeless with backend and data engineering as i learnt a bit during my degree, but i figure i should just start from scratch. So where to start? Are there any good Udemy or online courses i should do? Are there any certificates or extra qualifications i should get?

I'm currently working full time as web developer and have 2 children to feed, so internships and unpaid work is probably not possible for me. I think i can manage to dedicate 10-15 hours a week to purpose of getting skilled up for a data engineering role.

What would you guys do in my position?",90,53,BlackneyStudios,2022-09-08 13:46:19,https://www.reddit.com/r/dataengineering/comments/x90yf4/how_do_i_become_a_data_engineer/,False,False,False,False
skrkoj,What is difference between data warehouse and data lake,What is difference between data warehouse and data lake? Please elaborate with examples and in simple layman term.,86,62,rajneesh4u,2022-02-04 23:20:12,https://www.reddit.com/r/dataengineering/comments/skrkoj/what_is_difference_between_data_warehouse_and/,False,False,False,False
rymcs8,M1 macs are still riddled with compatibility issues,"I posted a while back asking about macs for data engineering, but neglected to specify that it is an m1 mac. After spending my first week troubleshooting tons of errors due to mismatching architecture in my packages, I would say to avoid m1 Macs for any local development unless you like spending weeks fiddling with every package",91,49,mistanervous,2022-01-08 00:19:42,https://www.reddit.com/r/dataengineering/comments/rymcs8/m1_macs_are_still_riddled_with_compatibility/,False,False,False,False
mob52r,Data Engineering Landscape in 2021 with Tobias Macey,,88,0,luminoumen,2021-04-10 19:19:55,https://youtu.be/D0Z6ZsNNeJs,False,False,False,False
17rpfn3,SQL versus Python?,"Though I am a data engineer by trade, I am often find myself as the in-house de facto data analyst.

My own experience has been that I often use about an equal mix of SQL and Python to work on data analysis requests. But it’s not set in stone. For quick requests, I usually go with SQL. For more complex ones, I will rely on Python and Jupyter notebooks.

I am the only one in this situation? Do you go for SQL, Python (R?) or some other language?

Just wondering. Thanks.",88,128,BatCommercial7523,2023-11-09 22:51:42,https://www.reddit.com/r/dataengineering/comments/17rpfn3/sql_versus_python/,False,False,False,False
15la6wi,Just got certified! - Databricks Certified Data Engineer Associate,"Just got certified! I am happy to say I have completed the Databricks Data Engineering Associate certification. 

The exam wasn't as difficult I expected it be, primarily revolving around the Databricks platform as expected. The exam focused on concepts like Delta, Multi-hop architecture, Repos etc. Some coding questions on very basic SQL syntax(CTAS, create views etc.) nothing too out of the ordinary. 

I'd suggest taking the certification, it's not a difficult exam nor does it take too much time(about 10 days of studying). 

The resources I used are:  
1. The Databricks Data Engineering course (Free): I used my customer account, anyone can sign up for it, there's even a 2 week trial. I'd suggest downloading the .dbc files and uploading them to the community edition workspace and playing around. That's what I did!

2. Udemy Courses: [https://www.udemy.com/course/databricks-certified-data-engineer-associate](https://www.udemy.com/course/databricks-certified-data-engineer-associate/) \- was just brilliant. The course isn't too long, the instructor condenses the information really well. Overall pretty good imo 

3. Practice Tests:

1.  [https://www.udemy.com/course/practice-exams-databricks-certified-data-engineer-associate](https://www.udemy.com/course/practice-exams-databricks-certified-data-engineer-associate/?expanded=1014944232) \- was good to identify weak areas and revisit them 
2. [https://www.udemy.com/course/databricks-certified-associate-data-engineer-practice-tests](https://www.udemy.com/course/databricks-certified-associate-data-engineer-practice-tests/?referralCode=102E37D6BA7C7B8B5532) \- really good practice tests, the questions largely resembled the actual exam - (*70% of the actual exam questions*). Only practice test needed (wasted a lot of money on other tests). 

4. YouTube Resources: 

1. Advanced Analytics: Really good to find videos on alot of concepts - imo he breaks down concepts really well, but doesn't do a deeper dive. [https://www.youtube.com/@AdvancingAnalytics](https://www.youtube.com/@AdvancingAnalytics/videos) 
2. Stephanie Rivera: Okay, this is actual gold in terms of knowledge. She uploads the paid skill-builder series from Databricks to YouTube (though I'm not sure how accurate this is; a buddy of mine works at a company that has access to these, and he says they're the same). This is extremely useful for gaining in-depth knowledge. [https://www.youtube.com/@stephanieamrivera](https://www.youtube.com/@stephanieamrivera/videos) ",89,41,Background_Debate_94,2023-08-08 06:51:07,https://www.reddit.com/r/dataengineering/comments/15la6wi/just_got_certified_databricks_certified_data/,False,False,False,False
12jt9hl,What I have learned by solving (almost) all the SQL problems in Leetcode,"Hello,

I have written my first blog post in medium about what I have learned solving leetcode SQL problems. Shared some tips for those who want to start solving SQL problems too.

Also added some resources in the post which might be beneficial for you.

Happy reading!

[https://medium.com/@iamrafiul/what-i-have-learned-by-solving-almost-all-the-sql-problems-in-leetcode-670b8a2cb32e](https://medium.com/@iamrafiul/what-i-have-learned-by-solving-almost-all-the-sql-problems-in-leetcode-670b8a2cb32e)",87,51,rsabbir,2023-04-12 17:33:18,https://www.reddit.com/r/dataengineering/comments/12jt9hl/what_i_have_learned_by_solving_almost_all_the_sql/,False,False,False,False
11t4j09,"Tracking the Fake Stars Market with Dagster, BigQuery and dbt",,89,13,MrMosBiggestFan,2023-03-16 19:59:19,https://dagster.io/blog/fake-stars,False,False,False,False
zixrj6,"Data engineers, how do you approach ambiguity in your work?","I get a lot of vague questions at work, for example ""create a dataset so that so-and-so team can do this-and-that"" with little to no context. It took me a while to navigate this kind of ambiguity, and I'm not sure whether I'm there yet, but there are a few things that I do

1. **Ask clarifying questions**. Many times stakeholders have no idea what they're asking for and how they want the result to look like. It's worth getting into conversations with them because there's a chance that you can jointly agree on an end result that might look nothing like what they initially imagined.
2. **Start small, scale later**. I have analysts requesting data going back 5 years, but I know from experience that more recent data (going back 12 months) is actually better for answering business questions. Our product has evolved so much over the last years that the data collected pre-COVID looks nothing like the data we collect today, so analysts should be able to work with recent data, which I can readily provide. After I have done that and the analysts have worked with it, we jointly evaluate whether it's still necessary to use historical data. And no sooner than that do I actually look at that data far back.
3. **EDIT: Look at what's been done before**. I find that many DEs are more eager to reinvent the wheel and build something from scratch, rather than deal legacy codes and systems. However, I've found that it helps to at least pause and consider existing tech before deciding whether a new solution is really required. Oftentimes I've found that legacy stuff isn't actually all that bad, and even if I can incorporate even just 10% of it in the new solution, therefore saving myself some time and effort, I consider that a win.

How about you, how do you navigate ambiguity in your work as a data engineer?",88,26,DataScienceIsScience,2022-12-11 16:07:45,https://www.reddit.com/r/dataengineering/comments/zixrj6/data_engineers_how_do_you_approach_ambiguity_in/,False,False,False,False
z0srs8,Why FAANG+,"Can anybody please explain to me why people are obsessed with working for FAANG+? These companies are notorious for being morally dubious, having shitty working environments and high burnout churn. Plenty of more attractive options out there. Just curious as I see a lot of posts on here and other subs with people asking for FAANG+ specific interview prep advice and it just seems odd. You wouldn't want to work for SPECTRE would you, very passé.",87,65,None,2022-11-21 07:34:27,https://www.reddit.com/r/dataengineering/comments/z0srs8/why_faang/,False,False,False,False
xv0lfs,"Been working as a Jr. DE, but worried I am at a dead end.","I was hired last year in October as a Junior Data Engineer ( they have since removed the Junior portion, without a pay raise). I got lucky with random applications to places that looked like I could meet the qualifications before graduating with a Bachelor's.

I am getting paid 75K, pretty much no oversight, and no one looking over my shoulder and harping me if I happen not to get much physically done in a day. Pretty chill job, and I work fast and get many different things done. And my direct superior really doesn't care if I feel like working at home on my 'Office' days.

The problem I am having that has been slowly eating away at me for the past two months is that there is too much freedom and insufficient structure.

* I am the only person that knows any programming languages, let alone python (which is what I am using)
   * I am not counting SQL; my direct superior does know SQL very well.
* There is **NO ONE** to check my work.  - 
   * I have written multiple scripts as ETL pipelines to the best of my ability to grab data that the company has access to but no way to incorporate into their reporting--And they work...
   * I have no superior in the job I was hired to do.
   * I have learned **LOADS,** and I feel like the speed at which I have learned python has been, but when I see things about good coding habits or good ways to go about a problem, I get a little upset that I am the only person at my job. 
* There is no version control in place in any sort of usable form.
   * There is a GitLab, but my direct superior isn't quick on the commits, and he doesn't use it.
   * I have been using git on my computer for the last two months to get the habit down, at least for my work.

I have so much leeway and so much slack at this job. I don't want to be taking it for granted, and I am working hard and trying to be the most useful I can be. But, the freedom is going to come back to bite me, I know it. I am going to flounder if I go somewhere else with actual structure.

My superior is a vocal advocate for me and constantly encourages me to learn more techniques, programs, and platforms, if not for this job, then for the next job. But, it is just him and me, and he can't help or mentor me on the job I was hired for.

I am keeping my eyes peeled for potential next jobs, but I feel like I am less confident in my ability to get hired than I was before this job. I have no idea how to gauge where I am professionally, and I am not sure what job title to look for next. 

I know if you were to ask my superior how I was doing, he would say I am doing great, but I feel like I'm crashing. I don't know if it is imposter syndrome, burnout, realizing I made it to the deep end without knowing it, or something else. 

It just feels like the more knowledge and skills I gain, the less I know what it is I am doing.

If I need to put this down to a question or two here is it.

* Am I overreacting about the no supervision and lack of anyone else to mentor me?
* (I haven't provided much to show what I do and what I have done; I can if needed)  What should I look for in a different job?
* I am sure there is another question wrapped up in all this, but it isn't shining out to me.



Edit-- 

Thanks a lot for all the suggestions and encouragements. Got even more to think about now, but at least it feels directional now.",90,51,TheParanoidPyro,2022-10-04 00:45:30,https://www.reddit.com/r/dataengineering/comments/xv0lfs/been_working_as_a_jr_de_but_worried_i_am_at_a/,False,False,False,False
w2zprg,What problems is Snowflake and co. trying to solve?,like title. What problems does it solve compared to a traditional Data Warehouse in Postgres or MS SQL? What is its Unique Selling Point and why is so popular?,90,62,None,2022-07-19 18:27:20,https://www.reddit.com/r/dataengineering/comments/w2zprg/what_problems_is_snowflake_and_co_trying_to_solve/,False,False,False,False
tzzxkj,Tell us your Position / industry / rates / salaries / location?,"I will start

Position: DE Manager
Industry: non-tech
Rate/Salary: $150k CDN / bonus and long term incentive
Location: Western Canada",90,145,dongdesk,2022-04-09 19:05:28,https://www.reddit.com/r/dataengineering/comments/tzzxkj/tell_us_your_position_industry_rates_salaries/,False,False,False,False
rofnm0,Is being a data engineer just a specialised software engineer?,Ive been thinking about how similar both jobs are and what not and how alot of data engineers had backrounds in designing websites. So am I right or wrong with this analogy.,89,34,SeaworthinessFit7893,2021-12-25 19:21:34,https://www.reddit.com/r/dataengineering/comments/rofnm0/is_being_a_data_engineer_just_a_specialised/,False,False,False,False
m8iqiq,My team's code gives me anxiety,"I recently joined a data strategy team within a finance department to help with their data infrastructures.

So far, they've been using Python scripts to load data from data sources to a relational database, which is fine but as the number of pipelines increase, a data orchestration tool like Airflow, Dagster etc would be useful. This is what I've mostly been building for them.

I've just recently looked at one of those scripts and I almost had a heart attack. No version control, he's got input variables and code commented out all over the place, he's got weird functions with inner functions in them (FOR NO DISCERNIBLE REASON) scattered within the whole script (which is like 500 lines long).

What the hell did I get myself into? Lol rant over.",88,49,anAnonymousWolverine,2021-03-19 14:42:20,https://www.reddit.com/r/dataengineering/comments/m8iqiq/my_teams_code_gives_me_anxiety/,False,False,False,False
hqtu8r,Data engineering project on Github,"Howdy all - just posting this in hopes of possibly getting some feedback on a DE project I'm working on. 

Some background: I currently work as a data engineer but almost entirely in a Microsoft shop. The goal of this project was to get some experience with open source software including Airflow and the AWS ecosystem.

From my understanding, it's also just beneficial in general to have some personal projects up on GitHub when searching for a job so figured I'd kill two birds with one stone :)

The project and explanation of it is here:  [https://github.com/ilya-galperin/SF-EvictionTracker](https://github.com/ilya-galperin/SF-EvictionTracker) 

Any and all feedback is more than welcome - this is my first time doing something like this so I'm sure it's not entirely perfect.

Thanks!",91,28,ilya-g-,2020-07-14 03:15:18,https://www.reddit.com/r/dataengineering/comments/hqtu8r/data_engineering_project_on_github/,False,False,False,False
1bqmq9c,Anyone else struggling to keep up?,"Honestly, I am tired of learning new things and have zero motivation to pick up new technologies. I don't hate my work, I like solving interesting problems, implementing solutions and even figuring out ways as and when required.

But I know that just being able to apply is not enough and that one has to constantly endeavour to keep on learning latest tech otherwise one is obsolete in market. How do you guys keep yourself motivated to constantly learn something new? ",89,76,TheNirvanaSeeker,2024-03-29 11:16:38,https://www.reddit.com/r/dataengineering/comments/1bqmq9c/anyone_else_struggling_to_keep_up/,False,False,False,False
1bjbdv3,I am planning to use Postgre as a data warehouse,"Hi, I have recently started working as a data analyst in a start-up company. We have a web-based application. Currently, we have only Google Analytics and Zoho CRM connected to our website. We are planning to add more connections to our website and we are going to need a data warehouse (I suppose). So, our data is very small due to our business model. We are never going to have hundreds of users. 1 month's worth of Zoho CRM data is around 100k rows. I think using bigquery or snowflake is an overkill for us. What should I do?",89,72,Dodomeki16,2024-03-20 11:40:29,https://www.reddit.com/r/dataengineering/comments/1bjbdv3/i_am_planning_to_use_postgre_as_a_data_warehouse/,False,False,False,False
17oe4z2,Discussion: Data Engineering has a title problem,"After working in this industry for 10+ years, I strongly feel like the data engineering space has a title problem. Data Science also has this issue, but I believe in Data Engineering it is even worse. Companies release vacancies for ""data engineer"" when the role can mean so many different things.

I feel like a new set of titles is required to make the industry more mature. Such as:

1. **Analytics Engineer (AE)**: Previously the data warehouse developer. Mainly works with SQL, Airflow, Python to transform data within the data warehouse. Person building analytics pipelines. A lot of DE work falls here.
2. **Data Platform Engineer (DPE)**: An engineer who works on the platform, but not the pipelines. Cross-over with cloud engineer and dev/ops.
3. **Data Streaming Engineer (DSE):** Specialized in data streaming, specifically; coding and patterns here are an entirely different ballgame from all of the above.

Too many times I see companies ask for ""Data Engineer"" - expecting an analytics engineer with enough experience to do what the DPE does. Or companies who mainly to DSE, and end up with too many AEs in the pipeline.

Some companies get this right by specifying that there is a ""focus"" area for a DE role. For example, Data Engineers with a heavy cloud focus, or Data Engineers focused on streaming (sometimes also called Big Data Engineer).

More specific DE roles would help both candidates and companies. Ideally the DE title should disappear completely and have several different roles falling under it.

&#x200B;

What is your opinion? What titles would you propose?

&#x200B;

&#x200B;",91,91,exact-approximate,2023-11-05 15:20:27,https://www.reddit.com/r/dataengineering/comments/17oe4z2/discussion_data_engineering_has_a_title_problem/,False,False,False,False
15mhunt,Is our dbt project as bad as I think?,"I just started a new job and am shocked at the state of the dbt project. I've no idea whether I am used to too high standards and I am overreacting. Would appreciate to hear some other stories of dbt at your company!

So why it is so bad, we're two analytics engineers and 1 data engineer. The data engineer mainly manages airflow and databricks so they hardly work with dbt. So it's basically two people. And we have the following:

- 600+ models

- no tests for most of the models

- lineage is a mess. One of the core tables has 55 parents and 150 children. (Edit: wrong wording I mean rejoining of upstream concepts) Circular references all over the place.

- everything in the mart is materialized as a table. They run those tables multiple times a day. Costs have been increasing at a steady pace ofcourse.

- they use schemas to denote topics. The project.yml contains configs for each folder to materialize it to the correct schema. The file is 200+ lines long as a consequence.

Btw they managed to get to this state in less than a year :p

Oh and they are migrating to a new bi tool with deadline end of October. Work hasn't even started on that. So should I run? :P

Edit: fixed formatting",91,117,snackeloni,2023-08-09 15:03:53,https://www.reddit.com/r/dataengineering/comments/15mhunt/is_our_dbt_project_as_bad_as_i_think/,False,False,False,False
14ws3o3,There is no Data Engineering roadmap,,88,38,itty-bitty-birdy-tb,2023-07-11 13:55:15,https://www.alasdairb.com/posts/there-is-no-data-engineering-roadmap/,False,False,False,False
13ahkh7,Is Redpanda going to replace Apache Kafka?,https://redpanda.com/blog/redpanda-vs-kafka-performance-benchmark,87,72,Born-Comment3359,2023-05-07 08:30:50,https://www.reddit.com/r/dataengineering/comments/13ahkh7/is_redpanda_going_to_replace_apache_kafka/,False,False,False,False
10svt5y,Why do people re-invent wrappers for Airflow?,"For every company I worked for (3 of them), the data team managed to pull out a certain yaml/jinja based ""solution"" to wrap up Airflow. So basically developers write yaml instead of Python. But why? Everytime I had to learn a new syntax, and not everytime the ""solution"" has all functionalities we want. The guy who made it had a lot of fun for sure, but everyone else is not having fun. Why can't they just let people write Python?

Sure the reason might be -- oh BI developers don't want to learn Python or don't have best practices. Well the first reason doesn't hold up because they do (and do write in another repo), and the second reason...well I assume your yaml based solution has the best practices then?

I'm not even going to complain how little documentation each one has. One company managed to invent a second suite of wrappers when I was there, Jesus...

Sincerely, I'd like to know why. I don't really see any benefit. I mean whatever your ""syntax"" can do, Airflow can do that too. I don't see man, I don't really see.",88,65,levelworm,2023-02-03 21:27:55,https://www.reddit.com/r/dataengineering/comments/10svt5y/why_do_people_reinvent_wrappers_for_airflow/,False,False,False,False
ydedn2,A Few Pointers for Boot Camp Grads and Uni Students From a Sr. Engineer,"So for the last 3 months, my time has been almost solely dedicated to interviewing, onboarding, and training new people at my work. Throughout the whole process, I’ve been able to identify a couple patterns that I think would be useful to work on before going into interviews or if you are looking to have better chances at getting hired at popular companies.

Needless to say, these are my opinions alone and should be taken with a grain of salt. I might be a Sr. engineer but I still have much to learn and my experiences might be not a true reflection of the market.

# Things To Consider:

* This is more specific for boot camp grads. An interviewer will see the same ""Airflow pipeline with <insert name of free API  here> data"" project a million times with the same structure, tests, and GitHub readme. A lot of bootcamps (and online courses) tend to steer their students to reuse projects from previous years over and over, which leaves you at a huge disadvantage. If you think this is you, take the time to work on other things.  

* I thought everyone knew this but obviously, some missed the memo. Data engineering is not exempt from TDD practices. If can't talk about testing during the interview, this is a massive red flag.  

* Unwillingness to learn SWE concepts/topics will hurt you in the long run. It will not make it easy to interview at companies where Data Engineers are treated as specialized software engineers, or where you are expected to work with other engineers (Backend, DevOps, Architecture, Networking, etc).   

* Focus on collaboration during technical interviews. Getting to the right answer is great, but a lot of companies will not consider you if you don't communicate with whoever is interviewing you. Being silent during the entire coding session makes it awkward and makes interviewers think you won't be a good fit in a collaborative team.   

* For the love of god, come prepared with questions before an interview. And something else besides ""what is <insert name of company>, what do you do?"".   

* If you are applying to a position but have no experience (internships, coops, freelancing, etc.) or just come straight out of an 18-week boot camp course. Please understand that you must put in 300% more effort in the interview and your portfolio than most candidates. I would love to hire folks like this, mentor them, and see them become great developers (and I have). Unfortunately, for 1 job application I publish, I get \~170 applicants within a couple days sometimes. It is hard to say no to experience when jobs in the DE space are so popular these days. Maybe get an internship first or find a small startup that is willing to give you a shot.   

* There are sooooo many companies out there misusing the term ""Data Engineering"" in job postings. Some are just glorified BI/DA positions with fancy titles or jobs where you don't code and end up becoming stuck in a career where you will have a hard time making moves to other engineering positions. In my opinion, data engineers shouldn't settle for such jobs. Ask questions to weed out these jobs during the interview process if you really want to become a proper DE (sorry if it sounds a bit harsh, I truly appreciate good BI developers and Data analysts)  

* Finally, weekend projects and open-source contributions are always cool to see. I support those who want to keep their personal time completely separate from doing anything related to development. But you know, it's cool to see folks passionate about learning or contributing to our community. 

These are just a couple things that jumped out at me during the past three months. I think paying attention to some of these points will make you a more well-rounded candidate and possibly give you better chances in the long run.",87,37,uncomfortablepanda,2022-10-25 20:08:35,https://www.reddit.com/r/dataengineering/comments/ydedn2/a_few_pointers_for_boot_camp_grads_and_uni/,False,False,False,False
tdnxzw,OOP in python ETL?,"Hello, I’ve recently started a new job as a data engineer (junior) coming from backend web development and I’m having a bit of a hard time adjusting to they way etl pipelines are coded.

So far, I’ve had to main airflow pipelines that call about 15-20 tables and aggregate/transform data from different DB’s (mssql and Snowflake). The code is very procedural, with some functions for reusability but that’s about it. 

I’m having a hard time finding good python ETL design examples on the internet that aren’t extremely simple.

My question is: do people usually try to leverage OOP architecture when coding pipelines? Or does it just make the scripts harder to understand? 

I’m trying to find ways to improve the code to make it more maintainable/easy to expand. Is there any example somewhere of advanced pipelines where I could learn some best practices? 

Thanks!",84,46,Alexisbou1,2022-03-14 03:22:52,https://www.reddit.com/r/dataengineering/comments/tdnxzw/oop_in_python_etl/,False,False,False,False
qf5dsx,How do you test your pipelines?,"Junior engineer here trying to learn how to properly test data pipelines. Currently my team just has unit tests for our Scala code, but nothing to test the full pipeline run (SNS trigger + EMR + S3 write).

What is the best way to end to end test full pipelines?

My team has around 70 different transformations with 400+ source files, so it doesn’t seem realistic to create mock data and test every transformation.

Should we just have 1 test with mock data that flows through the entire pipeline? Test every transformation that was changed in git commits? What patterns does your team follow?",89,23,Isvesgarad,2021-10-25 00:52:17,https://www.reddit.com/r/dataengineering/comments/qf5dsx/how_do_you_test_your_pipelines/,False,False,False,False
ock6qr,Curious to know what a typical day in the life of a DE at FB look like,"I have heard DE roles at FB/AMZN are more sql focused. 
Wanted to know any pros/cons about it. 

Apart from the pay, what all can an employee be happy about when they start working for fb? Will the skills learned at FB help if one wants to make a switch later? 
Given the boom in Spark/ Airflow, Aws and more, for big data, how does working at FB ensure us of keeping up with the latest tech?",88,49,psyched_3,2021-07-02 21:42:16,https://www.reddit.com/r/dataengineering/comments/ock6qr/curious_to_know_what_a_typical_day_in_the_life_of/,False,False,False,False
mg6ldf,How To Use Window Functions in SQL,,88,15,amcquistan,2021-03-30 03:12:41,https://thecodinginterface.com/blog/sql-window-functions/,False,False,False,False
1aofkv9,"[Updated] Personal End-End ETL data pipeline(GCP, SPARK, AIRFLOW, TERRAFORM, DOCKER, DL, D3.JS)","Github repo:[https://github.com/Zzdragon66/university-reddit-data-dashboard](https://github.com/Zzdragon66/university-reddit-data-dashboard).

Hey everyone, here's an update on the previous project. I would really appreciate any suggestions for improvement. Thank you!

## Features

1. The project is entirely hosted on the Google Cloud Platform
2. This project is ***horizontal scalable***. The scraping workload is evenly distributed across the computer engines(VM). Data manipulation is done through the Spark cluster(Google dataproc), where by increasing the worker node, the workload will be distributed across and finished more quickly.
3. The data transformation phase incorporates ***deep learning*** techniques to enhance analysis and insights.
4. For data visualization, the project utilizes D3.js to create graphical representations.

## Project Structure

&#x200B;

https://preview.redd.it/ew1cjp8870ic1.png?width=9426&format=png&auto=webp&s=502a9d668f0f7453f770cd9513ac33c041309e7a

## Data Dashboard Examples

## Example Local Dashboard(D3.js)

https://preview.redd.it/fdhivgm970ic1.png?width=4038&format=png&auto=webp&s=1bbac51ef3929b0b0ec1c7c21ea7b450bf0e6ed7

## Example Google Looker Studio Data Dashboard

[Looker Studio Data Dashboard](https://lookerstudio.google.com/s/hEfY-Q6G4Fo)

&#x200B;

https://preview.redd.it/m5imqa5b70ic1.png?width=2886&format=png&auto=webp&s=32ea902c25e4f16b55da2085912d7585c743c6c5

## Tools

1. Python
   1. PyTorch
   2. Google Cloud Client Library
   3. Huggingface
2. Spark(*Data manipulation*)
3. Apache Airflow(*Data orchestration*)
   1. Dynamic DAG generation
   2. Xcom
   3. Variables
   4. TaskGroup
4. Google Cloud Platform
   1. Computer Engine(*VM & Deep learning*)
   2. Dataproc (*Spark*)
   3. Bigquery (*SQL*)
   4. Cloud Storage (*Data Storage*)
   5. Looker Studio (*Data visualization*)
   6. VPC Network and Firewall Rules
5. Terraform(*Cloud Infrastructure Management*)
6. Docker(*containerization*) and Dockerhub(*Distribute container images*)
7. SQL(*Data Manipulation*)
8. Javascript
   1. D3.js for data visualization
9. Makefile",87,18,AffectionateEmu8146,2024-02-11 19:02:29,https://www.reddit.com/r/dataengineering/comments/1aofkv9/updated_personal_endend_etl_data_pipelinegcp/,False,False,False,False
18cnsae,Adidas Sales data pipeline,"Fun project: I have created an ETL pipeline that pulls sales from an Adidas xlsx file containing 2020-2021 sales data..I have also created visualizations in PowerBI. One showing all sales data and another Cali sales data, feel free to critique.. 
I am attempting to strengthen my Python skills along with my visualization. Eventually I will make these a bit more complicated. I’m currently trying to make sure I understand all that I am doing before moving on.  Full code is on my GitHub! https://github.com/bfraz33",86,36,Fraiz24,2023-12-07 04:49:12,https://www.reddit.com/gallery/18cnsae,False,False,False,False
16thbm7,"I recently passed the DP-203, here are my notes to prepare","  

My Stats:  
Azure Engineering Experience: 2 years on and off  
Studied: 30 days – Ave 3 hours/day  
Scored: 892/1000

Exam Covers: Data Factory, Synapse, Stream Analytics, Event Hubs, Databricks, Data Lake Storage, SQL/Scala

Resources I paid for:   
Alan Rodrigues – Udemy - DP-203 Course - 21 hours long - Updated for 2023. Very good course, but not enough to pass the exam. 

Measure Up Test Prep Questions

You will get questions on:  
Parquet Files, Data Lake Gen2 storage, Slowly Changing Dimension Tables (SCDs), Fact & Dimension tables, Synapse Distributions (Hash, Replicated, Round-Robin), Synapse Partitions & Indexes, Security Options. I’d say half the questions involved Synapse. 

Final Tips:

• Make sure you’re getting near 100% on test preps before you attempt the exam.

• Have Azure open when you do the test prep questions, to practice for real.

• If you feel you’re not ready, re-schedule the exam, which is free.

• Book the exam in advance - Stopped me from procrastinating or chickening out.

• Organize your time, I needed 3 hours/day for 4 weeks to prepare. Depends on your skill level.

• Read the question & answer options first. Then look at the case study. Helps you find the answer quicker. 

Here are some free resources.

Free Test Prep Questions:  
 [https://www.analystlaunch.com/c/dp203-test-prep-download-landing](https://www.analystlaunch.com/c/dp203-test-prep-download-landing)

Video on passing the exam:  
 [https://www.youtube.com/watch?v=fRR5FLrv398](https://www.youtube.com/watch?v=fRR5FLrv398)

Sitting your first Microsoft exam at home:  
 [https://www.youtube.com/watch?v=hV\_DpwpzNZI&t=1s](https://www.youtube.com/watch?v=hV_DpwpzNZI&t=1s)

Good luck.",87,34,dojogreen,2023-09-27 10:52:00,https://www.reddit.com/r/dataengineering/comments/16thbm7/i_recently_passed_the_dp203_here_are_my_notes_to/,False,False,False,False
16i3u4l,How to build experience in Kafka and Spark if not in a data engineering job?,"I've worked as a data scientist / engineer for the last 9 years but always at a scale a bit below where you really need distributed computing (i.e. SQL databases of a few terabytes). I'm interested in developing the skills that can take me to the next level of scale, but at my job we simply don't have that amount of data. Launching and running a cluster just for fun also seems like it would be a bit expensive. And if I'd want to make a shift to a senior data engineering role at this larger scale, they're going to want me to have some of this experience _before_ I get hired.

What's a good way to expose myself to problems that I can solve with Kafka / Spark (i.e. I'm interested in streaming algorithms and mapreduce-like problems)? I'm wondering if there are (for example) open source geo datasets and public servers that you can do some work on (though obviously those cost money as well, so maybe I'm naive to think that).

Obviously I'm a bit new to this area so please do let me know if I said anything dumb :) I read ""Designing Data-Intensive Applications"" and have a decent grasp of CS fundamentals, but obviously there's some specialized expertise to be had here.",89,14,failarmyworm,2023-09-14 00:30:54,https://www.reddit.com/r/dataengineering/comments/16i3u4l/how_to_build_experience_in_kafka_and_spark_if_not/,False,False,False,False
168208y,I've been trying to wrap my head around the use of Snowflake,"So I've been doing Data Engineering for a while now, and understand the cloud based Data Engineering tools from AWS, CGP, Azure, etc. I keep seeing a lot of job descriptions requiring or preferring knowledge of Snowflake, which I've never had to use in my career.

I've read about it a few times and I think I generally understand the concept, but what I guess I don't understand is why you would use it?

From what I have read and watched some videos of (please correct me if I'm wrong), Snowflake works as a SaaS Data Warehouse that integrates with the big three cloud providers. For AWS, for example, you would host the data in AWS S3, and the processing comes from AWS EC2 containers that it would instantiate for you. So it seems to do a lot of the orchestration?

So in this case, why wouldn't you just use Glue, Athena, RedShift, etc to do this? It seems like you'd be paying AWS for their services, plus Snowflake. I know that you could potentially integrate data from the different cloud platforms together, but I know there are at least ways of bringing in, for example Google Analytics data into AWS through things like AWS Appflow, if not programmatically.

So I've been wondering, is it cheaper, more abstracted and easier to use, more performant? A lot of companies seem to be using it. I thought I'd ask this here so I had the advantages in mind before I sign up for a free trial or something.",87,79,DEDumbQuestions,2023-09-02 13:03:34,https://www.reddit.com/r/dataengineering/comments/168208y/ive_been_trying_to_wrap_my_head_around_the_use_of/,False,False,False,False
11vv3jo,"For those of you who were self taught, what was your path into data engineering","I’m looking to get into data engineering myself and come from an analyst background in healthcare.

I am curious as to what your paths into data engineering were for those who didn’t come from a traditional background. What has your career path been like? How did you decide to get into DE and how did you do it?",86,28,xyzabc123410000,2023-03-19 19:32:15,https://www.reddit.com/r/dataengineering/comments/11vv3jo/for_those_of_you_who_were_self_taught_what_was/,False,False,False,False
102d1fm,Writing a Python SQL engine from scratch,,86,11,captaintobs,2023-01-03 16:55:33,https://github.com/tobymao/sqlglot/blob/main/posts/python_sql_engine.md,False,False,False,False
x3bb11,Quarterly Salary Discussion,"This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering. Please comment below and include the following:

1. Current title

2. Years of experience (YOE)

3. Location

4. Base salary & currency (dollars, euro, pesos, etc.)

5. Bonuses/Equity (optional)

6. Industry (optional)

7. Tech stack (optional)",87,144,AutoModerator,2022-09-01 16:00:11,https://www.reddit.com/r/dataengineering/comments/x3bb11/quarterly_salary_discussion/,False,False,False,False
u7rk6a,Is Data Modelling still an expected part of a Data Engineer skillset?,"I'm currently recruiting for a data engineer with 2-3 years experience to create pipelines to supply a coud data warehouse, with the expectation that they will then support the activities to decide how the data should be structured in the warehouse and process it to this format. I'm getting a lot of people applying with Python, Spark, Scala, with demonstrable expertise in pipelines moving data from A to B, but I'm also finding people either don't have any data modelling experience, or don't want to engage with a role that involves data modelling. 

I'm thinking this might be the growing separation between Data Engineering and Analytics Engineering, but are others finding this distinction to be so clear cut now?",84,70,Length-Working,2022-04-20 08:14:48,https://www.reddit.com/r/dataengineering/comments/u7rk6a/is_data_modelling_still_an_expected_part_of_a/,False,False,False,False
ssrt2b,Data Engineering O’Reilly Humble Bundle,"https://www.humblebundle.com/books/data-engineering-oreilly-books

15 books for $18. Been collecting tech humble bundles for years. Causally reading them has helped my career. Often the books aren’t perfect, a few years outdated, but generally worth it. 

First bundle I’ve noticed catered to data engineering. Ifyky O’Reilly is a legit publisher.",89,14,turkey1234,2022-02-15 02:17:42,https://www.reddit.com/r/dataengineering/comments/ssrt2b/data_engineering_oreilly_humble_bundle/,False,False,False,False
ri2k6b,Can someone explain the big deal with dbt?,"I'm not sure I get what the craze is about? It just executes SQL using the already existing SQL engine in the database...

I get it's more observable than filling a database with a bunch of stored procedure scripts, but if I'm already orchestrating my SQL transforms with something like airflow or prefect anyways what do I need DBT for?

Is it like a citizen data engineer tool? So like there are data science tools that make doing some simple stuff into a pretty GUI. Is it that for data engineers?",88,114,BoiElroy,2021-12-16 22:56:40,https://www.reddit.com/r/dataengineering/comments/ri2k6b/can_someone_explain_the_big_deal_with_dbt/,False,False,False,False
qhtox6,Is our coding challenge too hard?,"Right now we are hiring our first data engineer and I need a gut check to see if I am being unreasonable.

Our only coding challenge before moving to the onsite consists of using any backend language (usually Python) to parse a nested Json file and flatten it.  It is using a real world api response from a 3rd party that our team has had to wrangle.

Engineers are giving ~35-40 minutes to work collaboratively with the interviewer and are able to use any external resources except asking a friend to solve it for them.

So far we have had a less than 10% passing rate which is really surprising given the yoe many candidates have.  

Is using data structures like dictionaries and parsing Json very far outside of day to day for most of you? I don’t want to be turning away qualified folks and really want to understand if I am out of touch.

Thank you in advance for the feedback!",90,109,DiligentDork,2021-10-28 18:45:30,https://www.reddit.com/r/dataengineering/comments/qhtox6/is_our_coding_challenge_too_hard/,False,False,False,False
hiyyhg,What do you want to see in a 6-month Data Engineering course?,"Hi all,

I've built the data infrastructure from the ground up at a couple of small, successful startups. I like teaching and mentoring and I believe my position as a lead and in startup world has given me the experience to really teach people ""core"" and future-proof data engineering knowledge.

I'm looking to write a 6-month long data engineering course with the goal of taking someone with no tech/programming experience to a place where they could get a junior DE position. I think there's a huge knowledge gap for someone to become a data engineer, and most online course I have seen are not great (including Udacity's).

My main questions are:

* Are most people wanting to get into data engineering complete beginners to coding or are they general SWE's looking to transition? What bucket do you or any of your friends who are interested in data engineering fit into?
* **More generally, what would you want to see in a data engineering course?**

Just to give an idea of what I'm thinking:

1. Curriculum will be: A heavy emphasis on SQL/Python for ETL and ELT, data warehouses and data modeling, distributed ETL tools (Spark and Hive in EMR, serverless tools like Athena), Airflow, some RDBMS, some BI tools/analysis, maaaaybe a little NoSQL. And general cloud knowledge (EC2, S3, RDS etc). Not sure about streaming.
2. Heavily cloud-based. We will work with rdbms, data warehouses, and ETL tools in the cloud. This will be billed on the students cloud accounts, so we can coast on free tier, or we can easily limit usage to under a $100 using certain instances. If something can be learned locally (like Airflow and Python), it will be. I will host data in S3 and snapshots. Leaning towards AWS.
3. The course itself might be $100 or broken into chunks so you can only pay for what you need to learn.
4. The course will be **very** project-based. I will provide helpful code, data, assignments, guidance etc. The course content itself will be videos/text. I may see if I can get short multiple-choice quizzes or code evaluation.
5. I'll create a discord to answer questions and to create a student community. I'm not sure how feasible real person evaluation of students work is possible. But if I can figure out a good solution, I will.
6. Students should graduate with interesting projects on their resume as a result of the course.

&#x200B;

Thoughts?",84,48,datamewhat,2020-06-30 23:35:09,https://www.reddit.com/r/dataengineering/comments/hiyyhg/what_do_you_want_to_see_in_a_6month_data/,False,False,False,False
e5vxuv,"Metaflow, Netflix's Python framework for data science, is now open source",,88,3,thundergolfer,2019-12-04 07:53:35,https://metaflow.org/,False,False,False,False
18gn43u,How do you make working with SQL enjoyable (or less tedious),"Although I work as a Machine Learning Engineer, sometimes I'm requested to build some queries, for instance for dashboarding purposes.

However, I find it really tedious when working with SQL. My main reasons being:

1. Most of the times we work with tables in the DataLake we don't own, hence it takes an awful lot of time to get an understanding of how that data is structured (and what kind of data problems it might have)
2. I feel really unproductive having to wait for query results. It really slows down any kind of exploration one might want do with the data like one would with tools like Pandas or Polars (and loading data locally is not an option as we're talking of billions of records here, which we're handling with Spark)
3. Sometimes queries grow to be extremely complex, which makes it harder for team mates to review
4. Along the previous point, I really feel SQL is extremely unreadable as compared to a programming language",86,96,barberogaston,2023-12-12 14:28:59,https://www.reddit.com/r/dataengineering/comments/18gn43u/how_do_you_make_working_with_sql_enjoyable_or/,False,False,False,False
186zm6c,My team only uses Excel to manage all of our critical data and I’m struggling to fix it,"This is my first job out of college and I’m more of a data analyst with little experience in any data engineering.

My manager tasked me with finding ways to improve our team’s data management practices.  We use one Excel workbook with 40+ sheets as our central hub for how we store, manage, and interact with our data critical to day-to-day operations.

Most of our team uses this Excel file, oftentimes simultaneously, which causes mistaken data entries, conflicting filtering, and so on.  It has “worked” so far but continues to grow beyond its useful limit.

My issue is I need to find a solution that my team will accept and be able to continue to maintain should I leave. They do not have rich technical knowledge, so I have to be careful about developing too crazy of a solution.

I’m just totally stuck.  On one hand, it seems stupid to have all this data laying around in Excel.  On the other hand, I don’t know how to find an acceptable solution that balances sophistication and user-friendliness for the “business users”.

On top of that, I have to develop this all alone and self-guided.  This is one of my first projects and I don’t want to fail.  Btw, this is a fortune 50 company in a highly regulated industry 😭.

Any suggestions would help greatly for my own sanity",86,83,CFCNandos,2023-11-29 20:46:44,https://www.reddit.com/r/dataengineering/comments/186zm6c/my_team_only_uses_excel_to_manage_all_of_our/,False,False,False,False
17pvhbv,Is it a must to be very good at SQL for a data engineer position?,"The question does sound silly. I was a programmer and loved web development and making something out of nothing but the heavy coding around the business functionality, CI/CD, Elaborate testing, is not appealing.

&#x200B;

What I love: 

data visualization, cleaning, statistics (the numbers not the math) and generally love information and DB design and optimization.

What I hate: 

my mind would rarely be able to wrap itself around sql queries that have more than a couple of joins, specially if its a query inside another. I hated reading those. I also despised functional programming and recursion because I couldn't visualize it

Why am I considering Data engineering? 

I mentioned my love for data and data cleaning, not to mention salary and I imagine with new tools the querying would not need to be SQL style. Is it realistic to do this job well without that skill?",88,99,knockedownupagain,2023-11-07 14:27:33,https://www.reddit.com/r/dataengineering/comments/17pvhbv/is_it_a_must_to_be_very_good_at_sql_for_a_data/,False,False,False,False
15kyl33,Is it hard to find Data Engineers with good SWE skills?,"At my company we have open Data Engineering positions but almost everyone falls short on their coding skills.

Then, people we’ve hired have needed quite a bit of hand holding on the SWE aspect of tasks (testing, coding, IaC, devops, etc).

Are we asking too much? I thought of SWE as a requirement for Data Engineering and the Fundamentals of Data Engineering book even defines SWE as an Undercurrent of Data Engineering.

Edit: We are only hiring in a single South American country so we can’t interview most of you.",86,88,SexySlowLoris,2023-08-07 22:01:49,https://www.reddit.com/r/dataengineering/comments/15kyl33/is_it_hard_to_find_data_engineers_with_good_swe/,False,False,False,False
15gf97e,Is traditional data modeling dead?,"As someone who has worked in the data field for nearly 20 years, I've noticed a shift in priorities when it comes to data modeling. In the early 2000s and 2010s, data modeling was of the utmost importance. However, with the introduction of Hadoop and big data, it seems that data and BI engineers no longer prioritize it. I'm curious about whether this is truly necessary in today's cloud-based world, where storage and computing are separate and we have various query processing engines based on different algorithms. I would love to hear your thoughts and feedback on this topic.",85,59,New-Ship-5404,2023-08-02 17:35:21,https://www.reddit.com/r/dataengineering/comments/15gf97e/is_traditional_data_modeling_dead/,False,False,False,False
15614vp,Data analyst/engineer at Tesla,"I just had 20 minutes interview (1st) with Tesla on a role called data analyst/engineer, which requires these skills below. I was asked right off the bat some technical questions without giving me chance to introduce myself. I was asked what confusion matrix is and I couldnt pull out from my brain what they are. I know it's very basic but I wasn't prepared. I told her I came in with DE readiness so they asked me on DDL, how to drop a column (I swear I never had to drop a column but I manage to give an answer that works lol). This interview makes me feel so rushed from their end and at the same time I feel underqualified.😭

What You’ll Do
Create and/ or enhance action-driven dashboards (e.g., using Tableau). 
Support ad hoc data, SQL query, analysis, and debugging requests. 
Create and maintain an optimal database schema and data pipeline architecture. 
Create ETL pipelines in Airflow for analytics team members that assist them in building and optimizing their reports. 
Communicate with stakeholders, gather business requirements, and brainstorm KPIs. 
Develop/ maintain internal documentation. 
Proficiency in SQL, and comfort with a scripting language (e.g., Python) is a plus. 
Proficiency with a data visualization tool (e.g., Tableau). 
A good understanding of relational databases and database engineering concepts. 
Familiarity with data pipelines and a Workflow Management Tool (e.g., Airflow) is desirable.",86,94,buianhthy1412,2023-07-21 21:41:29,https://www.reddit.com/r/dataengineering/comments/15614vp/data_analystengineer_at_tesla/,False,False,False,False
13k9fvu,What have you learned the hard way?,"Describe your experience & position and tell us about something you learned the hard way. I'll go first...

DE with 2 YOE. Background was non-technical. I design and implement pipelines to serve analytics.  


**Story**: We believed a service provider (service desk and software maintenance) was fudging numbers in various ways to make their service look better than it was. We had access to their service desk ticket data, and the business logic behind whether or not a ticket met the service target / SLA.

Our data analyst is almost done with his report for leadership on the matter. Asks me to add a calculated field to the materialized view that I provide him so he can finish up and send it out that day. I define a new CTE, left join it to the existing data, and define the new field. Briefly manually scroll through the new column and it looks good. Recreate the view and tell him to refresh on his end. He runs his calculation and starts finishing up his report.

Later on, he hits me up on Teams with the dreaded ""something looks off"". Long story short, I did not de-duplicate the CTE I created, and the join resulted in some entries having many duplicate records, making his numbers inaccurate. Disaster averted thanks to him catching this.  


**Lesson**: You need metadata capture and observability at every. single. step. in your pipelines, including the view layer.

I could have caught this manually if I had taken 5 extra seconds, but it's just better engineering to implement CDC or trigger function + metadata table, then put together a little metadata & lineage dashboard.  

So...what have you learned the hard way?",86,81,udonthave2call,2023-05-17 18:09:46,https://www.reddit.com/r/dataengineering/comments/13k9fvu/what_have_you_learned_the_hard_way/,False,False,False,False
136rwag,"Are there database design Standards out there? As in, formal documents listing exact best practices for OLTP database design?","When collaborating with Software Engineering, Product, etc. there are always things that come up regarding best practices in a production database.

* timestamps should always include a time zone and be stored in UTC (right?)
* Foreign key constraints should always be defined on foreign keys
* Column names should be descriptive
* Boolean columns should begin with is\_ or has\_

You get the idea. There are dozens or hundreds of standards I could think of if I kept going.

I see a few nascent attempts, but I'm surprised that with decades of SQL usage gone by, there aren't some standards that seem more.... authoritative at this point. Does anyone know of any semi-official standards, or have thoughts here?

 \- [https://ovid.github.io/articles/database-design-standards.html](https://ovid.github.io/articles/database-design-standards.html)  
 \- [https://fdotwww.blob.core.windows.net/sitefinity/docs/default-source/content/it/docs/standards/databasedesignstandards02042016.pdf?sfvrsn=115b611e\_0](https://fdotwww.blob.core.windows.net/sitefinity/docs/default-source/content/it/docs/standards/databasedesignstandards02042016.pdf?sfvrsn=115b611e_0)  


It would be really handy to have something authoritative, at least as a starting point, instead of arguing about these from scratch and not getting anywhere.",87,22,dlb8685,2023-05-03 16:18:10,https://www.reddit.com/r/dataengineering/comments/136rwag/are_there_database_design_standards_out_there_as/,False,False,False,False
11v9z1q,Manager denying use of Git,"Hi All,

I have been recently moved to project where we are developing application for client using Azure ecosystem. We are 25+ memeber  team where each team takes care of different part. I work with a sub-team of 3 members (manager,senior resource and myself -analyst)  where we do transformation using Azure databricks. Our manager is not from core development background and he was DBA before this project or involved in work around DBA. I have been trying to convince him to use Git for version controlling for reasons which are basic and every developer knows. But he is convincing us to work on same folders ( In databricks we can access same folders / shared access) . I tried my best to  convince him but no luck. Can you guys suggest any other ways to convince him  or has anyone faced similar situation before?

Update : 
Tried again with suggestions but no luck. Cannot highlight it to higher management as it will create other issues and will make the environment more bad for me. Planning to leave the team in the upcoming days.",87,84,Aswathama_001,2023-03-19 03:26:08,https://www.reddit.com/r/dataengineering/comments/11v9z1q/manager_denying_use_of_git/,False,False,False,False
zpt1ri,From /recruitinghell - (Name and Shame: NielsenIQ - Interviewer ends interview 12 mins in.),"Recruiter ends interview because candidate has no experience with Databricks but the job description is not explicit about it being required. The unprofessionalism that some of these companies display is astonishing..

&#x200B;

>The first interview was scheduled on Tuesday and 20 minutes before, I receive an email from the recruiter asking to postpone for Friday because the hiring manager had an “emergency”. I was extremely annoyed, but I accepted, nonetheless.  
>  
>We start the interview on Friday, and I immediately notice the hiring manager avoiding eye contact. I found that to be strange and it made me uncomfortable. Then, 12 minutes in, he asks if I have experience with DataBricks which I promptly told him that I do not, and he proceeds to say “I have to end the interview because DataBricks is a requirement”  
>  
>I pressed him by stating that the Job Description clearly states *(Airflow, Databricks, Snowflake, Serverless Functions, Cloud storage preferred)* and that I never included Databricks on my resume.  
>  
>He then says “Well, we are looking for someone Senior” which I replied by saying that the recruiter assured me on 3 different conversations that this was not a senior role. We went back and forth for some 5 minutes, and I eventually end the call.  
>  
>I emailed the recruiter right away to share my displeasure of the session and she calls my phone. She explains to me that the hiring manager had decided a few hours before that he wanted someone senior which angered me because he basically wasted my time twice.

[https://www.reddit.com/r/recruitinghell/comments/zpflbr/name\_and\_shame\_nielseniq\_interviewer\_ends/](https://www.reddit.com/r/recruitinghell/comments/zpflbr/name_and_shame_nielseniq_interviewer_ends/)",89,30,Manoloskinny,2022-12-19 14:28:10,https://www.reddit.com/r/dataengineering/comments/zpt1ri/from_recruitinghell_name_and_shame_nielseniq/,False,False,False,False
ym47ac,Snowflake Architecture Overview,,85,6,ian-whitestone,2022-11-04 16:58:42,https://select.dev/posts/snowflake-architecture,False,False,False,False
xojb5f,"Data Engineering Concepts: Definitions, Backlinks, and Graph View","If you are like me and daily confused about new terms in data engineering, I started a [Data Engineering Concept](https://glossary.airbyte.com/term/data-engineering-concepts) page. You can click on each of them, dig into details, and learn more about related concepts (everything is open on [GitHub](https://github.com/airbytehq/glossary)).

**Data Engineering** is still not well defined; However, in the latest book on Fundamentals of Data Engineering by Joe Reis, Matthew Housley tries and does probably best as of today, and it's getting clearer. Besides several boot camps, universities are starting to get a degree in Data Engineering as Data Science did before. 

Data Engineering is a discipline that has shifted over the years from a Database Administrator (DBA), ETL Developer, and Business Intelligence Specialist and merged with Software Engineers to a Data Engineer with the growth of data. Today you might call it a Data Engineer, a Software Engineering with business intelligence and big data capabilities.""

What is the recent term that confused you?",84,8,sspaeti,2022-09-26 13:28:13,https://www.reddit.com/r/dataengineering/comments/xojb5f/data_engineering_concepts_definitions_backlinks/,False,False,False,False
x6mbt3,What’s the best paid courses to prepare for DE and certifications?,"I know that there is plenty of free resources but since my company offers a good budget for individual trainings, and th end of the year is approaching, I’m looking for some excellent paid resources to do DE trainings that also enables me to get prepared for AWS certification (ML Specialty, DA Specialty). So, have you guys any recommendations?

P.S.: I’m an data analyst, half scientistchy, for about 3 years now. Work with fraud detection. I know a lot of python and SQL.",84,21,None,2022-09-05 17:43:27,https://www.reddit.com/r/dataengineering/comments/x6mbt3/whats_the_best_paid_courses_to_prepare_for_de_and/,False,False,False,False
vtwt3q,I want to move from startup to big company. where I'm always seeing spark as a requirement how to learn it in home I want a local setup but seems very resource extensive ?,Please let me know what can I do to practice and learn it,86,40,None,2022-07-07 23:59:36,https://www.reddit.com/r/dataengineering/comments/vtwt3q/i_want_to_move_from_startup_to_big_company_where/,False,False,False,False
r893rw,Why is Snowflake so popular?,"Hi all,

I am just wondering why so many companies use Snowflake. As far as I know, it is a Cloud Datawarehouse, which means that the main competitors are platforms such as BigQuery, Redshift, Synapse. What makes Snowflake better than the rest?

Do you use it in your work? In that case, which kind of application do you have? And which tools do you integrate it with?

Thanks!",86,54,francesco1093,2021-12-03 21:04:00,https://www.reddit.com/r/dataengineering/comments/r893rw/why_is_snowflake_so_popular/,False,False,False,False
mb004b,Monte Carlo Banned from Locally Optimistic Data Community,,87,28,KafkaStreamEsque,2021-03-22 22:40:53,https://i.redd.it/83qochx4qno61.png,False,False,False,False
1bxaz56,If you restarted your career - what would you do differently?,"I'm particularly interested in takes from Engineers with 8+ years of experience.

Would you grind harder in your earlier years - or would you have been more chill?

Would you job hop more frequently or stay at a company with good enough culture?

Salary or projects? Smaller companies or bigger companies?

Doesn't have to be technical advice, any kind of life advice is appreciated haha",85,78,the_underfitter,2024-04-06 12:59:56,https://www.reddit.com/r/dataengineering/comments/1bxaz56/if_you_restarted_your_career_what_would_you_do/,False,False,False,False
1b5986w,[AMA] From FAANG Data/Software Engineer to YC-Backed Startup Founder - Ask Me Anything!,"Hey Reddit! I'm a former engineer at FAANG who decided to take the leap and start my own venture. I'm now building Quary, a startup in Y Combinator's Winter 2024 batch. The journey from a mere concept to securing a $500k investment from YC has been filled with a rollercoaster of emotions, countless challenges, and a steep learning curve. I'm here to share my experiences, insights, and the realities of transitioning from an engineering role to entrepreneurship.

Feel free to ask me anything about the journey, the lessons I've learned, or any advice you might need for your own career or startup aspirations!

(And hey, if you happen to check out Quary and find it interesting, [a star on my GitHub repo would be a nice way to show support. No pressure, though](https://github.com/quarylabs/quary)!)",84,61,Background_Call6280,2024-03-03 05:18:26,https://www.reddit.com/r/dataengineering/comments/1b5986w/ama_from_faang_datasoftware_engineer_to_ycbacked/,False,False,False,False
17fgte6,vendor confession: there's just too many ETL/ELT tools,"Pulling back the charade for a moment here... as a vendor, I really empathize with all the DEs in the thread trying to sort through the noise.

I'm not sure if it is the vast amount of VC funding that came into the space, the potential level of nuance to every pipeline, or something else (act of god?)... but it is borderline unimaginable how many new/different vendors exist for creating data pipelines.

Just taking a basic example, googling say Postgres to Snowflake will quite literally yield hundreds of distinct possible vendors. I scrolled so long waiting for repeat vendor domains that I actually got bored and stopped. And this is just revealing all the companies that have put in the effort to try and hack Google SEO results (stitch even bought hundreds of domains of [https://postgres.tosnowflake.com/](https://postgres.tosnowflake.com/) \-- IMHO: Google Search for B2B is increasingly a failed product for surfacing quality products/content... but I digress.)

Add in all the open-source, native tooling, or code-based ways to create a pipeline... and I think there might literally be 200+ legit ways to ETL from Postgres to Snowflake.

How is there possibly \*SO\* many solutions? Is massive consolidation of point-to-point ETL tools coming immediately?

While I think what we do is somewhat unique ([estuary.dev](https://estuary.dev)), there is still a ton of overlap, and to my partially trained eye.... it feels like Rivery/Fivetran/Hevo/DMS/Stitch/Talend --- just all basically the exact same 'sometimes good enough' solution w/ up to 99% the same features

END RANT

&#x200B;",84,45,MooJerseyCreamery,2023-10-24 16:26:15,https://www.reddit.com/r/dataengineering/comments/17fgte6/vendor_confession_theres_just_too_many_etlelt/,False,False,False,False
16u1sxr,"How do you ""know"" your architecture is correct?","I am always wondering how can a data engineer know, and then convince product team or business that their architecture or design is correct? There is not exactly an exact science to data engineering. There are companies which are happy using out of the box tools and make it work, and there are engineers putting up with complex legacy codebases using microservices et al. Besides reading up and doing courses trying to get as close as possible to best practices, your architecture suggestions are subject to either lessons learnt from experience (takes 1-2 years for a big data project to take real shape) or googling and doing the best possible back of the envelope tradeoffs. While other non technical team members expect you to give them a silver bullet or quickly lose trust in you. How do you establish this confidence in a solution without seeming like a tinkerer?",85,47,None,2023-09-28 00:54:38,https://www.reddit.com/r/dataengineering/comments/16u1sxr/how_do_you_know_your_architecture_is_correct/,False,False,False,False
14ltv6p,"Which are the most inefficient, ineffective, expensive tools in your data stack?","With all of the buzz around the high costs of various platforms and tools used for building data pipelines, including data collection, data warehousing, data processing and transformation, extracting insights out of the data - 

Which are the most inefficient, ineffective, expensive products that you have experienced?

Top 5 or 10 products listicles in various categories are just paid marketing campaigns and provide biased information.

What is the tribal wisdom about the worst offenders in data tools and platforms that you would recommend staying away from and why?

Share away and help the budding data engineers out.",85,223,drc1728,2023-06-29 03:29:59,https://www.reddit.com/r/dataengineering/comments/14ltv6p/which_are_the_most_inefficient_ineffective/,False,False,False,False
zo7609,Any good resources to learn Apache spark?,"I’m learning Apache Spark through “Learning Spark” book. But I would prefer good video tutorial to learn.  

Can you guys please share good resources to learn how things work under the wood.",82,63,Consistent_Ad5511,2022-12-17 14:15:23,https://www.reddit.com/r/dataengineering/comments/zo7609/any_good_resources_to_learn_apache_spark/,False,False,False,False
zn35zt,How do you use Airflow 'properly'?,"My manager has tasked me with setting up Airflow on AWS. Depending on the cost I guess my options range from ECS, MWAA or simply deploying it on EC2. In any case my question relates to writing DAGs: Airflow is merely a scheduler and is not supposed to perform the actual ETL work. I want to deploy Airflow and write DAGs in a way that will be scalable and isn't going to incur unnecessary costs, so with that in mind, are there any tips/best practices to keep in mind when writing DAGs? Any operators I should avoid in keeping with the principle that Airflow shouldn't be performing the business logic?

As an aside, where do you typically store connection credentials in prod? Is the Airflow UI sufficient or is it better to use environment variables?

Appreciate the help.

Edit: For anyone curious, I decided to use Terraform to provision a VPC + network infra, IAM role and MWAA environment twice daily (when our DAGs are typically running) on a schedule. Given that other members of my team aren't so familiar with AWS, MWAA was probably the wisest and most straightforward deployment option. Regarding the DAGs/tasks themselves I'll be looking strongly at using ECS and its operator in the future once I've moved all the DAGs to S3, as many of the answerers suggested. Thanks all.",83,49,None,2022-12-16 02:02:13,https://www.reddit.com/r/dataengineering/comments/zn35zt/how_do_you_use_airflow_properly/,False,False,False,False
zl7gvh,Do you need to code in your job?,"What kind of scripts ir programs do you develop?

I'm currently a data engineer only using a low code cloud platform (informatica) and i am unmotivated with my routine.",86,82,Idalen,2022-12-13 21:26:23,https://www.reddit.com/r/dataengineering/comments/zl7gvh/do_you_need_to_code_in_your_job/,False,False,False,False
z90wtm,Great Expectations is annoyingly cumbersome,"You have to create a great expectation project, suite, checkpoint, data sources... and then several lines of code for everything you want to do, and it's not even trivial to validate an in-memory pandas dataframe.

Come on, I just want to have to do this:

    import pandas as pd
    import great_expectations as ge
    
    df = pd.read_csv(file_path)
    ge.check_duplicates(df, primary_key=""id"")
    ge.check_not_nulls(df, columns=""all"")

Why all the complications? I honestly don't get it. Maybe I didn't even understand the tool and what I say is possible to do, but then, oh boy, what a tutorial.

Is there any other alternative out there? Does it get easier when you have some experience with it?",86,72,L3GOLAS234,2022-11-30 19:38:10,https://www.reddit.com/r/dataengineering/comments/z90wtm/great_expectations_is_annoyingly_cumbersome/,False,False,False,False
xlwc4x,Data Modelling part of Data Engineering?,I'm having a fascinating conversation with senior management around this topic - what are your thoughts? What I thought was initially clear cut is turning to not be so!,85,59,mister_patience,2022-09-23 12:34:14,https://www.reddit.com/r/dataengineering/comments/xlwc4x/data_modelling_part_of_data_engineering/,False,False,False,False
v6xs2x,How are you guys validating your data?,"I saw similar posts but I also wanted to share the specific flow in our data pipeline. The data warehousing project started at the my company some months ago and we setup many ETL jobs in our pipeline using Spark-Scala to write the logic and Airflow to schedule and run the processes to deliver data to business teams on Redshift tables to be queried.

Currently everything is running smoothly and we never encountered any major problems within our pipeline, and now we're doing some research to setup a Data Quality project since we're not testing our data yet. 

I've personaly looked into the Great Expectations Library and it seems promissing to implement it at the end of some Airflow pipelines to do some basic validations, such as checking if a column value is Null and send us an alert. We don't need anything too fancy yet.

I wanted to know how are you guys doing Data Validation at your companies! Which tools are you using? Are you checking the resulting table at the end of your pipelines?",85,48,Deer-Antlers,2022-06-07 14:51:23,https://www.reddit.com/r/dataengineering/comments/v6xs2x/how_are_you_guys_validating_your_data/,False,False,False,False
v6ehhx,"You have total freedom for developing the data platform of your company, what tools do you choose?","Imagine that you are the first Data Engineer in the company for building a data platform, and you have total freedom for choosing the tools and a decent budget. It must cover the extraction from the backend/foundation sources (Kafka, Operational databases... whatever) to the Dashboard in a viz tool. 

What would your platform look like? The platform that you have always dreamed about, the one that will make every Data Engineer want to work in your team because of how cool the tech stack is.

I think I would choose something like:

Containerized Airflow for orchestrating  
Terraform for IaC  
Amazon as the Cloud (maybe there are needs for EC2, Lambda... whatever)  
DBT for the T of the ELT in combination with Airflow  
S3 for Storage  
GitLab for CI/CD and Version Control  
Python as the main language

And I have doubts about the DWH layer and the BI

I like very much Snowflake (more than Redshift and BigQuery), but maybe it makes more sense to use Spark in Databricks. Or maybe both? Or just use Spark in Amazon EMR? Or maybe some open source like Presto?

What about the BI? I don't know if it would make sense to don't go with the typical vendors (Tableau, PowerBI, Looker...) and maybe go to a more recent solution like Apache Superset or Metabase.",83,50,L3GOLAS234,2022-06-06 21:42:49,https://www.reddit.com/r/dataengineering/comments/v6ehhx/you_have_total_freedom_for_developing_the_data/,False,False,False,False
uso1re,DE interview horror stories,"With Data Engineering being so hot 🔥 these days, I want to hear some interview horror stories. Gotta be some good ones out there.",83,81,None,2022-05-18 22:04:05,https://www.reddit.com/r/dataengineering/comments/uso1re/de_interview_horror_stories/,False,False,False,False
rdaclp,dbt Coalesce 2021 takeaways,"Hi r/dataengineering, few months ago I've shared my takeaways of the Airflow Summit and this time I'd like to share takeaways from the dbt Coalesce 2021 conference. I really don't know if dbt is used by people in this sub but still I'm trying.

So, for people not used to it, dbt is a framework to **organize** and **run** your SQL queries on top of your favourite data warehouse. More and more companies are using it, the core project is open-source but they sell a Cloud version with and IDE and stuff to ease the deployment of your projects.

Also from the panel at the conference we can see that dbt is not a tool used by Fortune 500 companies. I would bet they often already have a internal system doing the same.

This year at the Coalesce we got 5 kind of talks:

* Food for thought — to help us seeing forward
* 101 talks about dbt or other concepts
* Feedbacks from companies implementing dbt
* Promotional content (often from the sponsors)
* Diversity talks about how we can be more open in the data field

# Food for thought

* Erica from dbt explained why you should aim from self-service rather than Data as a Service, she also defined what is self-service and what you should measure to get it — [Scaling Knowledge over Scaling Bodies](https://coalesce.getdbt.com/talks/how-dbt-labs-is-building-its-data-team/)
* Tristan had a chat with Martin Casado about investments in data and [how big is this wave?](https://coalesce.getdbt.com/talks/keynote-how-big-is-this-wave/) — They talk about the fact that the Modern Data Stack is probably a déjà vu but still something changed because companies key differentiator now is data, before it was software. But after all isn't data software?
* [Down with ""data science""](https://coalesce.getdbt.com/talks/down-with-the-phrase-data-scientist/) — I liked it because the key concept is that words have a meaning, job titles matters.

# dbt

* [Git for analytics engineers](https://coalesce.getdbt.com/talks/git-for-the-rest-of-us/) and how to [build a mature dbt projects](https://coalesce.getdbt.com/talks/how-to-build-a-mature-dbt-project-from-scratch/)
* The metrics layer could be the next big thing when it comes to analytics. It means we have a middleware between our warehouse and our customer facing apps that holds metrics definition and that is able to answer quickly to every question. [Benn](https://coalesce.getdbt.com/talks/the-modern-data-experience/) described it and Drew (dbt co-founder) [gave us a glimpse of the dbt Server](https://coalesce.getdbt.com/talks/keynote-metric-system/) for 2022.
* Showcase of the [v1.0 version that was released this week](https://coalesce.getdbt.com/talks/dbt-v10-reveal/)

# What other companies are doing

* [Use dbt packages to encapsulate data logic](https://coalesce.getdbt.com/talks/so-you-think-you-can-dag-supporting-data-scientists-with-dbt-packages/)
* Use dbt macros to dynamically detect sources schema and then create dynamic sources in you dbt projects
   * By [Mattermost](https://coalesce.getdbt.com/talks/automating-ambiguity/)
   * By [Aula Education](https://coalesce.getdbt.com/talks/surviving-schema-changes-with-automation/), they use dbt\_utils to do it
* [Slido](https://coalesce.getdbt.com/talks/from-100-spreadsheets-to-100-data-analysts-the-story-of-dbt-at-slido/) develop [dbt-coverage](https://github.com/slidoapp/dbt-coverage) to get a documentation coverage number you could put in your CI/CD to naively push back analysts merge requests :D 
* Companies showcases also how they use the Cloud Metadata API to get lineage or run information about the their projects

&#x200B;

That is a small glimpse of what has been shown at the conference and also my favourite stuff, I've a more detailed post on [https://www.blef.fr/dbt-coalesce-takeaways/](https://www.blef.fr/dbt-coalesce-takeaways/) and all the talks can be found the conf website.

&#x200B;

Once again I tried to summarize the stuff to be readable on Reddit, I hope you like this kind of content.",85,20,blef__,2021-12-10 14:52:55,https://www.reddit.com/r/dataengineering/comments/rdaclp/dbt_coalesce_2021_takeaways/,False,False,False,False
qyn24c,"Lesson learned: meme good, watermark bad. Here's another DE-flavored meme as compensation.",,85,20,LSTMeow,2021-11-21 04:29:28,https://i.redd.it/gnv05d5fmv081.jpg,False,False,False,False
pwq32y,What made you choose DE over DS?,"Just as the title states, what was the motivation for going down DE over DS?",87,71,Tender_Figs,2021-09-27 20:41:30,https://www.reddit.com/r/dataengineering/comments/pwq32y/what_made_you_choose_de_over_ds/,False,False,False,False
g69bpo,[META] We're Building a Wiki. Now what?,"Here's your chance to share your vision for the wiki. 

A few ~~weeks~~ months ago there was [some feedback](https://www.reddit.com/r/dataengineering/comments/ez9drx/data_engineering_wiki/) requesting a wiki for r/dataengineering. I offered to help bring it all together. 

Some unanswered questions on my mind:
- Does r/dataengineering even need a wiki? Other resources exist, e.g. [the GitLab Handbook](https://about.gitlab.com/handbook/business-ops/data-team/)
- What topics should be curated for the wiki? More broadly, what is the role of the wiki?
- Should there be limitations on who can/should contribute to or curate the wiki?

This isn't an exhaustive list. If something else is on your mind, please speak up.",88,20,vogt4nick,2020-04-22 21:00:39,https://www.reddit.com/r/dataengineering/comments/g69bpo/meta_were_building_a_wiki_now_what/,False,False,False,False
1chr0pp,In light of the news about the python team at google,"If you hadn't heard, google let go it's entire python development team. Posts in r/programming and others made the rounds and lots of discussion ensued.

What caught my attention were a few comments about the maintenance required to keep python code running. In one case, C++ was mentioned as being more performant and having better longevity even with the C++ extensibility within python. I'm wondering where this discussion would fall within a dataengineering-centric community. I'm on mobile otherwise I'd put all the links I've come across in the last few days of reading.

Edit: I really appreciate the contributions & conversations. I'm seeing quite a few people doing many of the same things I have been doing, especially in the realms of mypy, pytest, pydocstyle, pylint, etc. To reiterate, the purpose of my post is less about corporate shenanigans and more about identifying & discussing non-python value in the DE ecosystem.",83,48,tolkienwhiteboy,2024-05-01 16:52:38,https://www.reddit.com/r/dataengineering/comments/1chr0pp/in_light_of_the_news_about_the_python_team_at/,False,False,False,False
1bgct3c,When to use Spark vs Pandas?,"Hi guys, just wondering if some of the more experienced DEs have an answer to this. I’ve played around with pyspark before but I think there’s a df size threshold below which pandas is faster and above which pyspark is faster? Just not sure what this threshold is and whether anyone has tried finding out or have any heuristics.",84,41,last_unsername,2024-03-16 18:34:33,https://www.reddit.com/r/dataengineering/comments/1bgct3c/when_to_use_spark_vs_pandas/,False,False,False,False
17592w5,I'm a beginner in the data engineering field with 1.5 years of experience in developing pipelines on ADF and working with SQL & Pyspark on databricks. Would you recommend this book for me?,Is it more advanced for someone like me with respect to my experience in DE.,82,40,SignalCrew739,2023-10-11 08:27:09,https://i.redd.it/oj68ytseajtb1.png,False,False,False,False
14hw7kj,What are your weekend side projects?,"Do many DEs have weekend side projects? Seems like so many software devs have side projects, but I hear less about it from DEs. Personally my side projects are birding, yard work, and being a dad 🤣",84,129,itty-bitty-birdy-tb,2023-06-24 15:47:01,https://www.reddit.com/r/dataengineering/comments/14hw7kj/what_are_your_weekend_side_projects/,False,False,False,False
135kadw,Experience with Data engineering Influencers on Linkedin.,Has anyone worked or have any real life experience with these data engineering influencers on Linkedin. Just curious how good as they in real life in term of their level of knowledge in data engineering.,83,101,manu13891,2023-05-02 12:49:05,https://www.reddit.com/r/dataengineering/comments/135kadw/experience_with_data_engineering_influencers_on/,False,False,False,False
106ypk4,Recommendations for having Fundamentasl of Data Engineering,"Hi Everyone,

&#x200B;

I know this is a recurrent post, but in my case, I have been working as a DE for about 3 years, I have become a Ssr DE and I still do not have the basics knowledge (i feel ashamed lol) of how to develop a data warehouse, what is the best approach for a data modeling, how to orchestrate or which tools to use, so:  


\-would you please recommend to me where could I learn (possibly youtube or courses/boot camps) the basics of Data Engineer so I can level up my expertise and stop feeling like a Jr?

Edit: i know i am a shitty reader so if there are some youtubers or videos i would really appreciate the info :))",82,24,Glittering-Branch-44,2023-01-09 00:01:17,https://www.reddit.com/r/dataengineering/comments/106ypk4/recommendations_for_having_fundamentasl_of_data/,False,False,False,False
yq0t6l,Introduction to Snowflake's Micro-Partitions,,86,9,ian-whitestone,2022-11-08 22:35:12,https://select.dev/posts/introduction-to-snowflake-micro-partitions,False,False,False,False
ya7bly,Whats something that you don’t understand but are too afraid to admit because you don’t want to look like an imposter?,Be honest. Hop on that burner if you need to.,85,98,burningburnerbern,2022-10-21 22:56:54,https://www.reddit.com/r/dataengineering/comments/ya7bly/whats_something_that_you_dont_understand_but_are/,False,False,False,False
y41mv5,How are you maintaining data fidelity throughout your data integration and transformation pipelines?,,84,9,tchungry,2022-10-14 18:34:58,https://i.redd.it/tj2get4aftt91.jpg,False,False,False,False
vc4b2y,"Is all data engineering moving into SQL warehouses, or is there still a need for general purpose programming languages and systems?","It seems that modern data warehouses, exemplified by Snowflake et al, are good at efficient data storage, retrieval and transformation of everything from unstructured to structured data. In addition, these warehouses automatically scale and distribute query execution. With tools like DBT, it also becomes possible to manage and compose transformations expressed as SQL.

If that's true, then what is the remaining role of general purpose programming languages (PLs), like Python, and distributed systems like Spark for scale? It seems that PLs are at a disadvantage wrt SQL because they are much harder to automatically parallelize/make efficient/scale. It seems that distributed systems are at a disadvantage because they are harder to manage, and need more fine-tuning to work well. (I don't mean just setup cost of the system itself, which can be offloaded to e.g. Amazon EMR, I mean in actual day to day usage).

It used to be that heavily SQL-based code was a terrible mess, but it seems DBT has helped a lot with that (disclaimer: I have little actual experience with DBT), so ""modularity"" or ""maintenance"" of SQL is also largely solved, i.e. is not such a big argument in favor of using a general purpose language anymore.

In 5 years, will the bulk of data engineering be done via dbt-orchestrated SQL of some sort? Or am I missing some important area/use case/problem?",81,44,IndifferentPenguins,2022-06-14 14:08:07,https://www.reddit.com/r/dataengineering/comments/vc4b2y/is_all_data_engineering_moving_into_sql/,False,False,False,False
rkhevl,What did you guys wish you knew before implementing everything for your company?,So I'm in charge of the dev/dataengineering/devops for our startup and it's daunting how many things need to be done. What did you wish you knew before building data pipelines and choosing datawarehouses/data lakes/delta lakes etc. Thanks guys.,84,30,zer0crash,2021-12-20 07:17:34,https://www.reddit.com/r/dataengineering/comments/rkhevl/what_did_you_guys_wish_you_knew_before/,False,False,False,False
pc2yne,"If you could go back in time and tell yourself one thing before building your first pipeline, what would that be?","I'm about to build my first pipeline and warehouse, the ETL is pretty darn straightforward in this case... rip the data out of the shitty accounting platform architecture into something reasonably sane.

I plan to treat this as a learning exercise because I'll be responsible for building almost all the infrastructure in this firm for the foreseeable future.

I was hired as an analyst, when the firm had no DE or infrastructure to speak of, so I've been learning how to wear that hat and enjoying it a lot actually. I'm working my way through Kleppman's DDIA.

I don't need someone to hold my hand but I am curious, what sorts of things would you have kept in mind or done differently if you could do your first pipeline over again?

Note that I am the sole IC on this project so tips on making it maintainable are highly relevant.",83,42,stackedhats,2021-08-26 16:25:18,https://www.reddit.com/r/dataengineering/comments/pc2yne/if_you_could_go_back_in_time_and_tell_yourself/,False,False,False,False
p6j1l1,Any mid-career coaches for people like me?,"I'm kind of a DE. I'm kind of a SQL developer. I'm kind of a visual report dev. I'm definitely a data modeler, and I'm also kind of a BA.  Can you tell my organization is under resourced?

In short, I do a lot of stuff, and I'm stretched way too thin. I feel a strong need to angle for depth over breadth because I'm painfully aware of how much better I could get at any of these things if I just had the opportunity to focus.  I'm concerned that my broad shallow skill set will make it hard to score a senior role in another organization if I decide to leave my current job, and I'm also afraid that I'll never be able to focus and really level up my skills in my current one. It's worth mentioning that I actually like all the things I do, and I would be perfectly happy focusing on any one or two of them, if it were only possible to make it happen. It's also worth mentioning that I am getting real close to burnout because of the chaos and overwork.

The last thing is - I work in a low-paying sector and I know I could make a lot more if I could figure out how to make a change (which means figuring out a path first). And also, I like my job and coworkers as much as I hate the shitshow we're all in. 

Are there coaches out there to help people like me decide what I should do?",82,45,DevonianAge,2021-08-18 03:20:51,https://www.reddit.com/r/dataengineering/comments/p6j1l1/any_midcareer_coaches_for_people_like_me/,False,False,False,False
n916sy,What are the Golden Rules of Data Engineering?,"Hey r/dataengineering,

I'm starting an entry level data engineering job soon, and I wondered what the community thought were the ""golden rules"" of the field. These could be as broad or specific as you like - I'm just trying to get some food for thought in advance of my first day.",83,63,P4004,2021-05-10 10:25:35,https://www.reddit.com/r/dataengineering/comments/n916sy/what_are_the_golden_rules_of_data_engineering/,False,False,False,False
n2jm1s,What are the most commond advanced SQL interview questions asked at FAANG?,I am going to have a data engineering role interview pretty and would like to know what are the most difficult advanced question they could ask for SQL? Could you please share your experience?,82,19,Born-Comment3359,2021-05-01 14:38:44,https://www.reddit.com/r/dataengineering/comments/n2jm1s/what_are_the_most_commond_advanced_sql_interview/,False,False,False,False
jb6ody,How Airbnb trained 5K+ Employees to use data in their day to day job,,84,6,Drkpwn,2020-10-14 18:39:52,https://blog.getcensus.com/how-airbnb-achieves-data-democratization-to-empower-5k-employees//?utm_source=reddit&utm_medium=post&utm_campaign=airbnbuniversity,False,False,False,False
14scpwd,Ibis: The last dataframe API you'll need to learn? I hope...,,84,50,bitsondatadev,2023-07-06 15:34:26,https://i.redd.it/om8sn7dv4dab1.jpg,False,False,False,False
14rqj5o,Any self-taught data engineers here who feel like there are a lot of people who are focused on discrediting you?,"I get the sense during interviews that some people are more focused on trying to figure out anything I don't know, rather than talking about skills that actually pertain to the role. 

A lot of times, people with traditional computer science backgrounds talk to me in interviews like I'm an ""exhibit"". Getting comments like, ""Oh, I've always wanted to meet a self-taught person"", or ""You've done well so far, for a self-taught engineer."" 

Then proceed to ask me questions about something oddly specific like linting, or data warehouse processing algorithms. 

I feel like I have to be ""beyond perfect"" in a lot of situations, and even then people will still waste my time, and go with someone with a more traditional computer science background. 

It's really exhausting, and is making me consider doing a post-bach or masters, just so I don't have to deal with this anymore. 

Context - I'm a senior data engineer who leads a team with a lot of members that have under-graduate or masters degrees in comp sci or data science. ",81,52,Capable-Jicama2155,2023-07-05 23:05:15,https://www.reddit.com/r/dataengineering/comments/14rqj5o/any_selftaught_data_engineers_here_who_feel_like/,False,False,False,False
116a03p,What's the toughest DE problem you faced in your work career?,*Hoping for an interesting thread*,85,107,priprocks,2023-02-19 13:13:50,https://www.reddit.com/r/dataengineering/comments/116a03p/whats_the_toughest_de_problem_you_faced_in_your/,False,False,False,False
10a07h5,Why Doesn’t the Modern Data Stack Result in a Modern Data Experience?,"Just wanted to get the community's take on this topic. Any thoughts?

*""The data landscape is exploding with tools.*

*As data professionals we have at our fingertips specialized tools for anything: from specialized databases (graph, geo, you name it) to tools for SQL-driven transformations (looking at you, dbt).* 

***Yet, a lot of data work is about provisioning, selecting, administering, and just maintaining those tools. Which is just a pain.""***

[https://www.keboola.com/blog/why-doesnt-the-modern-data-stack-result-in-a-modern-data-experience](https://www.keboola.com/blog/why-doesnt-the-modern-data-stack-result-in-a-modern-data-experience)",81,50,CalleKeboola,2023-01-12 13:55:04,https://www.reddit.com/r/dataengineering/comments/10a07h5/why_doesnt_the_modern_data_stack_result_in_a/,False,False,False,False
zo4ntl,What did you screw up this year,"I will go first. Yesterday I deployed my spark script and it overwrote the table I been loading for a while.

Just started a new project this week and this project use older version of Spark. It was supposed to only overwrite the partition but instead, the whole table was wiped. 

Who knows that dynamic partition overwrite only work after spark 2.3

Now I am spending part of the weekend rebuilding the table.",82,75,543254447,2022-12-17 11:46:42,https://www.reddit.com/r/dataengineering/comments/zo4ntl/what_did_you_screw_up_this_year/,False,False,False,False
wou44a,Data pipeline project for beginners,"I successfully created my first end-to-end automated data pipeline, that takes youtube data (from Kaggle) as its source, transforms and analysis the data, and creates a dashboard in snowsight( offered by snowflake). I have briefly explained the architecture in my GitHub project repository.

Link: [https://github.com/Nupurgopali/youtube\_data\_analysis](https://github.com/Nupurgopali/youtube_data_analysis)

Feel free to let me know any type of improvement that I could have done to the pipeline architecture!",83,7,Nupur90,2022-08-15 08:22:31,https://www.reddit.com/r/dataengineering/comments/wou44a/data_pipeline_project_for_beginners/,False,False,False,False
urk1vw,Appreciation❤️,"Hi guys, I’ve been trying to pivot careers and I had a data engineering class for my masters. I did really well, and I honestly had to thank this subreddit group because y’all helped me out so much. I really appreciate it, and I hope you guys continue to be the amazing and supportive group that you are.",81,63,ShayBae23EEE,2022-05-17 11:12:01,https://www.reddit.com/r/dataengineering/comments/urk1vw/appreciation/,False,False,False,False
ubghg8,Let’s study together!!,I want to be a data engineer and I’m at the basic level. Just learning sql buti need support. Who wants to form a study group?,81,91,Honest-Wolf2231,2022-04-25 08:46:47,https://www.reddit.com/r/dataengineering/comments/ubghg8/lets_study_together/,False,False,False,False
u6zarh,Biggest debates in the industry?,"I'm new to data engineering, and I'm curious about what people have different opinions about. There's only so much you can learn by searching Google...",81,137,kirkwoodj,2022-04-19 07:22:26,https://www.reddit.com/r/dataengineering/comments/u6zarh/biggest_debates_in_the_industry/,False,False,False,False
pfwuyg,Quarterly Salary Discussion,"This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering. Please comment below and include the following:

1. Current title

2. Years of experience (YOE)

3. Location

4. Base salary & currency (dollars, euro, pesos, etc.)

5. Bonuses/Equity (optional)

6. Industry (optional)",85,158,AutoModerator,2021-09-01 16:00:16,https://www.reddit.com/r/dataengineering/comments/pfwuyg/quarterly_salary_discussion/,False,False,False,False
n3h23y,Do most salary’s cap at around 120-150k?,"Noticing with SWE the salary can go to 300k-400k. 

Have not really been seeing that salary DE. 

Is it worth it to stay in this field long term? 

If you do have a salary that exceeds the 150k mark, can you disclose the YOE? 

Thanks",83,139,relentless_bull_,2021-05-02 21:59:49,https://www.reddit.com/r/dataengineering/comments/n3h23y/do_most_salarys_cap_at_around_120150k/,False,False,False,False
l53wql,Hot take: You (probably) don't need high availability,"Seriously, I know you love writing your Kubernetes YAML and configuring  net topologies for highly resilient clusters, but before you embark into a months-long quest for high availability, ask yourself, what happens if your jobs don't run for fifteen minutes, or one hour, or even four! What's the real impact for the business? Nine out of ten times, one hour of downtime will be a minor nuisance, and you can recover a simple system from a backup in ten minutes or less, specially if you document and automate properly. I have, in the past, and with even basic monitoring, your infrastructure team can handle it and only warn you after the fact.

But you didn't listen to some random guy on the web and built your robust, high-availability system with a Kafka queue for your jobs and dynamically allocated Kubernetes instances. Great. It will probably hum along for much longer without failures, but when it does fail, you won't fix it in fifteen minutes. It may take a day. Maybe three. You will have to search the web trying to match this cryptic error spit out five levels deep with an incomplete Stack Overflow answer. Again, I've been there. It wasn't fun.*

I'm not saying you should *never* go for a spiffy, high-availability setup, but consider the cost, make sure you are comfortable with the tech, and have detailed plans to recover from failure, because as Douglas Adams wisely said, ""The major difference between a thing that might go wrong and a thing that cannot possibly go wrong is that when a thing that cannot possibly go wrong goes wrong it usually turns out to be impossible to get at or repair.""

\* It was HA Hadoop, though. That was a few years before Kubernetes became a thing, and we were still on-premise.",85,31,sciencewarrior,2021-01-26 02:20:57,https://www.reddit.com/r/dataengineering/comments/l53wql/hot_take_you_probably_dont_need_high_availability/,False,False,False,False
1c3mmbi,Just thanks to you all here,"Just that thanks to everyone in this sub, I find it one of the most responsive and non-judgmental subreddit ever! I’ve learnt a lot from you guys",82,11,josejo9423,2024-04-14 06:11:33,https://www.reddit.com/r/dataengineering/comments/1c3mmbi/just_thanks_to_you_all_here/,False,False,False,False
1bh2wha,How to become a good engineer ?,"Hi everyone,  
I am seeking guidance how can i become good overall engineer ? What i can learn to be better ?

**Experience**: 3 years  
**Age**: 27  
**Current role**: I am working on low latency data pipelines using scala, flink, cassandra, redis, s3 and kafka in a product based company.  
Other tools i am using prometheus, grafana for monitoring and kubernetes, docker for deploying apps.  
**Leetcode rating**: My Leetcode rating is 1960 and currently practing more on it. solved around 1200 problems. Using cpp.  
**System Design**: Completed Design data intensive applicaion book and Grokking the system design course.  
**Low level Design**: working on it this too using cpp.

Recently I read  design data intensive application book, reading it once again to get more out of it.  Next i am thinking of reading one os book (Galvin), microservices by sam richardson book. I also work on understanding how cassandra redis and kafka works internally in the free time.

I didn't try to learn kuberentes and docker in detail because they are vast in terms of concepts. My current job does allow only to used them as platform for deploying apps and there are other engineers for devops work.

I want your guidance on what can i do to become a good successful engineer and  join good company in future. I am not thinking of switching in few months becomes i want grind more in coming months and my current job able me to provide good free time.  **Is anything needs to be changed or added to what i am doing currently ?** Any comment would be much appreciated. Thanks in advance.

Sorry for my bad english.",82,27,AggravatingParsnip89,2024-03-17 17:16:21,https://www.reddit.com/r/dataengineering/comments/1bh2wha/how_to_become_a_good_engineer/,False,False,False,False
1904k5j,DBT Testing for Lazy People: dbt-testgen,"[dbt-testgen](https://github.com/kgmcquate/dbt-testgen) is an open-source DBT package (maintained by me) that generates tests for your DBT models based on real data.

Tests and data quality checks are often skipped because of the time and energy required to write them. This DBT package is designed to save you that time.

Currently supports Snowflake, Databricks, RedShift, BigQuery, Postgres, and DuckDB, with test coverage for all 6.

Check out the examples on the GitHub page: [https://github.com/kgmcquate/dbt-testgen](https://github.com/kgmcquate/dbt-testgen). I'm looking for ideas, feedback, and contributors. Thanks all :)",80,21,fuzzh3d,2024-01-06 17:07:47,https://www.reddit.com/r/dataengineering/comments/1904k5j/dbt_testing_for_lazy_people_dbttestgen/,False,False,False,False
188grde,Quarterly Salary Discussion - Dec 2023,"https://preview.redd.it/ia7kdykk8dlb1.png?width=500&format=png&auto=webp&s=5cbb667f30e089119bae1fcb2922ffac0700aecd

This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering.

# [Submit your salary here](https://tally.so/r/nraYkN)

You can view and analyze all of the data on our [DE salary page](https://dataengineering.wiki/Community/Salaries) and get involved with this open-source project [here](https://github.com/data-engineering-community/data-engineering-salaries).

&#x200B;

If you'd like to share publicly as well you can comment on this thread using the template below but it will not be reflected in the dataset:

1. Current title
2. Years of experience (YOE)
3. Location
4. Base salary & currency (dollars, euro, pesos, etc.)
5. Bonuses/Equity (optional)
6. Industry (optional)
7. Tech stack (optional)",83,173,AutoModerator,2023-12-01 17:00:46,https://www.reddit.com/r/dataengineering/comments/188grde/quarterly_salary_discussion_dec_2023/,False,False,False,False
15trl9q,One company wants me to attend 5 interview rounds in 2 days. Even worth it?,"Hi there,

I already have a job and this position is paying 160k in MA area and is looking to conduct 5 interview rounds.

3 on one day starting 11 am until 2 pm. with three different individuals

and 

2 on another day from 10 until 12 with 2 other individuals.

&#x200B;

I had asked them to respect my time and have two one hour interviews but they sent this anyways.

I feel like just saying no to this. This is getting out of hand.

Unless they want to pay 300k, I feel like this would just waste of my time.",81,87,jerrie86,2023-08-17 15:59:08,https://www.reddit.com/r/dataengineering/comments/15trl9q/one_company_wants_me_to_attend_5_interview_rounds/,False,False,False,False
13tffsk,Company wanted both Databricks and Snowflake so we have them (Airflow for data orchestration). Any advice on how best leverage these two platforms?,"So I started in in fairly new role at a new company. The tech lead who just left decided that we would be using both databricks and snowflake. Those tools are now both stood up and ready for use, but we have no plan or best practices about how to leverage the strengths of both these platforms.

My boss wants us to definitely have an architecture that is using snowflake and databricks because he thinks it would be a bad look if we went down to one platform after paying/pitching for both.

My starting thought is use Databricks as the data lake and transformation layers. Push “gold” datasets over to snowflake that you would put in a data warehouse or something that would be fed off BI tool traditionally. Use Databricks also for exploratory analysis and research, machine learning, etc. Use Airflow as master orchestration layer.

Any help or thoughts appreciated! I’m kinda being forced into this decider role, which is cool, just polling for some community support :)",85,67,lezgomama,2023-05-27 19:05:22,https://www.reddit.com/r/dataengineering/comments/13tffsk/company_wanted_both_databricks_and_snowflake_so/,False,False,False,False
1331rzp,What tools do you “need” to do your job?,"My company uses a lot of cool stuff from the typical MDS but I’m honestly not sure if I have ever completed a ticket that truly “required” anything beyond SQL, a database vendor specific ETL tool, and maybe some python.  

I feel like most of the work I do is the result of bad business analysts and bad requirements gathering. Building data lakes, giant complicated models, calling API’s constantly, unpacking absurd amounts of flat files, all to build data pipelines to drown teams with “real time” data because that’s what they asked for, when they really just need a few KPI aggregated monthly.  

Wondering how common this is? To feel like everything you do is extreme overkill relative to the actual business problem that needs solving",81,34,None,2023-04-29 17:33:53,https://www.reddit.com/r/dataengineering/comments/1331rzp/what_tools_do_you_need_to_do_your_job/,False,False,False,False
1289p1k,Which Data jargon or concept did you have a hard time grasping?,"For me, I couldn't for a while fathom what *Data Mesh* and its analogue, *Data Product* meant. For a long time I thought ""Aren't all software products data products? I mean, what product doesn't use data! Why do we need a new terminology?""

After much reading, I got to know that the term *data product* is being used in a different sense altogether - to literally treat data itself as a product & the resulting activities that a domain team would do to ship it (governance, infrastructure etc). And the Data Mesh (to put it succinctly) is the substrate that arises organically when different teams start using each other's data products!

Are there any such data lingo that you found cryptic?",79,60,booleanhunter,2023-04-01 03:24:10,https://www.reddit.com/r/dataengineering/comments/1289p1k/which_data_jargon_or_concept_did_you_have_a_hard/,False,False,False,False
112dz5s,A short tutorial on running Spark with Jupyter using Docker,"Hi r/dataengineering

I wrote a [short tutorial on how to run PySpark in a Jupyter notebook](https://www.datain30.com/p/run-spark-with-jupyter-using-docker). Let me know if you try it out and are able to finish in under 30 minutes.",84,17,datain30,2023-02-14 19:33:39,https://www.reddit.com/r/dataengineering/comments/112dz5s/a_short_tutorial_on_running_spark_with_jupyter/,False,False,False,False
10ghhal,"Has anyone here transitioned from Data Scientist to Data Engineer? What was your motivation, and do you regret the move now, or are you happier as a Data Engineer?","I posted the same in r/datascience, but thought it would get a better traction here instead, since many of you are actual data engineers. The original post is [here](https://www.reddit.com/r/datascience/comments/10fpmi2/has_anyone_here_transitioned_from_data_scientist/?utm_source=share&utm_medium=web2x&context=3) for anyone curious.

So basically, I'm curious to hear from people who work as data engineers that transitioned from a data scientist. What made you switch and do you regret you decision? Or was it one of the best decisions for your career? Thanks!",80,35,None,2023-01-19 23:59:52,https://www.reddit.com/r/dataengineering/comments/10ghhal/has_anyone_here_transitioned_from_data_scientist/,False,False,False,False
102qkc6,What are some questions you would ask a DE Team in an interview to see how advanced the team is?,"Kind of flipping the typical question around. What best practices would you ask a DE Team you were interviewing with to know the maturity/skill level of the team?

I am thinking of technical questions but also how they structure their work?",83,19,Culpgrant21,2023-01-04 01:53:50,https://www.reddit.com/r/dataengineering/comments/102qkc6/what_are_some_questions_you_would_ask_a_de_team/,False,False,False,False
x1yski,Senior data engineers! What should junior data engineers know?,"Hi

What makes you a ""senior"" in the DE field?

Is it the way you program? 

Is it extensive knowledge of distributed systems?

Is it data governance or data provisioning?

What makes you a senior?",79,34,thatsadsid,2022-08-31 00:07:22,https://www.reddit.com/r/dataengineering/comments/x1yski/senior_data_engineers_what_should_junior_data/,False,False,False,False
v5i71i,"Design, Development and Deployment of a simple Data Pipeline",,81,5,ramses-coraspe,2022-06-05 17:30:16,https://coraspe-ramses.medium.com/design-development-and-deployment-of-a-simple-data-pipeline-6f1d59d0fd6a,False,False,False,False
ujpaxm,Designing a whole data infrastructure from scratch,"I'm a part-time Data engineer/analyst/architect/everything in an e-commerce grocery startup.
The responsibilities are taunting, however very interesting.
I have some experience in data engineering but I have very limited amount of resources to use. The higher management want to see the value added for the business at each step I take. Is there any guidelines to build a warehouse in a fast and easy to mantain way ?
The second question would be what's the best warehouse solution AWS Redshift, GCP bigquery or snowflake, in terms of cost and maintainability, taking into account that we are launching the app in a couple of weeks and we have 30-50 orders per day ?",84,73,Advanced-Strategy-21,2022-05-06 14:56:04,https://www.reddit.com/r/dataengineering/comments/ujpaxm/designing_a_whole_data_infrastructure_from_scratch/,False,False,False,False
t7hdf9,What made you struggle as a beginner?,"I am writing several “Explain like I am five” blogs and would love to feature some of the community’s questions.

What were the most difficult concepts when you were starting out?",82,39,foundersblock,2022-03-05 19:53:27,https://www.reddit.com/r/dataengineering/comments/t7hdf9/what_made_you_struggle_as_a_beginner/,False,False,False,False
r3au4g,Reasonable experience requirement,,83,21,oneequalsequalsone,2021-11-27 10:25:53,https://i.redd.it/bkkmrpfh74281.jpg,False,False,False,False
qoxm2r,"how did you learn about writing ""optimized"" and ""clean"" SQL queries?","so I have a background in DS/ML, and although I have had previous coursework in database design, that is where it ends and I never used SQL in my projects seriously (outside undergrad course projects). I have no issue writing queries for Hackerrank, interview questions but my problem is that from a code-review perspectives, I have received this feedback that my codes are verbose and not optimized. For example, it seems like I am using too much subqueries, join operations, etc. when it is not necessary in the context of the question.  

I am interviewing for DE positions (along with DS/ML positions) and I want to brush on my SQL skills to be able to write codes that are of production-level qualities. Any advice is appreciated",83,36,None,2021-11-07 20:55:37,https://www.reddit.com/r/dataengineering/comments/qoxm2r/how_did_you_learn_about_writing_optimized_and/,False,False,False,False
n6vmta,Suggested courses / material / books to start working as an DE,"Hi, I worked as a Data Analyst last year and I will start working as a Data Engineer.


In this new position I will work with Python, PySpark, SQL and Airflow, IoT platforms such as ThingWorx, container technologies and cloud solutions such as Azure, namely Data Lake and Machine Learning services.

My last year as a Data Analyst I worked daily with Python and some SQL.


I would like to know suggestions for courses / videos / books / material to learn more about PySpark and Apache Spark, Airflow and the rest of the technologies mentioned above.


Thank you very much

EDIT: I have recently bought the book ""Designing Data-Intensive Applications"" and know about the ""Awesome Data Engineering"" github (https://github.com/igorbarinov/awesome-data-engineering)",83,30,D1yzz,2021-05-07 10:45:31,https://www.reddit.com/r/dataengineering/comments/n6vmta/suggested_courses_material_books_to_start_working/,False,False,False,False
ma51fb,Spark (Python particularly) learning materials,"Hello!!

Does anyone have some recommendations on how to learn Spark? I'm thinking udemy/coursera. I've already done awsdojo and that was super useful to get set up but I want to really dive in.

At the moment I'm essentially just porting python to pyspark which works, but probably isn't going to be the best way of doing things",80,17,sieltigre,2021-03-21 20:13:56,https://www.reddit.com/r/dataengineering/comments/ma51fb/spark_python_particularly_learning_materials/,False,False,False,False
1bsmfsq,Celebrating my first Data Engineering Project,"Hey everyone!

After dedicating over 6 years to software engineering, I've decided to pivot my career to data engineering. Recently, I took part in the Data Engineering Zoomcamp Cohort 2024, and I'm thrilled to share my first data engineering project with you all. I'd love to celebrate this milestone and hear your feedback!

[https://github.com/iamraphson/DE-2024-project-book-recommendation](https://github.com/iamraphson/DE-2024-project-book-recommendation)  
[https://github.com/iamraphson/DE-2024-project-spotify](https://github.com/iamraphson/DE-2024-project-spotify)

Feel free to star and contribute to the project.

The main goal of this project was to apply the various technologies I learned during the course and use them to create a comprehensive data engineering project for my personal growth and learning.

Here's a quick overview of the project:

* Implemented an end-to-end data pipeline using Python.
* Fetched dataset from Kaggle.
* Automated infrastructure setup with Terraform.
* Orchestrated workflow with Airflow
* Deployed on Google Cloud Platform (BigQuery and Cloud Storage).
* Created visualizations dashboard in Metabase.

Looking for  job opportunities in data engineering

Cheers to new beginnings! 🚀  


&#x200B;

https://preview.redd.it/3zu86f0qtqrc1.png?width=1043&format=png&auto=webp&s=5a882ad736755af0f388a14d7f8758e2db3e87f3

&#x200B;",81,29,Imaginary_Split520,2024-03-31 22:08:52,https://www.reddit.com/r/dataengineering/comments/1bsmfsq/celebrating_my_first_data_engineering_project/,False,False,False,False
1b26kf9,Favorite SQL patterns?,What are the SQL patterns you use on a regular basis and why?,82,136,AMDataLake,2024-02-28 13:53:22,https://www.reddit.com/r/dataengineering/comments/1b26kf9/favorite_sql_patterns/,False,False,False,False
1asegcy,Blog 1 - Structured Way to Study and Get into Azure DE role,"There is a lot of chaos in DE field with so many tech stacks and alternatives available it gets overwhelming so the purpose of this blog is to simplify just that.

**Tech Stack Needed:**

1. SQL
2. Azure Data Factory (ADF)
3. Spark Theoretical Knowledge
4. Python (On a basic level)
5. PySpark (Java and Scala Variants will also do)
6. Power BI (Optional, some companies ask but it's not a mandatory must know thing, you'll be fine even if you don't know)

The tech stack I mentioned above is the order in which I feel you should learn things and you will find the reason about that below along with that let's also see what we'll be using those components for to get an idea about how much time we should spend studying them.

**Tech Stack Use Cases and no. of days to be spent learning:**

1. **SQL**: SQL is the core of DE, whatever transformations you are going to do, even if you are using pyspark, you will need to know SQL. So I will recommend solving at least 1 SQL problem everyday and really understand the logic behind them, trust me good query writing skills in SQL is a must! **\[No. of days to learn: Keep practicing till you get a new job\]**  

2. **ADF**: This will be used just as an orchestration tool, so I will recommend just going through the videos initially, understand high level concepts like Integration runtime, linked services, datasets, activities, trigger types, parameterization of flow and on a very high level get an idea about the different relevant activities available. I highly recommend not going through the data flow videos as almost no one uses them or asks about them, so you'll be wasting your time.**\[No. of days to learn: Initially 1-2 weeks should be enough to get a high level understanding\]**  

3. **Spark Theoretical Knowledge**: Your entire big data flow will be handled by spark and its clusters so understanding how spark internal works is more important before learning how to write queries in pyspark. Concepts such as spark architecture, catalyst optimizer, AQE, data skew and how to handle it, join strategies, how to optimize or troubleshoot long running queries are a must know for you to clear your interviews. **\[No. of days to learn: 2-3 weeks\]**  

4. **Python**: You do not need to know OOP or have a excellent hand at writing code, but basic things like functions, variables, loops, inbuilt data structures like list, tuple, dictionary, set are a must know. Solving string and list based question should also be done on a regular basis. After that we can move on to some modules, file handling, exception handling, etc. **\[No. of days to learn: 2 weeks\]**  

5. **PySpark**: Finally start writing queries in pyspark. It's almost SQL just with a couple of dot notations so once you get familiar with syntax and after couple of days of writing queries in this you should be comfortable working in it. **\[No. of days to learn: 2 weeks\]**  

6. **Other Components**: CI/CD, DataBricks, ADLS, monitoring, etc, this can be covered on ad hoc basis and I'll make a detailed post on this later.

Please note the number of days mentioned will vary for each individual and this is just a high level plan to get you comfortable with the components. Once you are comfortable you will need to revise and practice so you don't forget things and feel really comfortable. Also, this blog is just an overview at a very high level, I will get into details of each component along with resources in the upcoming blogs.

&#x200B;

Bonus: [https://www.youtube.com/@TybulOnAzure](https://www.youtube.com/@TybulOnAzure)Above channel is a gold mine for data engineers, it may be a DP-203 playlist but his videos will be of immense help as he really teaches things on a grass root level so highly recommend following him.

[Original Post link to get to other blogs](https://www.reddit.com/r/dataengineering/comments/1arpamc/guiding_others_to_transition_into_azure_de_role/?utm_source=share&utm_medium=web2x&context=3)

&#x200B;

Please do let me know how you felt about this blog, if there are any improvements you would like to see or if there is anything you would like me to post about.

Thank You..!!",82,48,Vikinghehe,2024-02-16 17:35:12,https://www.reddit.com/r/dataengineering/comments/1asegcy/blog_1_structured_way_to_study_and_get_into_azure/,False,False,False,False
170rvz3,"Is the reason SFTP file transfer in banks, healthcare, etc is so ubiquitous just because they are using older systems without robust REST APIs...?","Apologies as my background isn't in DE but come more from BI systems administration. In my career I've seen a lot but the craziest thing I've seen is business users using email heavily to send Excel and CSV data dumps using BI tools...at a huge scale. Of course there are also some secure file transfers out there, stuff you would see with MoveIT and IBM Sterling, etc. Both of these models beg the question to me which is -- why do we need this?

Is it literally just because our system doesn't have a reliable REST API to pull the data easily? Is REST that insecure? So instead we rely on the other party generating a CSV and sending it to us in batch like it's 1995? Couldn't a REST API send a CSV over HTTPS anyway? Do we not trust it?

Is data integration really that dirty? What can be done to eliminate these file transfer overhead monsters? Is progress with good REST APIs just moving that slow?

I feel like there's something I'm fundamentally misunderstanding but not sure what it is...",84,66,TheWikiJedi,2023-10-05 20:07:20,https://www.reddit.com/r/dataengineering/comments/170rvz3/is_the_reason_sftp_file_transfer_in_banks/,False,False,False,False
15yz47k,Do you truly enjoy data engineering?,"Just reflecting on my career and how fortunate I feel to have found something I really enjoy doing. 

I just think data is so important to civilization, and being able to build proper data pipelines, scalable infrastructure, and accessible data warehousing feels like my calling. 

Granted I’ve only been doing this for 5 years, and obviously some days it just feels like work. Anyone feel the same or feel differently?",80,74,Justanotherguy2022,2023-08-23 09:49:22,https://www.reddit.com/r/dataengineering/comments/15yz47k/do_you_truly_enjoy_data_engineering/,False,False,False,False
13v6dku,Is your boss technical in that he/she can help you with difficult problems or non technical?,"Is you boss someone who can help you with technical stuff or do they just manage you and you team from above? I'm just asking because my boss is completely non technical and sometimes its not an issue, but other times it would be nice to have someone who understands what his team is doing. It prevents hours wasted trying to explain something rather than a quick call or maybe an email. More importantly, it helps when you're trying to explain ""why"" something can't be done or why something might take longer than expected when you're dealing with someone who understands certain technical aspects of your job.",81,41,Significant-Flow5900,2023-05-29 20:58:56,https://www.reddit.com/r/dataengineering/comments/13v6dku/is_your_boss_technical_in_that_heshe_can_help_you/,False,False,False,False
zsprzt,"Organization wants to use SharePoint as a ""database""","So my organization has had O365 with the power platform for a few years with a handful of individuals getting really good at treating SharePoint as a database for there power apps(""front-end"") and power bi reports with power automate work around. The company doesn't want to invest in a proper database like SQL server or full Datavers.

I think this is a recipe for disaster and would like to get options for or against continue using SharePoint as a ""database"".",84,90,Benmagz,2022-12-22 16:12:23,https://www.reddit.com/r/dataengineering/comments/zsprzt/organization_wants_to_use_sharepoint_as_a_database/,False,False,False,False
sonq4y,Do you like your job as a DE?,"Do you like what you do? 

I’m debating switching careers.

I’m currently an analyst and realized idgaf about actually analyzing the data 😂",83,62,None,2022-02-09 20:56:49,https://www.reddit.com/r/dataengineering/comments/sonq4y/do_you_like_your_job_as_a_de/,False,False,False,False
pgsv68,"We Don't Need Data Engineers, Data Scientist Need Better Tools - A Data Engineers Response To A Data Scientist","A few weeks ago I came across an article titled ""[We Don't Need Data Engineers, Data Scientists Need Better Tools](https://towardsdatascience.com/we-dont-need-data-engineers-we-need-better-tools-for-data-scientists-84a06e6f3f7f)"".

Now, reading the article, this title was somewhat clickbait. The author was focusing more on the end portion the data workflow. Where data scientists sometimes need data engineers or ML engineers to implement their code.

I have many times ""productionized"" data scientist's logic.

So I get what they were going for. However, I also feel like they drastically minimized the role of a data engineer to being a support role of a data scientist vs. a key component for most businesses.

Regardless if they have a data science team or not.

I decided to respond to this article with my own content titled. 

[We Don't Need Data Engineers, Just Better Tools - A Data Engineer Responds To A Data Scientist](https://www.youtube.com/watch?v=z_sR2CVG8kk)

But I would like to know your thoughts? Do you agree with the authors opinion or do you see data engineers having a much larger role.",81,35,nonkeymn,2021-09-02 23:05:35,https://www.reddit.com/r/dataengineering/comments/pgsv68/we_dont_need_data_engineers_data_scientist_need/,False,False,False,False
oipivt,I LOVE BEING A DE. But I also hate it..,"If I wasn't paid so handsomely, I'd think twice about being a DE.  Maybe move onto Program Management or something.  I consider myself a pragmatic programmer: therefore am easily able to get out of pickles here and there.  But good God, do I hate being the bloody single point of failure for so many data assets (work in a small ""agile"" team).

You know.. thinking on your feet and crafting replies that would elicit ""tipping of the hat"" by politicians (to make sure your stakeholders leave you alone and STFU while you have to hose the fire).

I know this is dependent on team/org/company, but having been in various DE roles, stress will always be a part of the job.  For folks who are experienced:  does this ever get any better?  Or do you just develop a mentality of ""leaning in"" - e.g. leaning into stress/the job/the bullshit?",80,29,que_wut,2021-07-12 11:40:51,https://www.reddit.com/r/dataengineering/comments/oipivt/i_love_being_a_de_but_i_also_hate_it/,False,False,False,False
nbua2l,ETL Pipelines Learning Resources,"I’ve been an Azure Cloud engineer for a bit and starting to get into big data in the hopes of making a move into a Data Engineering role. Anybody have any solid learning resources on properly designing end to end ETL pipelines using Spark/Databricks + orchestration tools + whatever else goes into making a production-ready ETL pipe?

EDIT for clarity:

So in software engineering there are design patterns and most fields have some sort of best practices. How would I learn these sort of things specifically for data engineering? Is there a learning resource that would help me understand how to approach various scenarios that require ETL in whatever shape and what the best practices in those scenarios would be?",80,32,dostoyevsky01,2021-05-13 23:15:16,https://www.reddit.com/r/dataengineering/comments/nbua2l/etl_pipelines_learning_resources/,False,False,False,False
bs2a7p,Udacity Data Engineering Nanodegree Course Review,"# Overview

Udacity's new Data Engineering Nanodegree. The course is broken up into five sections, Data Modeling, Cloud Data Warehouses, Data Lake with Spark, Data Pipelines with Airflow, and a capstone project. Each section has different instructors, with each one bringing a different teaching style in a way that keeps things refreshing while still keeping you wondering if it happened simply due to lack of communication. The structure for each section consists of introducing concepts through lectures, reinforcing the material with demos and exercises (typically in a Jupyter Notebook), and concludes with 1-2 project(s) dealing with designing an ETL process using song data for an imaginary company called Sparkify.

# My background

I have about two years of professional experience wrangling data with Python and SQL and about a year and a half of web development experience. I have a bachelors degree in engineering and took a few introductory computer science courses. A few months ago I completed [Dataquest's Data Engineering Path](https://www.dataquest.io/path/data-engineer/) and have taken a few [DataCamp](https://www.datacamp.com/) courses as well as [CS50](https://www.edx.org/course/cs50s-introduction-to-computer-science) and [CS50 Web](https://cs50.harvard.edu/web/2019/spring/). I enrolled in this course due to its focus on cloud technologies, which I have been learning through trial by fire at a data engineering job I started a few months ago, mostly using AWS, Postgres, Python, and Airflow.

# Individual Sections Review

## Data Modeling

This section introduces what data modeling is, why it's important, and what the differences between a relational and NoSQL database are. It speaks on important concepts such as ACID transactions, what fact and dimensional tables are, and what the difference between star and snowflake schemas is. This section uses Postgres and Apache Cassandra and consists of a project for each of them where you design schemas and load song logs and song metadata into fact and dimension tables.

Pros:

* Introduces most of the Postgres and Apache Cassandra commands a data engineer would probably ever use
* Provides a good explanation on when you'd want to use SQL vs. NoSQL

Cons:

* Most lectures consisted of watching the lecturer read slides off her laptop
* This section's exercises seemed to have more bugs than the rest
* There were a few questionable practices in this section such as a try / except block around everything and always inserting rows individually instead of in bulk

## Cloud Data Warehouses

This section builds on the previous section and explains the need for a data warehouse and what the benefits of hosting it in the cloud are. AWS basics such as IAM, creating an EC2 instance, and security groups are introduced, as well as a brief introduction to infrastructure as code using boto3. Other concepts such as OLAP cubes, rollup, drill-down, grouping sets, and columnar storage are discussed. The project consists of designing tables in Redshift and loading data from S3 to Redshift.

Pros:

* Provides practical example exercises such as loading S3 files in bulk to tables in Redshift using the COPY command
* Makes creating a sandbox data warehouse environment much more approachable. Prior to this I always thought it would be too expensive and complicated to build one on my own and this section proved me wrong

Cons:

* Tries to cover too much ground. Topics like infrastructure as code are glimpsed over and overly simplified

## Data Lakes with Spark

Introduces what big data is and why big data tools like Hadoop and Spark are necessary. Provides a conceptual overview of how distributed systems like Hadoop and Spark work. Hands-on exercises consist of using PySpark to wrangle data. Explanations of why an organization may need a data lake instead of a data warehouse are provided. The project consists of ingesting raw S3 files, creating fact and dimension tables, partitioning them and writing them back to S3 all with PySpark.

Pros:

* Provides an excellent explanation on how distributed file systems and cluster computing works
* Gives a good explanation on when to use PySpark data frames vs PySpark SQL and how to port your data between the two styles

Cons:

* This project involved filling in a lot more blanks than the rest of the projects and I found it to be particularly time-consuming. The number of files to ingest from S3 seemed too large to run in a reasonable amount of time
* I wish it would have included more information and exercises about using PySpark on a cluster of machines instead of on a single local one

## Data Pipelines with Airflow

Data pipelines, DAGs, and Airflow concepts such as operators, sensors, and plugins introduced. The final project involves using Airflow to load S3 files into partitioned Redshift tables and perform data quality checks afterward.

Pros:

* The only tutorial I've found on how to use data quality checks with Airflow. I've started using this technique at work and it is a game changer
* Airflow is a bitch to deploy and someone they engineered a way for people to run it on Udacity's workspaces. Kudos to the engineers on that

Cons:

* This section felt a bit shorter and was more focused around a specific technology than the other sections. Not necessarily a con but I would have liked to have the lectures be more generalized around the concepts of a data pipeline

## Overall

Overall, I really enjoyed this nanodegree and learned a lot of practical things from it that I have already started using at my job. I would estimate I spent about 40 hours completing it so I definitely felt short-changed in content and think it is incredibly overpriced for what it is. What I don't like is how Udacity markets their courses as a way for someone to make a career change with no real-world experience. I find that incredibly hard to believe and can't imagine a company hiring someone with no real world experience after completing this nanodegree. I found the content to mostly be of very high quality and I think this is really the only intermediate-advanced data engineering course out there. If you have the cash and are interested in learning data engineering in the cloud I would highly recommend it.",77,32,WannaBeGISGuru,2019-05-23 12:23:04,https://www.reddit.com/r/dataengineering/comments/bs2a7p/udacity_data_engineering_nanodegree_course_review/,False,False,False,False
1bklthc,When NOT to use PostgreSQL? ,"Hello, I'm on my way to learn DE, and this sub has helped me a lot to understand better many concepts, but one thing I see very often is that Postgres seems to be the best DWH in most cases, so my question is, which are those cases where Postgres is NOT recommended? What alternatives are better suited for that situation and Why? 

Thank you! ",78,47,None,2024-03-22 00:09:41,https://www.reddit.com/r/dataengineering/comments/1bklthc/when_not_to_use_postgresql/,False,False,False,False
1asim63,How do you prep for SQL heavy technical rounds?,"Leetcode SQL problems? Review concepts like window functions, joins, etc?

I wrote tons of SQL in a past job but it was data modeling heavy and more ""practical"" than what these interview seem to be asking.",80,38,Firm_Bit,2024-02-16 20:25:19,https://www.reddit.com/r/dataengineering/comments/1asim63/how_do_you_prep_for_sql_heavy_technical_rounds/,False,False,False,False
18vdch8,Should I be offended? Project manager send me a code from Chatgpt,"I'm working on multiple things at the same time and last week a PM added some tasks and was pushy about it but other priorities are taking place, all the sudden he emails me a python code and asked me just to schedule it. I don't know how to react to this situation, and the code he sent is flawless, I'm at the point that I feel I can easily get replaced. Wanted to vent out with fellow DEs. What would you do if you were in my position?",81,110,Zack-s21,2023-12-31 18:05:45,https://www.reddit.com/r/dataengineering/comments/18vdch8/should_i_be_offended_project_manager_send_me_a/,False,False,False,False
17heehr,How to earn the big bucks in Data Engineering?,"I have been a data analyst for the last 2 years and just managed to land a decent mid level data engineering (Thanks to my DP - 203 certification) role, which is way less compared to what I can achieve in this field. I am not complaining but I just want to know what would be required off me to get paid the big bucks.

I wanted to know what would be expected of me if I were to apply for a senior data engineering roles? Asking just so I could focus on those areas once I commence at my new role. Also, any certifications you guys would suggest? I see videos on Instagram where people claim to earn 450k as a data engineer.",80,107,EmergencyHot2604,2023-10-27 03:24:32,https://www.reddit.com/r/dataengineering/comments/17heehr/how_to_earn_the_big_bucks_in_data_engineering/,False,False,False,False
17aadnb,"I've got a DE Interview, but they're not letting me use libraries or SQL?","I applied for a DE role. After the first screening, HR shared some of the next steps in the recruiting process, among which is a technical interview. Their e-mail says they'll have me do ""big data"" code challenges, and explicitly states that SQL, Pandas, etc. won't be allowed (only default Python). 

I'm honestly confused as most interviews I've had expected me to use data-related technologies, I find it odd that they'd explicitly exclude them. Has anyone encountered a similar situation?

Maybe I'm reading too much into this, and their description was just a weird way of saying ""expect standard data structures leetcode""?",80,100,arkoftheconvenient,2023-10-17 22:16:02,https://www.reddit.com/r/dataengineering/comments/17aadnb/ive_got_a_de_interview_but_theyre_not_letting_me/,False,False,False,False
160xwua,interview: this a red flag?,"During an interview for a Sr. DE role, the team lead told me:

""In this role, you will be using X,Y,Z technologies which you are not familiar with. This is an urgent position, and you will be expected to hit the ground running and deliver. There will be no KT. Will you be comfortable in this situation? I want to be transparent with you and not hide anything.""

I took this personally as a red flag for me, given how I am not familiar with the tech stack and I interpreted their comments as me possibly not being given ramp up time to get familiar with the tools.

Thoughts? Should I flee?

EDIT: Data Engineer role, not Data Analyst. Company has +60K employees. Tools in question are for migrations from on-prem to cloud.",81,61,_Vion_,2023-08-25 12:29:36,https://www.reddit.com/r/dataengineering/comments/160xwua/interview_this_a_red_flag/,False,False,False,False
14irwgo,"I want to practice my skills in Airflow, AWS cloud provider, and Snowflake by working on a project, and I came across this project pipeline. Do you think this is a good project to enhance my skills and include on my resume? I am aiming to find an end-of-study internship at a reputable company.",,79,22,Kratos_1412,2023-06-25 17:29:12,https://i.redd.it/jl512r4m878b1.jpg,False,False,False,False
14b5t87,How old were you when you landed your first real data engineering job?,I’m going to guess early to mid 20s.,83,150,SeriouslySally36,2023-06-16 19:33:29,https://www.reddit.com/r/dataengineering/comments/14b5t87/how_old_were_you_when_you_landed_your_first_real/,False,False,False,False
13cb3xl,Future of DE,"Folks in the data engineering community like Zach Wilson have expressed that DE will change within the next 5 years or so because of AI, where the responsibilities of a DE will basically go into 2 buckets. One more focused on the business like data analysts and the other more technical like software engineers. I have a background in analytics and really thought that would lead to my first full time job, but I naturally got more and more into data engineering as I learned more. Do you think it’s still worth it, for someone like me who got offered an entry level Data Engineering role, to stay in this field of Data Engineering or should I start looking into Data Analyst type roles? 

My reasoning to accept the role would be to gain a lot of technical and business knowledge. Plus having a background in analytics and dashboards would also improve my skill set. Do you guys have any advice/career tips?",80,60,WorldlyDirt5024,2023-05-09 01:20:36,https://www.reddit.com/r/dataengineering/comments/13cb3xl/future_of_de/,False,False,False,False
11oml2t,"Are there any other platforms like Kaggle, but for data engineers instead of data scientists?","I see lots of competitions on data science specially on Kaggle, but i didn’t find anything for data engineer. I want to work on datasets to improve my Data Engineering skills. Is there any platform for data engineers, if no, how can I use kaggle to learn data engineering.

Thank you in advance",80,35,Delicious_Attempt_99,2023-03-11 15:14:07,https://www.reddit.com/r/dataengineering/comments/11oml2t/are_there_any_other_platforms_like_kaggle_but_for/,False,False,False,False
115vbvp,Boss wants a data mesh,"My boss heard talk of “data meshes” at a conference and now thinks the future of our org depends on us building one now (we currently have a data warehouse and s3 lake for our data needs). Our organization currently has a centralized team of 4 data engineers and one architect.

Our conversation:

“So we’re giving ownership of data engineering and architecture to the business units?” “No, there’ll still be just the central data team continuing to fulfill requests in a round-robin fashion.”

“Why are we doing this?” “Because it’s better and we want to be forward thinking.”

Am I missing something here about the magic of data meshes if you’re not actually transferring data ownership to the business units? Isn’t that kinda the point?",80,41,demost11,2023-02-18 23:26:32,https://www.reddit.com/r/dataengineering/comments/115vbvp/boss_wants_a_data_mesh/,False,False,False,False
wby0c0,HUGE imposter syndrome as a junior. What do I do?,"I got hired as a junior data engineer 2 months ago after completing my bootcamp. For the past two months I've been doing Datacamp modules set out by my company. They were pretty chilled and straightforward.

This week I was moved to a team working on a project. OMG that was a difficult experience. I'm not the best coder, but I tried. I was given my first ticket and I really struggled, even on the basic of things. I can sense my senior just getting annoyed and pissed at me.

I can't help but feel I'm a burden/nuisance to my team. My first PR was full of mistakes, and honestly, I've lost my sleep. My heart literally hurts thinking I'm way out my depth.

I spoke to my seniors and asked them what they expected of me, they replied with ""magnificence"". Over the next couple of months I've been tasked with completing my AWS certificate and Terraform stuff.

Should I be feeling like this, is this normal? What advice do you have to improve my situation?",79,56,None,2022-07-30 13:51:14,https://www.reddit.com/r/dataengineering/comments/wby0c0/huge_imposter_syndrome_as_a_junior_what_do_i_do/,False,False,False,False
w5xbku,What should a data engineer’s GitHub look like?,Please share examples or descriptions.,79,36,Delicious-Cicada9307,2022-07-23 06:48:15,https://www.reddit.com/r/dataengineering/comments/w5xbku/what_should_a_data_engineers_github_look_like/,False,False,False,False
vnbgc4,"Where can I get the hot or latest news about data lakehouse or dataengineering？ Are there blogs, websites, or media to recommend it?","I don't know where to get the information about data lakehouse, such as news, latest technology, industry analysis, etc. For example, the news about Google JAX is hot these days, but where can I get this information? I read about it on Reddit, but what blogs or news sites are they reading about it from? Could you recommend me some dataengineering or technology news websites or blogs? Thank you!",82,17,qazmkopp,2022-06-29 09:14:12,https://www.reddit.com/r/dataengineering/comments/vnbgc4/where_can_i_get_the_hot_or_latest_news_about_data/,False,False,False,False
trqki4,Is it worth me applying to Data Engineering roles with this Resume/CV?,,81,114,None,2022-03-29 22:21:13,https://i.redd.it/a0c0or20aeq81.png,False,False,False,False
qqec8x,"Designing a Data Platform, when to choose Databricks over other DWH tools","Hi all:

I'm currently designing the data platform of my company and I have seen the following article that Databricks has shared in their social media:

[https://blog.denexus.io/databricks](https://blog.denexus.io/databricks)

With a description on why they have decided to use Databricks and their data lakehouse approach rather than other data warehouse based tools. 

According to what its said: if you are going to do a lot of machine learning, if your data could be of different types or formats and if your data volume is going to scale, the best option in the market seems to be Databricks.

What do you think? Did you have a similar scenario and used another solution? Why?",83,64,ichacas,2021-11-09 21:54:03,https://www.reddit.com/r/dataengineering/comments/qqec8x/designing_a_data_platform_when_to_choose/,False,False,False,False
pyr9vx,Your default tool for ETL,"For both small and big data. Whats your preferred tool for running ETL processes, and why?",83,137,scraper01,2021-09-30 19:43:19,https://www.reddit.com/r/dataengineering/comments/pyr9vx/your_default_tool_for_etl/,False,False,False,False
pl3pos,I'm a Data Engineer. How do I become a better Software Engineer?,"I became a DE after working in analytics and gravitating towards building infrastructure. I work with the modern data stack, so I'm comfortable building data pipelines using tools like Airflow and dbt. I've also worked on CI/CD, data quality, and observability within data.

However, I still have a large skill/knowledge gap when it comes to software engineering fundamentals.      I can write DAGs and data models, but when it comes to spinning up new services, provisioning infrastructure, or debugging complex software issues, I'm much less comfortable.

I recognize that since I can code and know my way around the stack, I'm starting from a good baseline. For someone with me background, what is the best way to learn software engineering?",81,37,TheLoveBoat,2021-09-09 18:35:38,https://www.reddit.com/r/dataengineering/comments/pl3pos/im_a_data_engineer_how_do_i_become_a_better/,False,False,False,False
pkwx94,Is this group moderated actively? Can it be improved?,"As the title says. I love this sub, but not sure if it's just me, but everyday there are new posts created with people asking same questions (I.e. how to get in, get a job, doing x,y,z what are my chances) which creates a lot of unnecessary spam. 
Are mods looking into organising the sub and moderating it to avoid this? For example, create weekly/daily thread of 1. Help on Tasks; 2. Help on entering field, 3. Learning resources, 4. Leetcode problems, etc. 

I am happy to help the mods (or join them) if help is wanted/needed. Or am I missing the point somewhere and my expectations are not relevant here?",80,45,Vabaluba,2021-09-09 12:34:45,https://www.reddit.com/r/dataengineering/comments/pkwx94/is_this_group_moderated_actively_can_it_be/,False,False,False,False
p9uvp9,Trigger a data engineer with one sentence ? ( Fun ),Just wanted to try this trend in here. Let's see how it turns out.,81,144,mouhcineTo1,2021-08-23 07:35:21,https://www.reddit.com/r/dataengineering/comments/p9uvp9/trigger_a_data_engineer_with_one_sentence_fun/,False,False,False,False
lih2xg,“Data Engineering is the new Data Science” DS Interviews: -15% DE Interviews: +40% 🚀🚀🚀,,80,29,Suitable-Chemistry-9,2021-02-12 18:18:35,https://blog-interviewquery-com.cdn.ampproject.org/c/s/blog.interviewquery.com/blog-data-science-interview-report/amp/,False,False,False,False
kv1vc5,How To Become a Data Engineer in 2021,,80,17,adilkhash,2021-01-11 12:41:00,https://khashtamov.com/en/how-to-become-a-data-engineer/,False,False,False,False
1bf2ntf,Steam Prices ETL (Personal Project),"Hello everyone. I have been working on a personal project regarding data engineering. This project has to do with retrieving steam games prices for different games in different countries, and plotting the price difference in a world map.

This project is made up of 2 ETLs: One that retrieves price data and the other plots it using a world map.

I would like some feedback on what I couldve done better. I tried using design pattern builder, using abstractions for different external resources and parametrization with Yaml.

This project uses 3 APIs and an S3 bucket for its internal processing.

[here you have the project link](https://github.com/edseldim/steam_prices_data_engineering)

This is the final result

https://preview.redd.it/139jgrlemeoc1.png?width=824&format=png&auto=webp&s=204769ad503885eef0153d565f9243e5c5f56add",79,16,Confident_Watch8207,2024-03-15 01:56:56,https://www.reddit.com/r/dataengineering/comments/1bf2ntf/steam_prices_etl_personal_project/,False,False,False,False
1ac9fpf,What was your DE job from hell?,"I used to work at an extremely big company (actually one of the biggest corps in the world) as a data engineer. Their offer was great, great salary, fully remote and other nice perks.

I joined in and my first thought was how incredibly disorganized everything was. There wasn't even like an internal tooling to check your organization members. I only knew who my direct boss was, but I had no idea about any further hierarchy or even who my other teammates were.

It took them literally around a month and a half for them to assign work to me. Despite being part of an established team, I was ""loaned"" to another team led by a contractor and this guy was completely unaware about my capacities, so he was not able to properly assign work to me because he just couldn't find a task that fit me. The work I was assigned was literally just writing view definitions with SELECTs and JOINs. They'd give me a humongous Excel listing the columns they wanted, and their datatypes, but it was a major PITA because the amount of columns was huge, and every single one of them had an encoded name, so I was essentially modeling data from a blackbox. I NEVER knew what this data was, who was its consumer, or what could they do with it.

To this day, I'm still not sure what these people were doing, as in, what was being developed? What did the pipelines process? Were there even pipelines? No clue, all I I did was looking at an Impala UI and writing SELECT.

Needless to say, I left pretty fast. Happy to be at a place where you're actually aware of what is going on.",78,37,yourAvgSE,2024-01-27 12:05:53,https://www.reddit.com/r/dataengineering/comments/1ac9fpf/what_was_your_de_job_from_hell/,False,False,False,False
18i9w4x,Small Group of Data Engineering Learners,"Hello guys!

I am making a small group of people learning data engineering where we get on a call together every other week and talk about tools we're learning and other DE-related things. This will be good for everyone in the group to get better at DE and help each other out when needed. 

Thanks, and happy learning to everyone!

Edit: If more of you are interested consider making small groups with each other.

Edit, again: If you are still interested please reach out to other people who want to make groups.",79,156,RepresentativePen297,2023-12-14 14:31:41,https://www.reddit.com/r/dataengineering/comments/18i9w4x/small_group_of_data_engineering_learners/,False,False,False,False
1860n25,What do data engineers want for christmas?,What do data engineers want for christmas? ,79,131,RadioDramatic3040,2023-11-28 16:52:17,https://www.reddit.com/r/dataengineering/comments/1860n25/what_do_data_engineers_want_for_christmas/,False,False,False,False
17jg1x5,i'm seeing less star schemas,"Lately, i've done a few jobs consulting and one thing I found is a lot of teams are using the one big tabl approach as opposed to star schemas. Is anyone else noticing this or is it just me?",80,97,Tough-Error520,2023-10-29 23:44:51,https://www.reddit.com/r/dataengineering/comments/17jg1x5/im_seeing_less_star_schemas/,False,False,False,False
10lyfl0,Got The Job!,"Good News, Everyone! 

Since September of last year I have sent hundreds of applications, been interviewing regularly, and turned down a few lackluster offers. This morning I received an offer from the best company I have interviewed with over this entire endeavor. 

I interviewed with \~10 people from the company from recruiter to director over the past couple of weeks. All of which have shown themselves to be intelligent and enjoy the work that they do, which is shockingly uncommon.  

The company mission is not just vapid corporate-speak, but something I believe in and it seems the entirety of the team gets behind. Without doxing myself, I can say they do research and analytics for Government entities and foundations with an overarching goal of public welfare.

The company has work on all three cloud platforms, has mature+modern tech infrastructure, and offers the ability to learn and experiment with building solutions from scratch. 

I couldn't be more ecstatic to move to get away from the ""use <ETL Tool> to move data from this place to <Datawarehouse> and create a view for analysts to access it"" type of *engineering*\--and I use that term loosely*--*work I was relegated to previously. 

Me: 2YOE, BA in Philosophy, M.Sc. in Information Management

Job: Software Engineer (Cloud Data Platform), Full Remote (USA), 106k , 4 weeks PTO, Casual down-to-earth work culture

A big thanks to this community for all of the advice and guidance over the past 2 years!",77,12,Touvejs,2023-01-26 18:15:45,https://www.reddit.com/r/dataengineering/comments/10lyfl0/got_the_job/,False,False,False,False
zb329e,Where to Practice Complex SQL questions?,Hello every one. I am searching for websites where I can be tested for complex sql questions. Thanks in advance.,82,39,s1va1209,2022-12-03 00:40:55,https://www.reddit.com/r/dataengineering/comments/zb329e/where_to_practice_complex_sql_questions/,False,False,False,False
y4ijk6,Layoffs?,"Are any one of you guys been laid off recently? I’ve seen many PO, analysts and some SWE getting laid off but haven’t really seen any DEs. What I’ve seen is an insane amount of demand for DEs lately. My company is still hiring DEs like crazy, same goes for other companies that our DEs go to lol.",79,78,rudboi12,2022-10-15 08:34:00,https://www.reddit.com/r/dataengineering/comments/y4ijk6/layoffs/,False,False,False,False
xkgc8v,Just landed my first role as a DE,"A quick background. I’m a full-time bartender and physics student and nearly done with my degree. I got involved with data and analytics about a year ago with the goal to land a data analyst role.

After having applied for numerous data analyst roles this opportunity fell on my lap. A recruiter happened to reach out to me on LinkedIn about a DE role within their company. Mind you, the only training I have are courses from Udemy and DataCamp in python and SQL. I looked into the role and the company and decided I would apply. To my surprise, I made it through the interview rounds and they made an offer that I accepted.

So here I am, nervous and excited all at the same time but I honestly have no clue what I’ve gotten myself into. Need some insight into common entry-level DE everyday tasks and some guidance toward resources you used when you had just started your journey.

&#x200B;

**Edit:**

I've seen a few people asking about compensation so I'll provide more info here. The initial offer was 87k base with 10% yearly bonus and 40k RSUs. Ended at 91k base, 10% base, 40k RSUs. I was also able to negotiate for a $750 stipend to get my home office set up.

If you wish to continue:

I took a step back and accounted for the 401k with employer contributions, HSA and the contribution, and a plethora of other benefits like health and fitness stipends, mental health assistance, ""unlimited"" PTO (taking this with a grain of salt), fully remote work from within the US, flexible hours and a few other not so glamorous benefits like pet insurance. It's possible that I left money on the table because I'm not the best at negotiating but for now as an entry-level employee I am not complaining. I'll be ready to negotiate my base when evals come around.",79,43,Enthusiasm-Available,2022-09-21 20:40:29,https://www.reddit.com/r/dataengineering/comments/xkgc8v/just_landed_my_first_role_as_a_de/,False,False,False,False
v09zez,What should i practice for the PySpark Interview round?,"I have studied the concepts of Spark and practice few basic data frame, RDD and spark sql based questions. Can you list some important to cover / good to practice spark related questions for a DE interview? I have heard there are a lot of questions around Spark optimizations. Can you point out few important topics or techniques to cover that? Any link to blog or article would also help.",81,13,lucky-Chipmunk-119,2022-05-29 12:17:13,https://www.reddit.com/r/dataengineering/comments/v09zez/what_should_i_practice_for_the_pyspark_interview/,False,False,False,False
u1w5fs,"Do data engineer's write a lot of code? Thinking of switching from SWE, but don't want to use GUI tools / drag and drop.","Title... how much code do you guys write on a daily basis? I enjoy programming, but also enjoy working with data.",79,98,None,2022-04-12 10:47:31,https://www.reddit.com/r/dataengineering/comments/u1w5fs/do_data_engineers_write_a_lot_of_code_thinking_of/,False,False,False,False
schltg,Beginner mistakes to avoid in building Data Pipeline,"Hi everyone,

I've recently been promoted to a Data Engineering position at work. That being said, my first project is helping migrate data from SAP ECC to SQL Server and solidify our data pipeline so my Analytics team can extract data in a more streamlined way for our dashboards and modeling.  
 

I don't have much guidance from technical leadership or access to technical expertise in this undertaking, and I wanted to see if there were any Sr. DE's that had common ""rookie"" mistakes they've seen in similar initiatives that I should look out for.   


Any insights are appreciated.",79,31,data_questions,2022-01-25 16:27:54,https://www.reddit.com/r/dataengineering/comments/schltg/beginner_mistakes_to_avoid_in_building_data/,False,False,False,False
r49lr5,I am looking for a roadmap on getting into Data Engineering. I can't hope to follow the popular roadmap shared on this sub.,"I know this roadmap https://www.reddit.com/r/TheInsaneApp/comments/pjt1le/data_engineering_roadmap/?utm_medium=android_app&utm_source=share

It is just massive and I can't hope to do all of that in 1-2 years (I have a full time job too). It looks like a multi year experience check list. Can anyone share a shorter curated roadmap for getting into the field?",78,49,intexAqua,2021-11-28 17:34:50,https://www.reddit.com/r/dataengineering/comments/r49lr5/i_am_looking_for_a_roadmap_on_getting_into_data/,False,False,False,False
1b18mfk,I built an open-source CLI tool to ingest/copy data between any databases,"Hi all, ingestr is an open-source command-line application that allows ingesting & copying data between two databases without any code: [https://github.com/bruin-data/ingestr](https://github.com/bruin-data/ingestr)

It does a few things that make it the easiest alternative out there:

&#x200B;

* ✨ copy data from your Postgres / MySQL / SQL Server or any other source into any destination, such as BigQuery or Snowflake, just using URIs
* ➕ incremental loading: create+replace, delete+insert, append
* 🐍 single-command installation: pip install ingestr

We built ingestr because we believe for 80% of the cases out there people shouldn’t be writing code or hosting tools like Airbyte just to copy a table to their DWH on a regular basis. ingestr is built as a tiny CLI, which means you can easily drop it into a cronjob, GitHub Actions, Airflow or any other scheduler and get the built-in ingestion capabilities right away.

Some common use-cases ingestr solve are:

&#x200B;

* Migrating data from legacy systems to modern databases for better analysis
* Syncing data between your application's database and your analytics platform in batches or incrementally
* Backing up your databases to ensure data safety
* Accelerating the process of setting up new environment for testing or development by easily cloning your existing databases
* Facilitating real-time data transfer for applications that require immediate updates

We’d love to hear your feedback, and make sure to give us a star on GitHub if you like it! 🚀 [https://github.com/bruin-data/ingestr](https://github.com/bruin-data/ingestr)",78,54,karakanb,2024-02-27 10:23:58,https://www.reddit.com/r/dataengineering/comments/1b18mfk/i_built_an_opensource_cli_tool_to_ingestcopy_data/,False,False,False,False
1aviw3v,"GPT4 doing data analysis by writing and running python scripts, plotting charts and all. Experimental but promising. What should I test this on?",,78,46,ashpreetbedi,2024-02-20 14:26:42,https://v.redd.it/exzs2wto2rjc1,False,False,False,False
19aty1m,Data Engineer offer retracted after I moved cities.,"Hey everyone,

I don't know if this is the right place to post this but I finally landed a data engineering job after being a dashboard jockey for 4-5 years. It was everything I had dreamt of. I was due to start in two days and I find out that the company retracted the offer due to a massive layoff.
I moved to Toronto, Canada and  put down a deposit for a rental, and basically spent a lot of time, energy and effort.
Is there anything I can expect from the company? Should I go back to looking for data analyst roles?
Just a rant, any advice would be awesome.

Thanks!",79,17,gruntywolf,2024-01-19 21:13:48,https://www.reddit.com/r/dataengineering/comments/19aty1m/data_engineer_offer_retracted_after_i_moved_cities/,False,False,False,False
18m4e2i,"Non-technical boss, wanting to micromanage and kills our team","Throwaway because I know they are sometime on Reddit.

My boss is constantly asking for unnecessary details and micromanaging us. I am out of ideas on how to manage them.

It ranges from anything like wanting to be in meetings where we change the name of a cluster, up to asking why we need a tool to track bugs (and since we never had one before, why bother now?). Even not being technical they are:

- checking the SQL queries of my colleagues;
- they want to know if this or that has been documented for tiny operations;
- they are asking for ingestion of new datasets before we have even finished patching bugs or validating stuff, no unit tests because we are always short on time

I am trying to handle the stress of cascading requests by having a roadmap and shielding our team, but my manager will constantly challenge the roadmap and put pressure on my reports.

This is not tenable in the long term, do you have any advice on managing stakeholders above that aren’t technical?",78,57,SuperMarioDataGalaxy,2023-12-19 15:22:46,https://www.reddit.com/r/dataengineering/comments/18m4e2i/nontechnical_boss_wanting_to_micromanage_and/,False,False,False,False
1725spn,What stack do the small players have here?,"For a few weeks I've been lurking this subreddit and most posts are aimed towards bigger stacks. Big datalake, etc. What do the small players here for their ETL pipelines? Most recent popular tools seem very big overkill for small tasks",79,90,KatZegtWoof,2023-10-07 12:57:40,https://www.reddit.com/r/dataengineering/comments/1725spn/what_stack_do_the_small_players_have_here/,False,False,False,False
10uhjfg,8 Key Data Structures That Power Modern Databases,,78,13,theporterhaus,2023-02-05 17:09:33,https://youtu.be/W_v05d_2RTo,False,False,False,False
zx8ss8,You guys ever puzzled by how some organizations are generating petabytes of data?,"Case in point, [this](https://www.reddit.com/r/dataengineeringjobs/comments/zwix9s/data_engineer_cloud_infrastructure_remote_110150k/) post from r/dataengineeringjobs for The California College Guidance Initiative. Like which part of their data is petabyte scale? how can i find out? am i underestimating how small a petabyte of data is?",81,46,blue_trains_,2022-12-28 12:50:35,https://www.reddit.com/r/dataengineering/comments/zx8ss8/you_guys_ever_puzzled_by_how_some_organizations/,False,False,False,False
yhhqbf,Am I a Data Engineer?,"So I realize that job titles tend to be arbitrary - but I’ve recently gotten told by multiple people that my title is incorrect and compensation is too low based off my job responsibilities.

My official title is operations analyst and my total comp is around 70k - I currently have 3 years of experience working as an analyst. And my company is a mid sized saas tech firm based on the west coast. 

Responsibilities- I  spend 50% of my time in snowflake analyzing data and creating various scripts/schemas using dbt - and 30% of the time using etl/elt tools to load data from various sources into our warehouse.. and the remaining 20% of the time I’m creating dashboards with our bi tool. The business would love the bi portion of my work to be higher - but I really try keep my work to 60hrs a week - and the moving/cleaning of data takes up the majority of time.

I’ve been told my title should be anything ranging from data analyst, analytics engineer, data engineer and even product analyst.. so I’m just curious what everyone on the de subreddit thoughts are regarding my title and compensation? And if I’m not near or at the official data engineer title yet - what steps should I take to move myself in that direction?",82,53,General-Geologist-53,2022-10-30 15:55:51,https://www.reddit.com/r/dataengineering/comments/yhhqbf/am_i_a_data_engineer/,False,False,False,False
uenci9,GCP Data Engineer Certification Courses available for free (for a month),"Hi guys,

All the google cloud courses are available for free for a month on Coursera. I didn't check yet but they probably offer qwiklabs as well.

Link is here: [https://cloud.google.com/blog/topics/training-certifications/how-to-build-job-ready-cloud-skills](https://cloud.google.com/blog/topics/training-certifications/how-to-build-job-ready-cloud-skills)  (from April 28 - May 29, 2022)",82,23,baguetteFeuille,2022-04-29 14:37:56,https://www.reddit.com/r/dataengineering/comments/uenci9/gcp_data_engineer_certification_courses_available/,False,False,False,False
s490uh,[Novice here] For an internship is this too much? Or just a bit challenging,,77,39,saaaalut,2022-01-15 02:04:35,https://i.redd.it/te86uzjperb81.png,False,False,False,False
rgyt7b,Data Engineering Jargon - Part 5 (Final),"Hi - here are the final 10 and some bonus ones as requested by the community.

1-10 is [here](https://www.reddit.com/r/dataengineering/comments/rdw3b3/data_engineering_jargon/?utm_source=share&utm_medium=web2x&context=3)

11-20 is [here](https://www.reddit.com/r/dataengineering/comments/rem26j/data_engineering_jargon_part_2/)

21-30 is [here](https://www.reddit.com/r/dataengineering/comments/rfbuu8/data_engineering_jargon_part_3/)

31-40 is [here](https://www.reddit.com/r/dataengineering/comments/rg5vr0/data_engineering_jargon_part_4/)

41-50 is below

**41. Sandbox**

Usually refers to an environment where extensive testing can be carried out without compromising the sanctity of the live platform.

*A sandbox to prove a concept of keyboard metric before getting this accepted in a live environment.*

**42. Subject Area**

A way of defining a data model by grouping the enterprise’s data according to known business directorates.

*A customer subject area containing all customer information that can be utilised across the business*

**43. Raw Data**

This is the data as it has been collected in its rawest format before it is processed, cleansed and loaded.

*Raw data of all the customer’s orders from the day of trading*

**44. Transactional Data**

This is data that describes an actual event.

*Order placed, a delivery arranged, or a delivery accepted.*

**45. Reference Data**

This is data that allows the classification of other data.

*Country code* *GB representing Great Britain.*

**46. Master Data**

This is data that is the best representation of a particular entity in the business. This gives you a 360 view of that data entity by generally consolidating multiple data sources.

*Best customer data representation from multiple sources of information.*

**47. Structured Data**

Data that is nicely organised in a table using rows and columns, allowing the user to easily interpret the data.

*Finance data in a database table, easily queryable using SQL.*

**48. Unstructured Data**

Data that cannot be nicely organised in a tabular format, like images, PDF files etc.

*An image stored on a data lake cannot be retrieved using common data query languages.*

**49. Data Quality**

A discipline of measuring the quality of the data to improve and cleanse it.

*Checking Customer data for completeness, accuracy and validity.*

**50. Data Management**

A discipline encompassing the end-to-end management of data lifecycle, including acquiring, transferring, securing and querying data.

*Combination of improving the quality of data, governing the data, enriching and cleansing the data.*

\-------------------------

**Bonus terms:**

**51. Metadata**

This is information about the data itself.

In DE this is likely to be table names, table size, data types and sizes, column names, even constraints like Foreign and Primary Keys.

*In a table with Country information. UK or US will be the data itself, whereas the Column name ""Country"" will be the metadata*

**52. Data Marketplace**

Concept of creating a marketplace where buyers and sellers come together to trade data. This has become even more popular due to IoT data that has rapidly increased over the past decade.

*Snowflake has a data marketplace where you can buy anonymised third party data to help with your use cases. Royalmail in the UK licenses PAF (Postcode Address File) to businesses, another way of buying data.*

*I expect with blockchain for this kind of use case to become more consumer-focused. A consumer maybe able to earn crypto tokens for willingly sharing their data, so instead of the ad revenue going to Google etc. it could come to you as an end consumer.*

**53. Data Product**

Solving a business problem using (mainly) data is defined as a data product. And using data could mean anything from simple dashboards to ML models helping with recommendations on products to buy on Amazon.

*A search function on Amazon is an example of a Data Product, without well-catalogued data, this function would be useless.*

**54. Scalability**

Scalability is generally defined as the ability of the application to scale in light of changing (increasing) demands.

In DE this could mean the datawarehouse or datalake platforms. Could also mean creating scalable data pipelines.

*There are many ways of scaling a database (datawarehouse): for example indexing a table would allow fast retrieval of information as your query would not need to search every row of data. This would be a simple change for significant benefits. Another scalability example is number 55.*

**55. Database Sharding / Partitioning**

A performance improvement technique of breaking up large data sets into small subsets. 

Sharding is when this break up of data set happens across multiple machines

Partitioning is when this break up of data set happens on the singular database

The point is, if the data is divided into smaller subsets and into different machines, you are not bottlenecking the same machine with your queries each time.

\-------------------------------------------------------------------

**Parting Thoughts**

This is the final post in the series, I hope you all enjoyed reading this as much as I enjoyed writing it. I also hope you at least learnt a few new terms, it certainly helped me clarify my thinking. Thanks also to everyone that took part in the comments and helped me improve some of these definitions.

Various people have posted the original Medium link of this article, where I usually write. So feel free to check that out.

Let me know what else you'd like to learn and I could write up a future series about it.

Thank You!

1-10 is [here](https://www.reddit.com/r/dataengineering/comments/rdw3b3/data_engineering_jargon/?utm_source=share&utm_medium=web2x&context=3)

11-20 is [here](https://www.reddit.com/r/dataengineering/comments/rem26j/data_engineering_jargon_part_2/)

21-30 is [here](https://www.reddit.com/r/dataengineering/comments/rfbuu8/data_engineering_jargon_part_3/)

31-40 is [here](https://www.reddit.com/r/dataengineering/comments/rg5vr0/data_engineering_jargon_part_4/)",77,5,Data_Cog,2021-12-15 13:12:50,https://www.reddit.com/r/dataengineering/comments/rgyt7b/data_engineering_jargon_part_5_final/,False,False,False,False
q9dgub,Is using pandas considered terrible practice in ELT?,"Hello,

all the python pipelines we have are using pandas. Basically final important step is always dataset.to\_sql

(and then transformation SQL script that will be executed as a transaction in our DWH/ Datalake itself.

Is that considered bad practice and I would be laughed off on DE interviews or something completely acceptable?

Average table has \~500k rows, with the biggest ones around 40M. So far I found this method as very effective, but I doubt it is also recognized in the industry.",80,63,UnderstandingFit9152,2021-10-16 14:49:31,https://www.reddit.com/r/dataengineering/comments/q9dgub/is_using_pandas_considered_terrible_practice_in/,False,False,False,False
oeyu81,Data Analyst transitioning to Data Engineering: Need some tips on the courses and a few other things!,"Hi All,

I'm currently working as a data analyst, and I have good experience working with Python and SQL. I have basic to intermediate level skills in both. I'm looking to transition to Data Engineering and want to update my skills, especially w.r.t. to cloud computing, ETL, DBA, etc., which I think would be useful for mid-level DE jobs.

I checked Coursera and the IBM Data Engineering course ([https://www.coursera.org/professional-certificates/ibm-data-engineer](https://www.coursera.org/professional-certificates/ibm-data-engineer)) looked good to me. Before I apply for that, just wanted to check on forums if there are any better learning paths available, considering that I do have a background in Data Analytics. Please let me know if this one seems good enough.

That was the first thing. Coming to the next part, as I mentioned, I am currently in a job, so have to use the office-provided laptop. I can't really quit at the moment so it's my compulsion to use this laptop, on which admin access is restricted. And as far as I can understand, setting up the environment for DE projects, will require multiple scenarios where I would need admin access. Is there a way around that? Like can DE projects be done on a web-based interface somehow (like Google colab, as an alternative for local Jupyter notebooks)? Please guide me on this.

Note: I do have Anaconda Navigator, and SSMS installed (for Python and SQL purposes). If there's a  way to work with that, do let me know!

Thanks in advance!",77,34,marshr9523,2021-07-06 16:47:04,https://www.reddit.com/r/dataengineering/comments/oeyu81/data_analyst_transitioning_to_data_engineering/,False,False,False,False
lbdnh4,Article/Tutorial: How to develop data pipeline in Airflow through TDD (test-driven development),"Hey folks I wrote an article/tutorial about how to develop DAGs using TDD (test-driven development) and how to setup a CI with Github Actions, you can read [here](https://blog.magrathealabs.com/how-to-develop-data-pipeline-in-airflow-through-tdd-test-driven-development-c3333439f358).

All the code is here: [https://github.com/marcosmarxm/airflow-testing-ci-workflow](https://github.com/marcosmarxm/airflow-testing-ci-workflow)

and for those just starting with Airflow/Data Engineering. I created a detailed step-by-step of the project: [https://github.com/marcosmarxm/airflow-testing-ci-workflow/blob/master/assets/how-to/create-dag-using-tdd.md](https://github.com/marcosmarxm/airflow-testing-ci-workflow/blob/master/assets/how-to/create-dag-using-tdd.md) 

Hope you enjoy! any suggestions are welcome",78,5,None,2021-02-03 02:46:47,https://www.reddit.com/r/dataengineering/comments/lbdnh4/articletutorial_how_to_develop_data_pipeline_in/,False,False,False,False
f3az0z,An Awesome List of Open-Source Data Engineering Projects,,76,7,gunnarmorling,2020-02-13 14:44:57,https://github.com/gunnarmorling/awesome-opensource-data-engineering,False,False,False,False
1b8l1nd,"As a data engineer, what do you find the most challenging task in modern data engineering?","As a data engineer, what do you find the most challenging task in modern data engineering?",80,87,andalibansari,2024-03-07 04:10:12,https://www.reddit.com/r/dataengineering/comments/1b8l1nd/as_a_data_engineer_what_do_you_find_the_most/,False,False,False,False
18xb4ug,None of what I learned is a job requirement. I am essentially skill-less.,"Hi there, here's an example job requirements I just found (shortened it), which has the same feel as the last 50 job adverts I've seen recently.

""Proficiency in Bash, Python, and SQL. Experience with Linux and Docker. Knowledge in Databases, Data Modeling, ETL, dbt, and Snowflake. Expertise in Spark, Databricks, EMR, Streaming, and Kafka. Familiarity with AWS services such as EC2, S3, Lambda, EMR, Glue, and Athena.""

So.. I'm about to graduate from a Master's in Data Science, where I took mostly Data Engineering stuff for my optional units. Literally all I have had is some exposure to Bash, Python and SQL, and data types. The only reason why I know Linux and Docker is because I started writing something on a Raspberry Pi to open my garage door when I was 16, with a few other small projects.

Yes the master's teaches lots of stats, modelling concepts, ML, DL, and some Data Warehousing etc.. but not a single job, not even entry position that I have found, require skills I learned in my Master's. Every student in my class is now great at R but useless in Python, literally never see job adverts with R on it. Feels like the Master's was a Bachelor's or an ""Intro to Data Literacy"" course.

Where do you even learn these skills? I doubt that you guys just bullshit-apply to jobs and watch YouTube before the interview.. Should I take a full year OFF after my Master's to just learn everything about Azure, Google Cloud, Microsoft Analytics, bloody software development practices even and empty all the Udemy/Coursera courses out there? Then maybe I can get a job?

Gee. I feel like uni has absolutely not made me job ready in any way.",75,55,Zomdou,2024-01-03 05:06:32,https://www.reddit.com/r/dataengineering/comments/18xb4ug/none_of_what_i_learned_is_a_job_requirement_i_am/,False,False,False,False
17puedp,"Spark, Dask, DuckDB, Polars: TPC-H Benchmarks at Scale","I gave this talk at PyData NYC last week.  It was fun working with devs from various projects (Dask, Arrow, Polars, Spark) in the week leading up to the event.  Thought I'd share a re-recording of it here

[https://youtu.be/wKH0-zs2g\_U](https://youtu.be/wKH0-zs2g_U)

This is the result of a couple weeks of work comparing large data frameworks on benchmarks ranging in size 10GB to 10TB.  No project wins.  It's really interesting analyzing results though.

DuckDB and Dask are the only projects that reliably finish things (although possibly Dask's success here has to do with me knowing Dask better than the others).  DuckDB is way faster at small scale (along with Polars).  Dask and Spark are generally more robust and performant at large scale, mostly because they're able to parallelize S3 access.  Really-good-S3 access seems to be the way you win at real-world cloud performance.

Looking more deeply at Dask results, we're wildly inefficient.  There's at least a 2x-5x performance increase to be had here.    Given that Dask does about as well as any other project on cloud this really means that \*no one\* has optimized cloud well yet.

This talk also goes into how we attempted to address bias (super hard to do in benchmarks).  We had active collaborations with Polars and Spark people (made Polars quite a bit faster during this process actually).  See [https://matthewrocklin.com/biased-benchmarks.html](https://matthewrocklin.com/biased-benchmarks.html) for more thoughts.

&#x200B;

This also shows the improvement Dask made in the last six months.    Dask used to suck at benchmarks.  Now it doesn't win, but reliably places among the top.  This is due to ... 

1. Arrow strings
2. New shuffling algorithms
3. Query optimization  

There's a lot of work for projects like Dask and Polars to fix themselves up in this space.  They're both moving pretty fast right now.  I'm curious to see how they progress in the next few months.

For future work I'd like to expand this out a bit beyond TPC-H.  TPC-H is great because they're fairly serious queries (lots of tables, lots of joins) and not micro-benchmarks.  We could use broader coverage though.  Any ideas?",77,22,mrocklin,2023-11-07 13:34:19,https://www.reddit.com/r/dataengineering/comments/17puedp/spark_dask_duckdb_polars_tpch_benchmarks_at_scale/,False,False,False,False
16lu20w,Process 250TB in 20 minutes for $25 using a 200 VM cluster,"Dear subreddit, there have been many complaints here lately about the quality of posts. I present to you this hidden gem that I found, which has gone under the radar. Please read the article and I encourage some healthy discussion. I also highly encourage criticism of this artchitecture because I am smitten by these numbers and I would like someone to slap me back to reality.

The article can be found here and it's from Coiled. It uses Coiled, Dask and Xarray:

Blog: [https://blog.coiled.io/blog/coiled-xarray.html](https://blog.coiled.io/blog/coiled-xarray.html)    
Code: [https://github.com/coiled/examples/blob/main/national-water-model/xarray-water-model.py](https://github.com/coiled/examples/blob/main/national-water-model/xarray-water-model.py)

Apart from Dask, I haven't heard of the Coiled or Xarray but they seem to be pip installable so I don't think there's not much overhead in using them.

Given that Spark might be the goto choice for this scale of data and given that I'd never willingly use Spark and admit to it, I'd have to come up with alternative strategies to process 250TB. So far, I have not been able to come up with an alternative strategy that can achieve the same time and cost efficiency.

**What would be your approach to replicate what's been done in the article? You can assume an alterate but similar problem. Please mention the following:**

* **Tools you would use?**
* **How much time it would take you to bootstrap this?**
* **Estimated time to completion?**
* **Estimated total cloud cost?**
* **Additional comments on why your strategy is better?**

In my opinion being able to download and process 250TB within 20 minutes is an darn good benchmark. I do want to point out that initially, I was repulsed by the fact that they use 200 VMs in their Coiled cluster. It seems like too much overhead. But when I thought about it further, it might actually be a pretty genius move. It wasn't mentioned in the article so I'm of the opinion the author doesn't realize it either that each AWS VM, ie. the r7g.2xlarge (64GB RAM) has a network bandwidth limit of 15 Gbps (1.875 GBps) but the r7g.metal (512GB RAM) instance has a network bandwidth limit of only 30 Gbps (3.75 GBps) which only 2x more than the r7g.2xlarge but is 8x more expensive. So this means it is optimal to use many smaller VMs instead of a few large ones if you want to actually download 250TB really quickly.

P.S. Not affiliated to any of the companies here.",77,27,nitred,2023-09-18 12:21:04,https://www.reddit.com/r/dataengineering/comments/16lu20w/process_250tb_in_20_minutes_for_25_using_a_200_vm/,False,False,False,False
16e9iqw,Does anyone regret moving to Data Engineering,"I am currently a data analyst working in healthcare and looking to move to data engineering. For those of you who moved into the field from something else:

1. What was your previous role?
2. Why did you move to data engineering
3. Pros and cons of the move compared to your previous job
4. Do you regret moving to DE and why if you did

Any info would be much appreciated",77,98,xyzabc123410000,2023-09-09 16:08:41,https://www.reddit.com/r/dataengineering/comments/16e9iqw/does_anyone_regret_moving_to_data_engineering/,False,False,False,False
162utkn,"Just picked up this big boy for $1 at good will. Worth keeping? Wanted to get a refresher on Hive and cluster scaling for an interview, but this seems overkill and maybe outdated (4th edition is latest)",,78,47,uncomfortablepanda,2023-08-27 16:23:37,https://www.reddit.com/gallery/162utkn,False,False,False,False
15rws69,How much SQL do you use as a DE?,"I ask since I am an aspiring DE, currently studying the core skills, and I don’t want to spend unnecessary time grinding SQL exercises because

1) I know most of my sticky learning is going to come from hands on, real situations/projects 

2) I still need to make time to learn Python/ETL ect. 

I have been studying SQL for 3-5 hours a day for that last 2 weeks (I am so tired of my current career field that I have this burning desire to get out, hence the aggressive study schedule) 
And so far have been working on and can answer the easy and medium topics/concepts on stratascratch
 

1.	⁠SELECT and WHERE for filtering and selection
2.	⁠COUNT, SUM, MAX, GROUP BY, HAVING for aggregating data""
3.	⁠DISTINCT, COUNT DISTINCT for producing useful distinct lists and distinct aggregates
4.	⁠OUTER (e.g. LEFT) and INNER JOIN when/where to use them
5.	⁠Strings and time conversions
6.	⁠UNION and UNION ALL.
7.    Window functions like PARTITION
8.    CTE’s 
9.    CASE 
10.   ROW_Number and Rank 

So back to my original question, how much SQL do you use as a DE and is this short list of concepts above enough for me to move on to learn Python and other relevant skills?",77,83,PoloParachutes,2023-08-15 16:03:03,https://www.reddit.com/r/dataengineering/comments/15rws69/how_much_sql_do_you_use_as_a_de/,False,False,False,False
151npp7,Best way to execute Python scripts on a schedule?,"Our data warehouse is a SQL Server. We’ve been using Python to do a lot of scheduled ETL tasks. Currently I’m executing the tasks on a schedule (10 minutes) using Windows Task Scheduler and batch files on the same Windows server as the SQL Server. 

Is there a better way to do this? I’ve read that you can use stored procedures or scheduled events, but is that going to be faster? 

Currently 85% of memory is allocated to SQL Server.

Any pros or cons to consider?",76,46,BestTomatillo6197,2023-07-17 01:21:08,https://www.reddit.com/r/dataengineering/comments/151npp7/best_way_to_execute_python_scripts_on_a_schedule/,False,False,False,False
14syob6,Is data engineering the next data science?,"It very much looks like DS becomes over-saturated with passionate and (self)educated employees.  
Additionally, AI such as co-pilots and no-code environments make this domain even more competitive.

Look at the number of subreddit members r/datascience \~ 1 million, r/dataengineering \~ 10 times less

Do you think that DE will be the next DS (sexiest job...)?  
What do you think is worth to put time and effort in to get one step ahead - is this really platform engineering, distributive computing (Kubernetes) or something else?",75,88,scriptosens,2023-07-07 06:21:44,https://www.reddit.com/r/dataengineering/comments/14syob6/is_data_engineering_the_next_data_science/,False,False,False,False
14nl01c,Introducing English as the New Programming Language for Apache Spark,,77,21,OverratedDataScience,2023-07-01 04:00:28,https://www.databricks.com/blog/introducing-english-new-programming-language-apache-spark,False,False,False,False
14az4gz,The Dagster Master Plan,,76,33,floydophone,2023-06-16 15:01:14,https://dagster.io/blog/dagster-master-plan,False,False,False,False
136d100,Sr. Data Analyst -> Data Engineer,"Got an internal job role, but as I'm moving from Sr. to Intermediate role, there's a pay cut from 115k -> 90k. Will you take it?  


I'm a 28 y/o unmarried dude, just fyi in case.

Edit: I considered applying for this new position as I thought it'd be a good chance to expand my skills, etc. and the hiring manager told me he would try to at least match my current salary, but I came to know from him today that it's not possible.",77,72,Horror-Career-335,2023-05-03 06:58:35,https://www.reddit.com/r/dataengineering/comments/136d100/sr_data_analyst_data_engineer/,False,False,False,False
11sq68k,"""ingestion is a solved problem""","I see this sentiment a lot in this sub. Seems to be mainly from analytics engineer types who are focused on data modeling inside the warehouse. The reasoning is normally along the lines of ""tools like fivetran and airbyte have connectors for everything, so no need to write integrations for anything, we can just deploy those and get on with the real work""

While on some level this is true, i really do feel like its missing a big part of the picture. For one it doesn't consider streaming and real time data but that's a debate for another time. The big problem with the above vision for me is it's overly focused on just lifting and shifting data from OLTP prod systems, with no consideration for things like data quality, schema validation, schema evolution, data contracts etc. To me it's overly coupled to the specific OLTP technologies used. Shouldn't we be looking to see OLTP systems wrapped in an interface that doesn't rely on internal implementation details?

Curious to hear other people's thoughts on this as it's something that's been bothering me lately.",74,70,the-data-scientist,2023-03-16 10:27:04,https://www.reddit.com/r/dataengineering/comments/11sq68k/ingestion_is_a_solved_problem/,False,False,False,False
101k1xv,Dataframes vs SQL for ETL/ELT,"What do people in this sub think about SQL vs Dataframes (like pandas, polars or pyspark) for building ETL/ELT jobs? Personally I have always preferred Dataframes because of

* A much richer API for more complex operations
* Ability to define reusable functions
* Code modularity
* Flexibility in terms of compute and storage
* Standardized code formatting
* Code simply feels cleaner, simpler and more beautiful

However, for doing a quick discovery or just to ""look at data"" (selects and group by's not containing joins), I feel SQL is great and fast and easier to remember the syntax for. But all the times I have had to write those large SQL-jobs with 100+ lines of logic in them have really made me despise working with SQL. CTE's help but only to an certain extent, and there does not seem to be any universal way for formatting CTE's which makes code readability difficult depending on your colleagues. I'm curious what others think?",79,94,Mobile_Yoghurt_9711,2023-01-02 18:42:49,https://www.reddit.com/r/dataengineering/comments/101k1xv/dataframes_vs_sql_for_etlelt/,False,False,False,False
zy7q2n,How do data engineering interviews work?,"Hi I am a “backend engineer” but truthfully I am more akin to a data engineer. My dumpster fire of a job has made me a de facto SQL, ETL, and SSIS/SSRS expert. Given that all my current experience in my current job is related to data engineering, I wanna make a shift to data engineering but I have no idea what to expect from these kind of job interviews

Any advice or tips would be appreciated. My educational background is Computer Science software development so I am curious how similar data engineering job interviews are to software engineering job interviews",74,24,Dats_Russia,2022-12-29 15:14:33,https://www.reddit.com/r/dataengineering/comments/zy7q2n/how_do_data_engineering_interviews_work/,False,False,False,False
z0wzb0,Made a post about weird hybrid titles coming out. Who can guess the job duties without reading the full posting (which can be Easily found on LinkedIN for anyone interested),,77,27,DrRedmondNYC,2022-11-21 11:45:56,https://i.redd.it/9lc0okovka1a1.png,False,False,False,False
ydwud8,Why is Data Engineering considered “not as attractive” compared to DS?,"I’ve been a Data Analytics intern coming up on a year now, but I’ve doing some work wearing the Date Engineering hat for the first time working on our company’s ETL process and creating a data pipeline to use in Azure to make departments more satisfied with the speed in which they receive their data, so I feel like I’ve gotten a decent view into some aspects of Data Engineering at least according to my supervisor.

What I don’t fully understand is how come they say Data Science is “sexier”? Is it salary? From what I saw they both seem well compensated and both of them have very high ceilings that seem to even breach 200k at a senior level in some areas. Is it the fact data science uses more forecasting for companies? I would think when you are in charge of the sources of huge volumes of data and you are essentially just as vital to these reports as the Data Science aspect of things I’m just wondering where is the line drawn between the two? 

Asking really just for curiosity but also trying to figure out which career path to focus on, going all in on DE or focusing some more on DS. Funny enough my internship in here Analytics doesn’t seem to be touching too much in DS, but seems to be putting me in more DE positions due to the lack of actual usable data that can be used",80,98,ToothPickLegs,2022-10-26 12:25:51,https://www.reddit.com/r/dataengineering/comments/ydwud8/why_is_data_engineering_considered_not_as/,False,False,False,False
wmpz5t,My first DE project about flight punctuality in Europe,"I want to build a career in Data Engineering, so I have built my first personal project. Please be so kind to leave some feedback on what I should improve on.

 

# About The Project

The  goal of this project is to display how flight punctuality changes over  time considering the temperature deviation from the average monthly  temperatures in European airports.  
The inspiration came to me from  recent headlines stating the unprecedentedly high flight delay and  cancellation figures across most of Europe.

# How to Read the Dashboard

A  flight is considered to be delayed if it departs 15 minutes after the  scheduled departure time. Flight punctuality shows the ratio of  non-delayed flights on a day.  
The columns represent how much the daily average temperature deviates from the historic average monthly temperature (from 1980).

&#x200B;

 [Link to the dashboard.](https://app.powerbi.com/view?r=eyJrIjoiMTk3MjQxY2QtNzM0Yy00OGI4LTk5N2ItYmRkOGIwMjVkNGYxIiwidCI6IjRiZWYyNmNiLWFhZTktNDg4ZC1iNTk2LWVkNzcyZWUzODBjYyIsImMiOjl9) 

&#x200B;

&#x200B;

[Architecture](https://preview.redd.it/dbsb4ko69bh91.png?width=1623&format=png&auto=webp&s=6f2f80fb3405d85236031efdad4e02e487567a61)

&#x200B;

 

# The Data

The flight data is downloaded in an xlsx format from Eurocontrol’s [website](https://ansperformance.eu/traffic/punctuality/).  It is updated daily with the previous day’s data, but unfortunately it is not retained in a day-by-day historic format, only in an aggregated  report.  
I chose the busiest airport from each country, to represent as many countries as possible, while keeping the list of airports at a reasonable level.

The weather data is taken from the National Oceanic and Atmospheric Administration’s [servers](https://www.ncei.noaa.gov/pub/data/gsod/). Each weather station’s data is stored in a yearly file, and occasionally small corrections are made on past days’ figures. Historic datasets are available going back for almost a 100 years.

Both data sources are updated daily, so Airflow runs the full ETL process each night, loading the flight data incrementally, and refreshing weather data for the full year. The historic average monthly temperature is also re-calculated daily, using observations starting from 1980.

# Tools Used

I wanted to build a completely free project, so I decided to run the whole process on my Raspberry Pi.

Orchestration — Apache Airflow  
ETL — Python and Bash scripts  
Local Database for bronze data — Postgres  
Cloud Database for gold data — Azure Data Lake  
Visualization — Power BI

The data usage on the Azure Data Lake is very small, so it should be in the free tier.

# Potential Improvements

* The  whole project could be migrated to the cloud. I would probably use  Azure Databricks and Azure Data Factory, as I have some experience with  those, and the visualization part of the project is already in the Azure ecosystem.
* The scale could be improved by adding flights in the United States, potentially from the [Bureau of Transportation Statistics](https://transtats.bts.gov/ONTIME/Departures.aspx).
* Additional aspects of the weather (visibility, precipitation, wind speed) are  already part of the bronze data, they could easily be added to the  visualization.
* Additional visualizations, potentially of the above mentioned aspects.
* Unit tests.

# Additional Notes

The  visualization tracks only one aspect, the temperature. I am fully aware  that the current situation is not caused by the higher than usual  temperatures in Europe, it is rather due to various circumstances, originating from the travel restrictions in 2020 and 2021, resulting in a staff shortage, and pent-up demand on traveling abroad. Nonetheless, if  the project goes on for a longer period, and we experience a return to normal situation, it might be interesting to see whether there is any  correlation between the temperatures and flight delays.

# Feedback

This is my first project, which is not based on a course material or guide, so it is rough around the edges. Please let me know what you think, how I can improve it in both technical and aesthetic aspects.",78,16,crepitation,2022-08-12 16:46:29,https://www.reddit.com/r/dataengineering/comments/wmpz5t/my_first_de_project_about_flight_punctuality_in/,False,False,False,False
u8xb20,"Those who transitioned from data analytics to data engineering, why?","I'm in data analytics and enjoy working with data, creating pipelines, and the visualizations, but I'm starting to get bored. I am, however, not a fan of the statistics and find myself enjoying the programming side the most. I'm curious why others in data analytics may have switched to data engineering?",76,87,what_duck,2022-04-21 20:52:23,https://www.reddit.com/r/dataengineering/comments/u8xb20/those_who_transitioned_from_data_analytics_to/,False,False,False,False
ty2662,What's the coolest problem you've ever solved?,Just was wondering what's the most innovative or largest problem you've ever solved on the job or for your own side project?,76,34,rob121212111,2022-04-07 01:58:26,https://www.reddit.com/r/dataengineering/comments/ty2662/whats_the_coolest_problem_youve_ever_solved/,False,False,False,False
qrq7mb,How and where did you learn SQL from and become good at it?,"I want to learn and become really good at SQL (and Python too!).

This sub seems to have both DEs and SWEs. Could I get some guidance on how and from where I should learn SQL and Python to become a skilled and competent engineer?


**Thank you everyone**


**Edit: I forgot to mention this. Most tutorials and courses seem to focus on the querying part of SQL. Where and how do I learn to design, structure and create Data and databases?**",77,74,8sleepok,2021-11-11 17:24:59,https://www.reddit.com/r/dataengineering/comments/qrq7mb/how_and_where_did_you_learn_sql_from_and_become/,False,False,False,False
l2noa4,Data Engineer Interview at Amazon,Hi everyone! Who has had an interview at Amazon for this role? Can you share with me your experience and what to expect and some sample questions?,73,14,thisiskcl,2021-01-22 13:25:39,https://www.reddit.com/r/dataengineering/comments/l2noa4/data_engineer_interview_at_amazon/,False,False,False,False
1bzvyhk,Asking for more RAM for Data Engineering jobs,"This is so embarrassing to ask that I have written this post several times and erased it before submitting.

The IT guy at work won't upgrade my Data Engineer's RAM from 16GB to 32GB since his RAM capacity is always in the 90%s. He has a Windows machine and uses a lot of tools like DataGrip, which he has experience with.

I'm really annoyed that I have to keep justifying why he needs it.  
He handles a lot of data and I think it is beneficial that he uses tools that he is comfortable to work with. Also, RAM is dirt cheap nowadays.   


Are there any other reasons I should include?  
",77,54,charlenecassar,2024-04-09 16:08:15,https://www.reddit.com/r/dataengineering/comments/1bzvyhk/asking_for_more_ram_for_data_engineering_jobs/,False,False,False,False
1b91l3e,What's that one WTF moment you had that made you leave a company?,What's the worst data related decision a company made that left you speechless ,73,108,Eastern-Education-31,2024-03-07 18:19:03,https://www.reddit.com/r/dataengineering/comments/1b91l3e/whats_that_one_wtf_moment_you_had_that_made_you/,False,False,False,False
1azl4lz,Landing a data engineering role with the help of this group,"Hey beautiful people, I recently got a data engineering role after trying for a while. It wouldn't have been possible with the support of this group. I would like to thank you guys for this. 
I would like to know what should my next steps be? Though I got the job, i still have that imposter syndrome even though my team members are super supportive. I want to take my learnings to a next level but I feel that I lack the basics so want to start from the very scratch like learning the alphabets.
So if you were to start a job in IT, how you would have started? I want to become an engineer in true sense. I am ready to devote sufficient time along with my job. 
Suggestions are most welcome.

Thank you!!
Have a great day!",78,21,frustratedhu,2024-02-25 10:46:46,https://www.reddit.com/r/dataengineering/comments/1azl4lz/landing_a_data_engineering_role_with_the_help_of/,False,False,False,False
194d07l,"Data Engineer - What's the best course, certification or degree of all time?","Hello guys,

I hope you guys are well. I'm curious about your opinions. I'm a data engineer trainee. I want to learn A LOT. Not only SQL, Python, but PySpak, etc, etc.

But I'm curious: What's the best course, or certification (specialization) or degree of all time for you, that you can end the course and say: ""Wow, f\*\*\*\*\*\* hell! This was amazing! I learned so much with this!""

I want to know your opinions :)

You can also share books, share what really help you with to grow as a Data Engineer and as a professional :)

Have a good day/night",76,60,GigabyteWarrior,2024-01-11 21:28:02,https://www.reddit.com/r/dataengineering/comments/194d07l/data_engineer_whats_the_best_course_certification/,False,False,False,False
18zfz14,Preparing for DE Interviews at FAANG+ companies,"I will try not to dox myself but the end goal for me is to end up as a Senior DE at a large tech company. At the moment I'm ambivalent on whether this results in Data Platform Engineering or Data Analytics Engineering.

Here is my general framework for studying:

1. LC Easy/Medium (Arrays & Hashing, Two Pointers, Sliding Window, Stack, Binary Search, try to solve in 20-25 minutes with no/minimal help)
2. SQL Medium/Hard (Try to solve in 3-5 minutes with no/minimal help)
3. Data Modeling (Identify business needs using Product Sense and create a Star/Snowflake schema from this)
4. Behavioral (standard STAR answers)

I am decidedly not good at algorithmic questions, which is part of the reason why I transitioned to DE (also I think it's cooler, among other things). Is this a good framework to abide by to target dedicated DE roles at FAANG+ companies (I specifically have Meta and Amazon in mind)? Any comments or insight would be welcomed.",76,25,KingTyranitar,2024-01-05 19:59:59,https://www.reddit.com/r/dataengineering/comments/18zfz14/preparing_for_de_interviews_at_faang_companies/,False,False,False,False
17wragh,Running Python Jobs in cloud,"Most of our ETL jobs are python scripts that hit APIs and write to a blob storage. Data is small, so I don’t need any tools like spark. What are some easier tools to schedule and run these python jobs?

I tried using AWS lambda functions, but there are significant limitations like 15 minute max timeouts or maximum size is for uploading python environments.

Airflow on a server is ideal, but extremely difficult to set up and manage currently. Companies that manage it for you (like astronomer) are pretty expensive.

Anyone used Dagster?",79,107,D-2-The-Ave,2023-11-16 16:37:40,https://www.reddit.com/r/dataengineering/comments/17wragh/running_python_jobs_in_cloud/,False,False,False,False
17t0d2p,Does anyone else hate the idea of writing code for job scheduling like airflow?,"It's a rant really about some of our airflow jobs having more lines of code than the actual code and also more effort in writing airflow code than the real work.

I tried building airflow factory models etc but it did not help. Hope something like autosys or control m comes up as open source for big data job scheduling.",76,71,RepulsiveCry8412,2023-11-11 18:23:19,https://www.reddit.com/r/dataengineering/comments/17t0d2p/does_anyone_else_hate_the_idea_of_writing_code/,False,False,False,False
16qim1i,What is your background education?,"I work as a data engineer for a year now, but I graduated in chemical engineering. I had no prior knowledge in this field before (except for some code experience in matlab and very little in python) and I had to learn everything along the way. I consider myself extremely lucky, because this opportunity just appear out of the blue and it was a blessing of the universe. Anyway, iam just curious of how much of you guys entered in this field with non tech background.",74,121,Entropico_88,2023-09-23 23:32:31,https://www.reddit.com/r/dataengineering/comments/16qim1i/what_is_your_background_education/,False,False,False,False
16c0p6h,Huge influx of recruiters messaging me on LinkedIn starting September 1st despite nothing changing on my LinkedIn… anyone else?,Is data engineering hiring ramping up all of the sudden? Could be that LinkedIn changed the recruiter search tool algo I suppose. Curious if anyone else is seeing the same.,79,50,Ok-Positive-7272,2023-09-07 00:08:01,https://www.reddit.com/r/dataengineering/comments/16c0p6h/huge_influx_of_recruiters_messaging_me_on/,False,False,False,False
16b3k7m,Ok to not really use any python as a DE?,"I work in a DE role but find myself doing about 99% of my job in SQL. I’m building out dim tables and scripting the stores procs needed for the ETL. I rarely use any python and when I do it’s to throw my stored proc into a really simple DAG and schedule it. 

Most of our data comes into a GCP bucket and I’m just pulling that data from a external table.

I just don’t seems to have a strong use case to use python in my day to day.",76,96,burningburnerbern,2023-09-05 23:11:57,https://www.reddit.com/r/dataengineering/comments/16b3k7m/ok_to_not_really_use_any_python_as_a_de/,False,False,False,False
14ul4sl,Is Kimball’s data modeling really functional?,"Does Kimball's star schema really work in practice or is it just a theoretical ideal model that companies try to implement?
Have you ever seen an by-the-book star schema implementation that solves business needs?

This is a sincere question. I see star schema being required as “state of the art” and standard knowledge for analytics engineering (or other data warehouse related positions), but is there an alternative?",75,56,Tough_Passion4667,2023-07-09 01:16:43,https://www.reddit.com/r/dataengineering/comments/14ul4sl/is_kimballs_data_modeling_really_functional/,False,False,False,False
14oqn1f,Seeking Guidance: Roadmap for Becoming a Competent Junior Data Engineer in 3 Months," I'm in need of your expertise and guidance as an aspiring data engineering student. Despite having knowledge in Python, SQL, Scala, Hadoop, Spark, and Airflow, I'm feeling overwhelmed and confused about where to start. Can you provide any advice or resources to set me on the right path?

If there's already a similar roadmap or guide available, please let me know where I can find it. Otherwise, I'd love to collaborate with you all to create a roadmap specifically tailored for junior data engineers with a similar background.

To kickstart the discussion, here are a few questions:

1. Essential skills and knowledge areas: What are the key competencies that a junior data engineer should focus on, considering prior knowledge in Python, SQL, Scala, Hadoop, Spark, and Airflow?
2. Recommended programming languages, frameworks, or tools: Are there any specific technologies that would complement my existing skill set and boost my growth as a data engineer?
3. Top resources for self-study: Can you recommend any books, online courses, tutorials, or other learning materials to further enhance my skills and knowledge?
4. Projects or exercises for hands-on experience: What practical projects or exercises would you suggest to gain hands-on experience and build a strong foundation?
5. Staying up-to-date with the latest trends: How can I stay informed about the latest advancements and trends in the field of data engineering?

I'm dedicated to investing my time and energy into this journey and excited to learn from your experiences and insights!

Thank you in advance for your support. Let's collaborate and create a comprehensive roadmap for aspiring junior data engineers!",75,30,Kratos_1412,2023-07-02 15:10:03,https://www.reddit.com/r/dataengineering/comments/14oqn1f/seeking_guidance_roadmap_for_becoming_a_competent/,False,False,False,False
12iaaka,I have been recently promoted to Data Architect Role. I need help on learning resources/pointers,"I have experience in DE for about 7 years and currently promoted to Data Architect role. I understand my responsibilities which include laying out data models, architecture diagrams, suggesting the best tools for a particular scenario. I have worked majorly in Azure and Databricks .

Please suggest the learning path / thinking methodology for me to be better at my current role.",79,32,inglocines,2023-04-11 05:53:27,https://www.reddit.com/r/dataengineering/comments/12iaaka/i_have_been_recently_promoted_to_data_architect/,False,False,False,False
zv2rzg,Best Place for Transformations: Python or SQL?,"I am a Data Engineer that has just started learning Python.  In my past experience, I always landed data in a STG table with as little changes as possible, then wrote SQL to load the Target and perform post-load logic.  As I'm working with pandas and data frames, I'm spending a lot of time on tasks like changing ""<NULL>"" values to NaN, parsing dates, setting datatypes, etc.

Before I go too far down that road, what are the opinions here about where to best perform these actions?  My alternative would be to define my STG tables with all ""Text"" columns, dump the data in raw, then perform all those actions with SQL on the way to my target.  Only downside I can foresee is possibly less options for error handling?  Is there a consensus on best practice for where to do these kinds of things?  Any insights are appreciated!",75,35,phobia42,2022-12-25 17:48:03,https://www.reddit.com/r/dataengineering/comments/zv2rzg/best_place_for_transformations_python_or_sql/,False,False,False,False
ybm75x,What would you want to hear and learn about in a PySpark workshop?,"I will present a virtual 1.5 hour PySpark workshop at a popular data science conference in a couple of weeks. It's introductory in nature. The target audience are absolute beginners and those with moderate PySpark experience.

I want to ensure that it is as useful as possible for anyone attending. In addition to hands-on technical exercises introducing the basics, I will go over PySpark's relevance for particular use-cases. This should help people make an informed decision rather than picking PySpark just for the heck of it.  


- How does the above sound? Is there anything else that you would wish to be covered in such a workshop?
- Have you attended anything similar in the past? What was your experience like?

For added context, I am an author of an O'Reilly book about PySpark and have worked as a data engineer for a few years.

-----

There are a lot of great ideas in the comments. Thanks everyone!
Considering the audience's DS background and the time restriction, advanced topics around optimization, performance or query plans may not fit. However, I will try to incorporate some of the accessible ideas (read low-hanging fruits).",76,37,analyticalmonk,2022-10-23 16:33:26,https://www.reddit.com/r/dataengineering/comments/ybm75x/what_would_you_want_to_hear_and_learn_about_in_a/,False,False,False,False
xijit6,Overwhelmed by sheer amount of resources,"I'm preparing for DE interviews, planning to apply after 3-4 months of preparation, but I feel overwhelmed by the sheer amount of resources i.e courses, books, YouTube channels etc available to prepare for it.

Currently I'm brushing up my python skills and there are so many courses and books available, I get confused and end up going through multiple resources fearing that I'd miss some concepts.

I need help to shortlist 1 or 2 best resources to study and prepare for these topics below:
1. Python (only relevant topics for data engineering), Scala
2. SQL
3. DBMS, Data warehousing,  Data modelling
4. Apache Hadoop and Spark

And if I missed out any other important topic, please share resources for those too, it'd be a great help.",76,26,ppv32,2022-09-19 17:52:50,https://www.reddit.com/r/dataengineering/comments/xijit6/overwhelmed_by_sheer_amount_of_resources/,False,False,False,False
x4yfed,layoffs in tech,Is ur company laying off staff? Is ur team reduced in size ?,77,126,fatbooi111,2022-09-03 16:24:19,https://www.reddit.com/r/dataengineering/comments/x4yfed/layoffs_in_tech/,False,False,False,False
v94z8g,Being a Data Engineer: are we destined to do less coding in the future?,"I have been a Data Analyst for a year when I slowly worked my way towards being a Data Engineer, because I just enjoyed coding a lot more. In my next role I could develop a bit in Python, work in SQL, but it still wasn't much. Same for my current role: i was excited to work with Spark and with the Azure stack, but from an intellectual challenge point of view, feels a bit underwhelming. Now with about 2 years of DE experience I have the occasional sense of something is missing: being in the flow when coding. Can't remember when was the last time I was so lost in development that hours just passed by.

I am guessing many of us got oriented towards Data Engineering, because we could work with data which we are interested in and we could also code which we also enjoy. Well, at least this was the case with me.

Couldn't help but notice that with every role I had, coding wasn't particularly on the agenda (more like occasional scripting). It is mostly about heading towards being able to configure stuff, clicking together stuff, some minor scripts here and there. How processes and things click together, link together or affect each other. For example, this squad wants to have this dataset, so we use Azure Data Factory to get the data from the DB, then put it into the data lake for them to consume. That's it.

It is safe to assume that there are jobs out there which are Data Engineers, with a hint of software engineering, but overall: does the role of Data Engineer inherently veers towards Tool knowledge rather than coding knowledge?

P.S: I thought about getting into SWE, but here comes the disadvantage - with Python and SQL, the supposedly main, but underutilized tools of a DE, that's a hard rocky road to take.",76,75,Labanc_,2022-06-10 10:46:17,https://www.reddit.com/r/dataengineering/comments/v94z8g/being_a_data_engineer_are_we_destined_to_do_less/,False,False,False,False
uvt1no,100+ Cheat Sheets for Data Science and Machine Learning,,76,3,TheInsaneApp,2022-05-23 05:25:54,https://www.theinsaneapp.com/2020/12/machine-learning-and-data-science-cheat-sheets-pdf.html#Data-Science-Cheat-Sheet,False,False,False,False
nxm7mw,"A book like ""Cracking The Coding Interview"" for Data Engineer Interview?","Could our veterans suggest good books about the interviewing questions and how to answer them perfectly for data engineer interviews? something like the great ""Cracking the coding interview"" book.",77,16,minafbeshay,2021-06-11 18:02:17,https://www.reddit.com/r/dataengineering/comments/nxm7mw/a_book_like_cracking_the_coding_interview_for/,False,False,False,False
kbynb2,Is there a SQL set of questions equivalent to the 75 Leetcode questions for FAANG interviews?,Im specifically looking to get into FANG as a DE. I’m sure a lot of you heard of the well known LC 75 questions made by the Facebook employee which covers most of the topics for data structure and algo questions for a SWE interview at FAANG. But is there a SQL set of questions for FAANG interviews? Specifically studying for data engineering position at FAANG. Basically for DEs they test you on DS/algos as well as very advanced sql. So I’m wondering if there is a SQL version for FAANG?,77,16,1337codethrow,2020-12-12 22:39:06,https://www.reddit.com/r/dataengineering/comments/kbynb2/is_there_a_sql_set_of_questions_equivalent_to_the/,False,False,False,False
icu95c,We built a public PostgreSQL proxy to 40k+ open government datasets,,75,5,mildbyte,2020-08-19 19:15:07,https://www.splitgraph.com/blog/data-delivery-network-launch,False,False,False,False
hbglth,Data Engineering Apprenticeship,"Professional lead DE here. I've been tossing an idea around for a while and wanted to get peoples' thoughts about it.  


I read or get asked quite frequently: ""What is data engineering?"" ""How do I start?"" ""What is important to learn?"" ""How would I transfer from a more traditional software engineering position into a DE role?""... and the list goes on.  


These questions are *really hard to answer* because the term ""data engineering"" can mean everything from database administration, to business intelligence, to dataops/devops, to data pipelining, to sysadmin, to just pure software engineering. It also means that ""data engineering"" is harder to teach, train for, or learn on a one-size-fits-all basis.  


My personal career learnings (software/data engineering for the past \~8yrs and a different career before that) have always been very hands-on, real-life, and immediately-applicable. I've learned ""why"" and ""what"" and ""how"" based on what companies need or want IRL.  


Which brings me to the ultimate question: ***Would people be interested in a*** [formal data engineering apprenticeship](https://www.bostata.com/training-data-professionals?utm_source=reddit&utm_medium=apprenticeship)***?***  


Is it scalable? Nope, and that's the point.  
Is it a proven model? Absolutely. Aristotle was taught by Plato. Philip Johnson studied with Breuer and Gropius. Most blue-collar industries have a significant apprenticeship-like component to professional development.",75,43,None,2020-06-18 15:19:47,https://www.reddit.com/r/dataengineering/comments/hbglth/data_engineering_apprenticeship/,False,False,False,False
1bt9j2j,Freelance data engineers - what's your story?,"What is it like going from full time employment to freelance?

How long have you been doing it? How many clients have you had? How much money have you made?

Any important things to keep in mind before making that move?

Do you regret it?",76,25,Novel-Tree7557,2024-04-01 17:15:55,https://www.reddit.com/r/dataengineering/comments/1bt9j2j/freelance_data_engineers_whats_your_story/,False,False,False,False
1bd500u,I feel like such a fraud,"Title. I have worked my way up in my organization from Service Desk Analyst to the title of a Data Engineernand have been with this company for almost 7 years. My primary tasks are maintening an AWS Glue pipeline into our data lake and bringing new tables from various sources into it. I know python somewhat well (I can pass some medium leetcode problems as a reference), my SQL skills are very basic, I feel like my organization is so far behind things techwise I am getting away with it and I can use chatgpt well enough to figure out anything I don't know. 

Typing this out I feel like I'm describing myself more competent than I feel. I have no formal education besides highschool and a boatload of AWS certifications I always have to study my butt off for. 

I'm not sure what the point of this post is, I just kinda needed to rant and get it off my chest to someone and maybe get some outside perspective.

How do you guys deal with thoughts like these? Do my skills line up with what's expected of a DE? ",75,31,Jaggedfel2142,2024-03-12 18:45:14,https://www.reddit.com/r/dataengineering/comments/1bd500u/i_feel_like_such_a_fraud/,False,False,False,False
1ap40zg,Data Engineering vs Data Engineering,"I've worked as a data professional with different job titles for about 10 years now and I'm noticing a pattern that I haven't seen explored before, and I'm interested in some structure.  I see two types of data engineering roles/teams and I'll try to describe them as best I can in a couple bullets.  


Type 1: the converted software engineer

* Comfortable in Scala/Rust/Spark/kubernetes
* Handles not just deployment but serving production systems
* Highly interested in optimizations as these save the company real money or just to make the problems tractable. 
* More likely to use streaming architectures.
* Further removed from the business problems.

&#x200B;

Type 2: the senior data scientist

* I've built a fancy model, now what?
* Has to set up their own OLAP architecture as the only dbs the software engineering team uses are OLTP, possibly even setting up their own data lakes as well
* Likely don't have replicated environments.
* Passes off data artifacts/exposes data products to the SE team for integration into core platform
* Focused on solving problems for the business, which necessitates data engineering.

&#x200B;

I could go on but I hope the distinction is clear.  Core skills SQL/Python/Data modeling/cloud are the same and of course many roles will be a hybrid.  Anyone have useful nomenclature for each of these archetypes?",75,23,apple_pie_52,2024-02-12 16:29:44,https://www.reddit.com/r/dataengineering/comments/1ap40zg/data_engineering_vs_data_engineering/,False,False,False,False
18kt2fc,2024 Data Engineering Top Skills that you will prepare for,"Are you thinking about getting new skills? What will you suggest if you want to be a updated data engineer or data manager?

Any certifications? Any courses? Any local or enterprise projects? Any ideas to launch your personal brand?",75,36,A-Global-Citizen,2023-12-17 22:26:53,https://www.reddit.com/r/dataengineering/comments/18kt2fc/2024_data_engineering_top_skills_that_you_will/,False,False,False,False
15nbhzf,dbt Labs to add usage-based pricing on top of their seat costs for dbt Cloud. $0.01 per model after free tier.,,73,70,PandaUnicornAlbatros,2023-08-10 12:43:39,https://www.getdbt.com/blog/consumption-based-pricing-and-the-future-of-dbt-cloud/,False,False,False,False
155rksi,At what point do you give up on training a co-worker?,"I have a jr co-worker I have been trying to train and enable to reduce work on my plate.

However it has been almost a year, and despite multiple documentation guides, live enablement sessions, working groups, and daily training sessions, I have not seen this teammate actually improve.

It to be quite frank has gotten to a point where it is more time-saving if I just do the work for him. Something I have been forced to do b/c they continually make the same errors over and over again.

At what point do I just give up on this Jr Employee and throw in the towel here?

Also, not sure if this is helpful, but this Jr Employee is an off-shore resource. But the training techniques I have used has worked for other off-shored an on-shored Employees with similar profiles

&#x200B;",75,96,recentcurrency,2023-07-21 15:41:36,https://www.reddit.com/r/dataengineering/comments/155rksi/at_what_point_do_you_give_up_on_training_a/,False,False,False,False
11bhvux,Do Right Joins even matter?,"This is one of those out-of-the-blue thoughts that you get randomly. I am an expert in sql with many years of experience, but have yet to use Right Joins lol. Is there any specific reason or use-case for this type of join?",75,44,TheQuiteMind,2023-02-25 10:29:14,https://www.reddit.com/r/dataengineering/comments/11bhvux/do_right_joins_even_matter/,False,False,False,False
ze798y,Interview coding question that I couldn't solve,"Hi,

I was asked this question for a Senior Data Engineer interview. A cycling race is composed of many legs. Each leg goes from one city(A) to another(B). Cyclists then rest for the night and start the next leg of journey from (B) to (C). If the legs are represented by Tuples (A,B), (B,C), (C,D)...and given a list of tuples out of order example \[(C,D),(A,B),(B,C)...\] can you print out the correct order of cities in the race (example ""A B C D.."")

Example \[(A C) (B D) (C B)\]

output: A C B D \[(C B) (D C) (B E) (A D)\] output A D C B E.

I was supposed to write code in C#. I was unable to solve this. This was my thought process. Treat it like linked list. If List-> next is null then it's the end of race and if List->prev is null it's the Start of race.

Can anyone guide me with the coding part?",74,47,meridian_12,2022-12-06 13:48:31,https://www.reddit.com/r/dataengineering/comments/ze798y/interview_coding_question_that_i_couldnt_solve/,False,False,False,False
x494kp,Wanted to share my open source incremental ETL framework: Meerschaum,"Hey fellow data engineers! 👋

For the last two years, I've been building my ETL framework [Meerschaum](https://github.com/bmeares/Meerschaum). It's been an exciting journey ― I've deployed several Meerschaum stacks for some contracts, it's running in production at my last job, and I've been putting a lot of work recently to address some pain points in my current role. It's made building smaller pipelines a breeze, and I thought some engineers here might be interested in how the framework can help make your jobs easier.

In a nutshell, Meerschaum updates your tables according to the data you ingest. Here's a code snippet to illustrate what I mean (gotta say, I wish Reddit supported syntax highlighting in Markdown):

    import meerschaum as mrsm
    pipe = mrsm.Pipe(
        'foo', 'bar', # labels, more on this later.
        target = 'my_table', # optionally set target table name instead of ""foo_bar"".
        columns = {'datetime': 'timestamp', 'id': 'id'}, # indices
        instance = 'sql:local', # built-in SQLite for your convenience.
    )
    docs = [
        {'timestamp': '2022-01-01', 'id': 1, 'value': 1.0},
        {'timestamp': '2022-01-02', 'id': 1, 'value': 2.0},
        {'timestamp': '2022-01-03', 'id': 1, 'value': 3.0},
    ]
    
    ### Insert the rows into the table.
    pipe.sync(docs)
    
    print(pipe.get_data())
    #    timestamp  id  value
    # 0 2022-01-03   1    3.0
    # 1 2022-01-02   1    2.0
    # 2 2022-01-01   1    1.0
    
    
    ### Duplicates are ignored.
    pipe.sync(docs)
    
    print(pipe.get_data())
    #    timestamp  id  value
    # 0 2022-01-03   1    3.0
    # 1 2022-01-02   1    2.0
    # 2 2022-01-01   1    1.0
    
    
    ### Rows with existing indices (timestamp and id) are updated.
    pipe.sync([{'timestamp': '2022-01-01', 'id': 1, 'value': 100.0}])
    
    print(pipe.get_data())
    #    timestamp  id  value
    # 0 2022-01-03   1    3.0
    # 1 2022-01-02   1    2.0
    # 2 2022-01-01   1  100.0

This example demonstrates the expected behavior of syncing. But the secret sauce lies in how the datetime index is used. Whenever new rows are ingested, Meerschaum pulls a sample from your table in the given time range. That is, if your table is massive and you frequently have small updates, this behavior will save a lot of wasted computing time.

For example, suppose we sync the following document:

    pipe.sync([
        {'timestamp': '2022-01-01 12:00:00', 'id': 1, 'value': 123.0}
    ])

The underlying query to retrieve the existing sample would look like this:

    SELECT *
    FROM {target_table}
    WHERE
      ""timestamp"" >= CAST('2022-01-01 11:59:00' AS TIMESTAMP)
      AND
      ""timestamp"" <  CAST('2022-01-01 12:01:00' AS TIMESTAMP)

Note that the default behavior only bounds on the datetime axis. I've benchmarked other [experimental syncing methods](https://github.com/bmeares/syncx) for my [master's thesis](https://tigerprints.clemson.edu/all_theses/3647/) and found that the extra processing needed to reduce the sample size isn't usually worth the saved bandwidth.

The other major part of the framework is that pipes have connectors, specifically the [plugin system](https://meerschaum.io/reference/plugins/). For example, consider a file `~/.config/meerschaum/plugins/example.py`:

    # ~/.config/meerschaum/plugins/example.py
    
    from typing import List, Dict, Any
    import datetime
    import random
    import meerschaum as mrsm
    
    required = ['requests']
    
    def register(pipe: mrsm.Pipe, **kw: Any) -> Dict[str, Any]:
        """"""Return the pipe's parameters.""""""
        return {
            'columns': { # Indices
                'datetime': 'timestamp',
                'id': 'id_col',
            },
            'num_rows': 1000, # Custom key
        }
    
    
    def fetch(pipe: mrsm.Pipe, **kw: Any) -> List[Dict[str, Any]]:
        """"""Do some magic and return a list of documents or a dataframe.""""""
        import requests
        req = requests.get(""http://worldtimeapi.org/api/timezone/Etc/UTC"")
        if not req:
            raise Exception(""Oopsie!"")
    
        timestamp = datetime.datetime.fromisoformat(req.json()['datetime'])
    
        return [
            {
                pipe.columns['datetime']: timestamp,
                pipe.columns['id']      : i,
                'some_other_value'      : random.uniform(1, 100),
            } for i in range(pipe.parameters['num_rows'])
        ]

This Python module can act as a connector for a pipe, denoted by `plugin:example`. Calling a bare `pipe.sync()` will invoke the function `fetch()`and sync the returned documents:

    import meerschaum as mrsm
    pipe = mrsm.Pipe('plugin:example', 'foo')
    pipe.sync()
    print(pipe.get_data())
    #                      timestamp  id_col  some_other_value
    # 0   2022-09-02 18:20:25.034960       0         86.206002
    # 1   2022-09-02 18:20:25.034960       1         52.897848
    # 2   2022-09-02 18:20:25.034960       2          5.324236
    # 3   2022-09-02 18:20:25.034960       3         68.776668
    # 4   2022-09-02 18:20:25.034960       4          1.641710
    # ..                         ...     ...               ...
    # 995 2022-09-02 18:20:25.034960     995         89.157034
    # 996 2022-09-02 18:20:25.034960     996         22.009482
    # 997 2022-09-02 18:20:25.034960     997         71.547525
    # 998 2022-09-02 18:20:25.034960     998         48.389565
    # 999 2022-09-02 18:20:25.034960     999         38.341501

Because the behavior of the function is defined by `pipe.parameters`, you can create as many pipes as you like with this plugin as its connector, each with different behaviors.

One last feature I'd like to highlight is how you can set a SQL query as a pipe's connector:

    # Environment variable for the DB connector:
    # MRSM_SQL_FOO=sqlite:////tmp/foo.db
    
    import meerschaum as mrsm
    pipe = mrsm.Pipe(
        'sql:foo', 'bar',
        columns = {
            'datetime': 'timestamp',
            'id': 'id_col',
        },
        parameters = {
            'query': """"""
                SELECT timestamp, id_col, (val * 2) AS val
                FROM src
            """"""
        }
    )
    pipe.sync()

This will wrap the query definition in a CTE and only select rows newer than the most recent timestamp (you can tweak this behavior). This is a common pattern for incrementally updating time-series tables and will save you the hassle of writing and ETL script from scratch. Think of it as an incrementally materialized view.

There's a whole lot more that you can do with the framework, but this post is getting kinda long. Please check out the [project homepage](https://meerschaum.io/) for more details, and I'd really love know what y'all think! Can you see a use case for the framework in your stack?",76,9,Obliterative_hippo,2022-09-02 18:41:57,https://www.reddit.com/r/dataengineering/comments/x494kp/wanted_to_share_my_open_source_incremental_etl/,False,False,False,False
wvg8pf,Update: Journey to Data Engineering,"Original post: [Journey to Data Engineering](https://www.reddit.com/r/dataengineering/comments/mck2td/journey_to_data_engineering/) 

About a year and a half ago I made a post about getting a Business Intelligence Developer job and looking to move towards Data Engineering in the future-- now, I'm happy to update that I got an offer from my current company to move to a Data Engineering position in the analytics department.

According to glassdoor, maybe I'm underpaid at 80k for 1.5 YOE in the midwest US, but at the end of the day I'm happy to get the experience and the opportunity to upskill on the job. 

For those looking to break into data engineering, I am a firm (though perhaps biased) believer that the easiest route is through entry level business intelligence/data analytics roles.

Thanks to the community for helpful responses and words of encouragement!",77,44,Touvejs,2022-08-23 05:28:10,https://www.reddit.com/r/dataengineering/comments/wvg8pf/update_journey_to_data_engineering/,False,False,False,False
v54fpm,What data tools are you using for your daily work?,"Hi, Guys,

What tool are you using for your daily data work?  Are those tools in modern data stack popular?  For the list of the following tools, which one are you using?

Data Warehouses: Snowflake, Databricks

Data Integration: Fivetran, Airbyte, Stitch, Segment

Data Modeling: dbt

BI: Mode, Preset, ThoughSpot, Sigma Computing, Hex",71,91,laoyan0523,2022-06-05 03:01:04,https://www.reddit.com/r/dataengineering/comments/v54fpm/what_data_tools_are_you_using_for_your_daily_work/,False,False,False,False
pvvtgj,What is a pipeline really ?,"I am primarily a ETL/Database developer and I am responsible for maintaining ETL workflows/ stored procedures that load data from applications to my Enterprise DWH. In my view, anything that is used for data transport is a pipeline. are ETL workflows also pipelines ? I just have trouble with this terminology . Can some one clarify what is and what's not a pipeline.",75,40,satz3,2021-09-26 15:16:25,https://www.reddit.com/r/dataengineering/comments/pvvtgj/what_is_a_pipeline_really/,False,False,False,False
ols39l,Repeated interview question: what do you if your ETL fails?,"So far I was asked this question twice in different interviews. It is a very generic question that comes in flavours like:

>\- what do you if your ETL suddenly cannot load the data because it lost the data warehouse connection?, what do you do?  
>  
>\- what do you if your ETL fails while transforming the data because of any reason?

I have some blur answers for this, but after two times still I feel I don't have an elegant reasoning about these scenarios.

Can someone help with this?. Surely there are multiple things to consider or useful examples, etc, but.... any help would be very appreciated. Thanks!",75,39,Complex-Stress373,2021-07-16 22:53:22,https://www.reddit.com/r/dataengineering/comments/ols39l/repeated_interview_question_what_do_you_if_your/,False,False,False,False
iz2ucy,What tools will be a must-have for the data engineering stack in 5 years?,"Snowflake's IPO is validation of the increasing importance of the data warehouse/data engineering as a value add for any company serious about data. What tools will be integral to the DS/data engineering stack in five years? What technologies will be the ""cloud warehouse"" of 2025? 

This article has some thoughts on how data observability will play into this future: [https://towardsdatascience.com/data-observability-the-next-frontier-of-data-engineering-f780feb874b?source=friends\_link&sk=c6f791910928c8d0d59000884a91917e](https://towardsdatascience.com/data-observability-the-next-frontier-of-data-engineering-f780feb874b?source=friends_link&sk=c6f791910928c8d0d59000884a91917e)",77,61,mkvor8,2020-09-24 18:29:42,https://www.reddit.com/r/dataengineering/comments/iz2ucy/what_tools_will_be_a_musthave_for_the_data/,False,False,False,False
1ccodiy,Comparison of Different Stream Processing Platforms,,74,39,wanshao,2024-04-25 10:17:54,https://i.redd.it/4jqfd7fnplwc1.png,False,False,False,False
1bu3v0b,"As data engineer, how much non-programming work do you do at work and what type of work it is?","By programming I mean all type of programming, including SQL and etc.",75,50,Alex_df_300,2024-04-02 17:00:41,https://www.reddit.com/r/dataengineering/comments/1bu3v0b/as_data_engineer_how_much_nonprogramming_work_do/,False,False,False,False
1avj2k2,What was your biggest challenge getting into data engineering?,"E.g. for me, one challenge is that I could never really practice on the cloud platforms unless I was using it at work (unless I pay for it myself). Also, it's hard to learn best practices in engineering with only fake / clean data (since the complexities, volumes, watchouts just arent the same)-- in which case, making pipelines with real data is just more effective, and hence only doable again in a work setting",74,44,Sensitive-Soup4733,2024-02-20 14:34:43,https://www.reddit.com/r/dataengineering/comments/1avj2k2/what_was_your_biggest_challenge_getting_into_data/,False,False,False,False
1advvbm,is programming in data engineering as complex as in software engineering?,in general,74,39,ryanwolfh,2024-01-29 13:55:59,https://www.reddit.com/r/dataengineering/comments/1advvbm/is_programming_in_data_engineering_as_complex_as/,False,False,False,False
1790jby,"You've just joined a new company who do everything in Excel, but....","There's a senior manager who's keen to modernize their approach to data, but doesn't know what they want. What are you asking for / putting in place?",75,62,Dog_In_A_Human_Suit,2023-10-16 07:40:29,https://www.reddit.com/r/dataengineering/comments/1790jby/youve_just_joined_a_new_company_who_do_everything/,False,False,False,False
15i8nlo,How did you go about learning K8s,"Seems more and more necessary to learn K8s in this space.

I stumbled into data engineering from a data science background. By virtue of working in a small team I have been exposed to some basic cloud infra stuff like using the cloud provider's cli to create resources and set properties etc.

I don't understand very much at all about networking apart from that it's a pain in my ass. 

I do feel relatively comfortable with using docker.

Can someone who has learned kubernetes recently give me any guidance about how best to go about learning?

My primary use cases would be running workloads adaptively on kubernetes. Specifically it would be nice to learn enough that I can create a nice workflow using Ray (KubeRay) to run ML training workloads.",76,37,BoiElroy,2023-08-04 19:02:12,https://www.reddit.com/r/dataengineering/comments/15i8nlo/how_did_you_go_about_learning_k8s/,False,False,False,False
14y3s4r,"Is airflow better for triggering jobs in a data pipeline, or actually running the jobs itself?","At work we’re building a lil’ pipeline. Nothing fancy, just reads data from a few API’s, normalizes them and sticks em in a table in our DB. 

We’re trying out airflow for this, and we’ve been putting all of the actual code into DAGs in airflow. So, all python.

I saw another post that mentioned how airflow is mostly a “job scheduler”, which made me second-guess keeping all of our code in airflow DAGs. So I’m wondering: do y’all use airflow primarily as a scheduler for jobs that are owned by other services, or do you also rely on it to run business logic?

If that’s too vague, here’s a specific example:
Ideally, I’d have all of my data pulling/normalizing code in rust. We already have a nicely setup rust environment, and that’s how I would handle our pipeline if it was just gonna be rust scripts and a bunch of cron jobs. 
But since airflow has so many easy integrations, we decided just to let airflow (and thusly python DAGs) handle all of the data pulling and normalization. 

Is the “correct” way to use airflow:
1) having airflow trigger rust scripts?
2) having airflow handle everything from within airflow?",76,29,chamomile-crumbs,2023-07-12 23:06:48,https://www.reddit.com/r/dataengineering/comments/14y3s4r/is_airflow_better_for_triggering_jobs_in_a_data/,False,False,False,False
14trirs,"Noob question, but how do you deploy and automate a data pipeline?","I’m relatively new to data ops and have a learned a bunch out of necessity, but I really like DE and want to pursue it further. One thing that’s always baffled me though is how you actually deploy and automate a pipeline. I’ve done a few snowflake labs where I did this via python on a local machine, but what’s the next step up?

Let’s say I want to take data from Postgres in AWS RDS and send it to Snowflake. Do I write a python script and upload to EC2 then schedule it to run via a cron job?",76,31,PablanoPato,2023-07-08 02:47:32,https://www.reddit.com/r/dataengineering/comments/14trirs/noob_question_but_how_do_you_deploy_and_automate/,False,False,False,False
13rgq25,Is it normal for companies to retain all raw data?,"I work in the IoT manufacturing space and each machine can collect upwards of 50 million points per year. For display/analysis purposes that will be aggregated, however should the raw values still be stored somewhere? That seems like a lot to store. Is it acceptable to aggregate across much smaller intervals to reduce the amount of “raw” data?",71,57,Reddit_Account_C-137,2023-05-25 12:40:27,https://www.reddit.com/r/dataengineering/comments/13rgq25/is_it_normal_for_companies_to_retain_all_raw_data/,False,False,False,False
13622x1,The SQL Unit Testing Landscape: 2023,,72,6,ryan_CritHits,2023-05-02 22:14:01,https://towardsdatascience.com/the-sql-unit-testing-landscape-2023-7a8c5f986dd3,False,False,False,False
133kbq3,"Which certification to get, Snowflake or Databricks?",We are given the choice to be certified in either Snowflake or Databricks. Which do you guys think is better in terms of career progression? We can only choose to specialize in one (in terms of company sponsorship).,70,43,_barnuts,2023-04-30 09:09:37,https://www.reddit.com/r/dataengineering/comments/133kbq3/which_certification_to_get_snowflake_or_databricks/,False,False,False,False
12otxy6,[Interview prep] Anyone in Zach wilson's data engineering bootcamp?,"Zach wilson is a data engineer at Airbnb and his linkedin post says that he is working on his first professional data engineering bootcamp.

Curious to know the reviews of it, if anyone's been there.",73,72,lnx2n,2023-04-16 23:52:32,https://www.reddit.com/r/dataengineering/comments/12otxy6/interview_prep_anyone_in_zach_wilsons_data/,False,False,False,False
117x39e,What lakehouse stack do you use,"Please share what lakehouse stack do you use. 

For example: 

* Storage: S3
* File formats: parquet
* Table formats: Iceberg
* Realtime: Clickhouse
* Modeling/Transformations: dbt, Spark
* Orchestration: Airflow
* Semantic layer: Cube
* BI: Tableau
* Data quality: Great expectations
* Data catalog: Amundsen
* ML: mlflow, kubeflow
* Other: lakeFS",74,56,romanzdk,2023-02-21 07:01:01,https://www.reddit.com/r/dataengineering/comments/117x39e/what_lakehouse_stack_do_you_use/,False,False,False,False
10esybb,DW toolkit book by Ralph Kimball,,74,15,rajekum512,2023-01-18 00:40:43,https://i.redd.it/yc0bxaypoqca1.jpg,False,False,False,False
yt56bf,"A step-by-step tutorial on how to build a Web application, combining the Streamlit Python library and Versatile Data Kit",,73,0,zverulacis,2022-11-12 12:03:10,https://towardsdatascience.com/how-to-build-a-web-app-with-data-ingested-through-versatile-data-kit-ddae43b5f62d,False,False,False,False
u6d2cd,What do you all think of vendors on this sub low-key trying to push their products,I've noticed quite a lot of those accounts while lurking the sub for interview tips and to see generally what everyone is working on/with.,73,25,CesQ89,2022-04-18 13:23:10,https://www.reddit.com/r/dataengineering/comments/u6d2cd/what_do_you_all_think_of_vendors_on_this_sub/,False,False,False,False
tpy5yd,Red flags to look out for when joining a Data team?,"https://eugeneyan.com/writing/red-flags/

This was written from the perspective of a Data Scientist. What would be red flags for a Data Engineer interviewing with prospective teams?",73,31,brainfuckguru,2022-03-28 01:32:24,https://www.reddit.com/r/dataengineering/comments/tpy5yd/red_flags_to_look_out_for_when_joining_a_data_team/,False,False,False,False
sfme7l,Can someone help me understand why data batch processing and data streaming processing pose such different challenges in data management?,I am a ds and I see the differences mentioned everywhere when it comes to data management. I am having trouble understanding why this is the case if anyone care sharing some insights. Thank you!,75,14,digital-bolkonsky,2022-01-29 17:02:40,https://www.reddit.com/r/dataengineering/comments/sfme7l/can_someone_help_me_understand_why_data_batch/,False,False,False,False
pmtenl,Data warehouse interview question,"Hi All,

In one of my recent interviews, I got this question -
How do you build the data warehouse from scratch?

My question is - 
What would be the sequence while answering this question?

Thanks in advance",77,50,Delicious_Attempt_99,2021-09-12 14:13:25,https://www.reddit.com/r/dataengineering/comments/pmtenl/data_warehouse_interview_question/,False,False,False,False
gm2rg7,Airflow: how and when to use it,,74,23,schoolgurllou,2020-05-18 14:46:13,https://medium.com/@alexagriffith/airflow-how-and-when-to-use-it-2e07108ac9f5,False,False,False,False
1bvdk95,How often do you write complex SQL solutions that eat at your soul?,"1. Happy that it works
2. Don’t know what you just really did
3. Used all the SQL tricks and idioms in the book
4. Tried to keep it as declarative versus imperative as possible 
5. Good luck to the next person who tries to deal with it (probably you)

Is this the occasional price of doing business in DE?  Or, am I being a (1/10)X Data Engineer?
",73,43,KheodoreTaczynski,2024-04-04 03:50:34,https://www.reddit.com/r/dataengineering/comments/1bvdk95/how_often_do_you_write_complex_sql_solutions_that/,False,False,False,False
1brp3kj,Game plan to become a data architect,"I want to become a Data architect someday. My goal is to get to a place where I can pretty easily beat out my competition because of my experience and capabilities to setup a data infrastructure for any business. I’m not the smartest guy in the room, so I am not shooting to become a Data architect for Netflix or other similar top-tier companies. I will be happy to become a Data architect for a stable company where I can see myself retiring eventually.

I am very far away from that currently. I have been in IT industry for ~6 years now, with ~5 years in data engineering and ~1 year in software engineering. I have expert skills in SQL and intermediate skills in data analysis, python, and some AWS cloud technologies used in building data pipelines (lambda, S3, Glue, Postgres, SFTP, API Gateway, etc). I am a data engineer, and I am “okay” at my job. I can build you a working pipeline with the technologies you give me, but it might be crap to an experienced data architect/engineer. I don’t know much about data modeling, data design patterns, automated unit testing and integration testing for data, building dashboards, etc. 

It’s not possible to learn all hundreds of technologies, concepts, and practices in data engineering. I have to focus on learning the fundamentals, separate technologies into categories and learn a few technologies from each category, and work on challenging data engineering projects applying what I learned.

It’s easier said than done though. I would love to hear some feedback from the data engineering community. How do you suggest I plan my learning and growth for the next 5-10-20-30-40 years so I can become a competent data architect that businesses can trust?",75,52,iemback,2024-03-30 18:46:20,https://www.reddit.com/r/dataengineering/comments/1brp3kj/game_plan_to_become_a_data_architect/,False,False,False,False
18lsb9p,I recorded a crash course on Polars library of Python (Great library for working with big data) and uploaded it on Youtube,"Hello everyone, I created a crash course of Polars library of Python and talked about data types in Polars, reading and writing operations, file handling, and powerful data manipulation techniques. I am leaving the link, have a great day!!

[https://www.youtube.com/watch?v=aiHSMYvoqYE&list=PLTsu3dft3CWiow7L7WrCd27ohlra\_5PGH&index=6&t=689s](https://www.youtube.com/watch?v=aiHSMYvoqYE&list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&index=6&t=689s)",74,11,onurbaltaci,2023-12-19 03:44:20,https://www.reddit.com/r/dataengineering/comments/18lsb9p/i_recorded_a_crash_course_on_polars_library_of/,False,False,False,False
17yy1wp,How do you sell the value of Data Engineering at your org?,"Title is the question, how would you change the mind of someone who is ambivalent?",73,27,DesperateForAnalysex,2023-11-19 14:13:07,https://www.reddit.com/r/dataengineering/comments/17yy1wp/how_do_you_sell_the_value_of_data_engineering_at/,False,False,False,False
175hslp,What the fuck *actually is* a Data Mart and why should I build one?,"I don't mean the definition of ""Data Mart"", that we all know. Subset of data... aimed at specific business teams... pre-aggregated... etc...

What I don't understand, and can't seem to find actual examples online that draw from realistic datasets, is what it is supposed to look like **in practice.**

Is it just a bunch of views that expose only certain tables to certain users?

Are there any aggregations and if yes, how does one decide which ones to build?

If we're building aggregations, thus losing in flexibility, does that mean that one has to aggregate for each necessary KPI the business users demand? Sounds like a massive PITA.

--- 

I often wonder, isn't it much easier to expose a single common layer (idk like OBT, or even a star schema if your analysts are good) to whatever BI software(s) one's using, and let the analysts build whatever aggregation/metric comes to mind over there, rather than at the end of ELT? 

Many thanks in advance for your help.",70,104,wtfzambo,2023-10-11 16:08:39,https://www.reddit.com/r/dataengineering/comments/175hslp/what_the_fuck_actually_is_a_data_mart_and_why/,False,False,False,False
11q1gyr,It's all gone... in a sec,"It's really sad and frustrating to lose so much hard work and a possible opportunity due to a stupid mistake.

For the past six days, I have been working day and night on developing an entire data streaming system using Docker (from streaming APIs and Kafka consumers/producers to the MLspark model). This was a task for a company I am interviewing with for a big data engineering position.

Every day, I told myself I needed to start a repository or back up the code somewhere, but for an unknown reason, I didn't. I kept procrastinating.

I was so excited to finish the project and share it with the interviewer on GitHub as soon as possible. But I told myself, ""One last test,"" and that's when I accidentally deleted all my code.

The main project directory was mounted by the Docker Spark container, which would write the output to the folder provided. However, it needed to remove or empty everything in the provided directory before writing. And that's how I lost everything.

I was so pissed off and spent three hours trying different methods to retrieve my work, but I couldn't. Now, I don't even feel like coding anymore.

(Note: I know I should have versioned the code with Git, which I usually do. But this time, I thought, ""What could go wrong?"")",71,53,Taylankab,2023-03-13 05:49:03,https://www.reddit.com/r/dataengineering/comments/11q1gyr/its_all_gone_in_a_sec/,False,False,False,False
ytg56j,My company has around 400 tables in production and only 80 of them have a unique identifier (PK). What do I do?,We are currently going through a large data warehouse redesign and have discovered this issue. It has come up in the past when I’ve been asked to integrate data but haven’t been able to do so because I can’t make them relate. What do you do in this situation?,72,47,Doyale_royale,2022-11-12 19:30:38,https://www.reddit.com/r/dataengineering/comments/ytg56j/my_company_has_around_400_tables_in_production/,False,False,False,False
wokrau,Advice on how to keep learning?,"I've personally found this subreddit and talking to other data engineers in real life to be some of the most useful tools for learning. I always get to hear how other people are doing things, new ideas, etc.

If I need to deep dive into anything, I usually just go on YouTube, read some blog post, or study some github repos. I also attend virtual conferences occasionally.

I feel like a may need to pick up a few books to get even more mastery, but I literally haven't picked up a book in like 5 years lol.

How do you all learn and keep up to date? Any advice or recommendations? Maybe my ways are a bit amateurish haha(I'm still young in my career)",70,8,Justanotherguy2022,2022-08-15 00:07:29,https://www.reddit.com/r/dataengineering/comments/wokrau/advice_on_how_to_keep_learning/,False,False,False,False
vl5d3d,Is a masters worth it?,"Hey everyone,

I’m considering applying to Georgia Tech’s online MSCS to further my CS/Data Eng knowledge.

Right now I work as a business intelligence analyst, starting to work with dbt and the logical next step In my career is moving into a data engineer role.
My background is in chemical engineering and I have no formal training in software. 

I know the MSCS may not be focused totally on data engineering but I see two benefits
1) Make me a better and more confident programmer 
2) There are courses that are still relevant to DE which I will benefit from 

Curious to hear everyone’s though/ alternative suggestions.",70,51,Tepaps,2022-06-26 14:00:57,https://www.reddit.com/r/dataengineering/comments/vl5d3d/is_a_masters_worth_it/,False,False,False,False
ne3fuk,What tools and technologies do you use the most as a data engineer?,"In your case, what tools/technologies/programming languages do you use at your job?",74,34,jana_50n,2021-05-17 01:36:47,https://www.reddit.com/r/dataengineering/comments/ne3fuk/what_tools_and_technologies_do_you_use_the_most/,False,False,False,False
n8gloz,Bad coding practices followed in team,"Hi! So it has been nearly a year or so since i shifted in a data engineering team. The work there is quite fun. We build data pipelines, streaming solutions etc on cloud. There is always something new which is nice to have. But the issue is that there are some bad developers in the team and they are quite senior to me. They were working with data warehousing stuff where they mostly had to deal with more sql and less scripting. Now they work with more scripting side and they don’t care about the code as long as it is working. When I comment on their PR’s they get annoyed that why do I care about the code. It’s doing the task and that should be the main goal. I am not that picky but when there are 5 for loops for a thing that can be done in a single loop I can’t ignore that. Maybe it’s because I am from a SWE background and that’s why it bothers me a lot. What do you guys think? Have you guys experienced that? Any advice on how to deal with this situation? Should i just suck it up and not do anything about the code as long as it is working?",74,31,dekardar,2021-05-09 15:32:49,https://www.reddit.com/r/dataengineering/comments/n8gloz/bad_coding_practices_followed_in_team/,False,False,False,False
lmod2e,Curated Github repository on how organizations around the world use dbt,"I've set up a knowledge repository of dbt best practices. The idea is to gather dispersed online resources and curate them in a single repository.

Contributions are more than welcome!

https://github.com/smomni/howtheydbt",76,2,smomni,2021-02-18 14:48:05,https://www.reddit.com/r/dataengineering/comments/lmod2e/curated_github_repository_on_how_organizations/,False,False,False,False
ibceit,Start writing better DAGs by discovering Apache Airflow Best Practices!,,73,6,marclamberti,2020-08-17 11:31:47,https://marclamberti.com/blog/apache-airflow-best-practices-1/,False,False,False,False
hc9ogc,Data Engineering Youtube Channel!,"Hey guys!

I recently decided to create a Youtube Channel dedicated to discussing data engineering topics and just released my first video. It was just a way to start, more interesting, and hands-on videos will be released soon but for the time being it would mean a lot if you could check it out and let me know what you think.

 [https://www.youtube.com/channel/UCBT9cPM6LbCAIrFgg9Mi9YQ](https://www.youtube.com/channel/UCBT9cPM6LbCAIrFgg9Mi9YQ) 

Show some love! Thanks",72,11,olympuk,2020-06-19 21:53:40,https://www.reddit.com/r/dataengineering/comments/hc9ogc/data_engineering_youtube_channel/,False,False,False,False
1c4w6cj,Databricks to snowflake,"Company shifting from databricks to snowflake.

Much of the work is orchestration of notebooks written in pyspark and sparksql, jobs that are executed by API which kick of the orchestrstion of our notebook curating data from tables and writing them into silver / gold layers.

For those familiar with the migration between these two platforms, what should I prepare for / be aware of on the snowflake side.",74,23,Operation_Smoothie,2024-04-15 20:10:27,https://www.reddit.com/r/dataengineering/comments/1c4w6cj/databricks_to_snowflake/,False,False,False,False
1borix1,History of questions asked on stack over flow from 2008-2024,"This is my first time attempting to tie in an API and some cloud work to an ETL. I am trying to broaden my horizon. I think my main thing I learned is making my python script more functional, instead of one LONG script.

My goal here is to show a basic Progression and degression of questions asked on programming languages on stack overflow. This shows how much programmers, developers and your day to day John Q relied on this site for information in the 2000's, 2010's and early 2020's. There is a drastic drop off in inquiries in the past 2-3 years with the creation and public availability to AI like ChatGPT, Microsoft Copilot and others.

I have written a python script to connect to kaggles API, place the flat file into an AWS S3 bucket. This then loads into my Snowflake DB, from there I'm loading this into PowerBI to create a basic visualization. I chose Python and SQL cluster column charts at the top, as this is what I used and probably the two most common languages used among DE's and Analysts.",69,37,Fraiz24,2024-03-27 02:59:57,https://www.reddit.com/gallery/1borix1,False,False,False,False
17tqj7b,Fellow DEs how do you manage data quality?,"There are so many apps, products, and libraries out there that help with data quality; Some tools are marketed toward enterprise-level while others are for data-pipeline monitoring. I am currently in a position to consider and experiment with data quality tools for my team, so I am curious about what folks are using right now.

What is your set up and how does it help you?

&#x200B;

Given that data quality is a very broad topic, and I'm not necessarily looking for a comprehensive solution -  just whatever you've found to be the most effectively for your team / org. 

Personally, I've mostly used custom scripts and tables to track the metadata I need (pipeline runs, data source baseline metrics, unit tests) with a Power BI dashboard to visualize the most relevant information for that project.",70,49,Raydox328,2023-11-12 18:39:38,https://www.reddit.com/r/dataengineering/comments/17tqj7b/fellow_des_how_do_you_manage_data_quality/,False,False,False,False
17qa1p6,How many hours do you work a week?,I’m new to my team and the field this year. We’ve all been working about 60 hours a week for the last 6 weeks. It seems like this may be the new norm. Is this normal in your experience?,72,90,HamburglerAlarmist,2023-11-08 01:14:14,https://www.reddit.com/r/dataengineering/comments/17qa1p6/how_many_hours_do_you_work_a_week/,False,False,False,False
15hvadk,I created a chart to explain why 90% of data setups contain custom data pipelines,,73,94,Thinker_Assignment,2023-08-04 09:30:43,https://i.redd.it/a3tqkpumb2gb1.png,False,False,False,False
154zbbu,What is the most impressive thing you’ve ever done as a data engineer?,"Did you perhaps singlehanded do [data engineering task]?

Or did you solve [difficult data engineering problem]?",72,107,SeriouslySally36,2023-07-20 18:38:57,https://www.reddit.com/r/dataengineering/comments/154zbbu/what_is_the_most_impressive_thing_youve_ever_done/,False,False,False,False
11qpfwo,What are some of the sticky problems in your data pipelines?,"Hi Folks,I am researching the challenges of data engineering to develop a deeper understanding of the problems and challenges faced by the teams building data products.

I used to be a data engineer between 2009 and 2014. In 2014, I got into product management and I am currently in a product lead role. I have always worked products which included a large amount of analytics, insights, predictive models, machine learning in software companies serving healthcare, surveillance, automotive, and ecommerce.

In my experience over the past 14 years, the data tooling ecosystem has expanded a lot.However, I am still in a scenario where the cost of data infrastructure and tooling is expensive.

* Projects are complex and long drawn.
* It is super hard to solve basic issues of data quality even reactively.
* Maintaining trust in the data assets is really hard.
* Data literacy of decision makers is not up to the mark in a lot of cases.
* Stakeholders expect miracles and stuff to just work.
* Very few people can explain the attributes, the calculations, the metrics, the insights, and the implications.
* Everyone is just promising stuff and punting the inevitable reality of face the hard problem and solve it properly.

I am just trying to research and gather inputs from the community on the nagging challenges of building products now to inform my product development and to inform a course that I am building to develop data product managers (because it is really difficult to find candidates to hire)

My question for you is:

* What are your top 3 challenges in engineering data flows and pipelines?
   * Is it the data inventory, quality, governance, accessibility, etc.?
   * Is the infrastructure, the complexity of building, deploying, administering the systems?
   * Is it the challenge of organizational structure, talent, capacity, leadership?
   * Is it communication, silos, lack of alignment?
   * Is it cost, performance, complexity of infrastructure?
* What is preventing you from building valuable data products?

For me, infrastructure cost, performance of existing tools, spaghetti code, lack of data expertise among leadership stakeholders has been the biggest headwinds to progress.

Last year, at one point, our AWS costs were $1.6 for every $1 a customer paid us. After working on a year and reducing substantial tech debt, we got to AWS cost of $0.6 for every $1 revenue. Still, there is no recognition, leadership is reluctant to fix data quality issues.

As a product lead, I have been able to influence some, but it's a lot of compounding challenges.

Does this resonate with folks?

What are the top challenges you are facing?

What are some solutions or workaround that have worked for you?

Looking forward to your responses.",75,64,drc1728,2023-03-13 23:20:00,https://www.reddit.com/r/dataengineering/comments/11qpfwo/what_are_some_of_the_sticky_problems_in_your_data/,False,False,False,False
117djjy,I got certified recently and prepared some notes while preparing for Azure DP-203,"ps: I know that certificates are not really a very important thing. But I do AWS/Azure certifications to get some hands-on practice on the cloud through labs. I use AWS at work, so I took an Azure certification to get my hands dirty with Azure as well.

Recently I've cleared DP-203 and received the Data Engineer Associate certificate. I [shared a post on here](https://www.reddit.com/r/AzureCertification/comments/105oxza/passed_dp203/?utm_source=share&utm_medium=android_app&utm_name=androidcss&utm_term=1&utm_content=share_button) as well.

I prepared some notes on Notion while preparing for the certification. And I'd like to share it with others so that It could help others while doing revision for the exam.

Notes link: [dp203-azure-data-engineering-notes](https://github.com/jithendray/dp203-azure-data-engineering).

Tips that helped me:

- I did a decent course on the Udemy. 
- Made notes while watching tbe last lecture videos.
- The most important thing is - I spent lots of time on doing stuff hands-on than just watching videos. The main goal of this certification for me is not to get the certification, but to be able to use all the services really well.
- Finally, revised the notes that I made a day before the exam.

All the best, for anyone who is preparing for the exam. Feel free to add ⭐ to my repo ;)",73,21,saiyan6174,2023-02-20 16:54:33,https://www.reddit.com/r/dataengineering/comments/117djjy/i_got_certified_recently_and_prepared_some_notes/,False,False,False,False
y9apc6,A good writeup on the newly-announced semantic layer for dbt,,72,12,rmoff,2022-10-20 21:34:48,https://blog.rittmananalytics.com/the-dbt-semantic-layer-data-orchestration-and-the-modern-enterprise-data-stack-78d9d9ed5c18,False,False,False,False
xryruv,Data Quality Comparison on AWS Glue and Great Expectations/Updated with V3 API,,70,8,TallAssociation0,2022-09-30 11:23:18,https://towardsdatascience.com/data-quality-comparison-on-aws-glue-and-great-expectations-70af5bdfe39c,False,False,False,False
wztac0,"Getting hired as a DE (Advice from someone who tried to get hired as a DE, and failed.)","**YMMV!**

I am a self-proclaimed ""general purpose developer"". That means I mostly know one language, Python in this case, and I can do a lot with that language: webscraping, API development, data analytics, etc. So after trying a bunch of things, from the beginning of this year, I started to focus on DE, specifically Analytics Engineering.

If you are applying to an entry-level role and you are being interviewed exclusively by someone who is from the engineering or data team, they only have two criteria-

1. They need a few basic DE specific skills
2. They need something that they NEED

### 1. The few thangs!

The standard DE tools are Airflow, SQL, Python+Pandas, and dbt. And you need to cover a DE cloud stack. For GCP it is: BQ and Cloud Storage. For the ETL process, it could be anything: Cloud Function, Compute Engine or Dataflow.

Now, a bit of greyzone advice-

BI tool... to be honest, you don't need a BI tool that much in DE. So just repeat this after me, ""I have 'explored' streamlit, Looker, and dataflow in the past."" You need to visit their product page and just know what they are, period. The BI tool domain is a slippery slope for DEs because the list of BI tools is endless and BI is a distinct field from DE. A serious recruiter wouldn't demand anything other than that you have heard of the tool they are using. But it is always good if you have built a dashboard or something, but in my opinion, it isn't super necessary.

It's almost the same advice for Spark or Kafka. Look dead center in the webcam or their eye, just say you have explored them. Okay, do explore them, period. Get an understanding of what they do, and have some idea of the syntax. If the data team is using Spark or Kafka, they wouldn't expect you to touch those projects in your first 2-3 months.

### 2. The need

I think you can get operational knowledge of the basic DE skillset in 6 months. But the catch is the NEED factor. The NEED factor can be many things. Every company is different, and they all need that thing that makes them different.

Like Scala. If you know Scala and someone is using Scala in their stack, they will try their absolute best to hire you even if you don't have the hands-on Spark experience. If they are using something like Rust, where there are only a handful of DEs who use Rust, they will try to hire you.

If they are using a method extensively that is often not prioritized in DE, they will try to hire for it. In my case, it is web scraping. Because I am confident in my web scraping skills, companies with large web scraping operations try their best to hire me even though I don't have much DE experience. I also got a few interviews because they were dealing with obscure documentation. I said, I have built a project based on the OLD YouTube API docs (those who knows, knows). They tried their best to hire me.

Now the interesting part is that BI/Dashboard and Spark/Kafka fit in here. In my opinion, they don't fall under the basic needs criteria. These are specialized needs for some DE teams. If you have used them or built projects using them, you would have an edge in a team that uses those tools extensively. But they wouldn't give you an edge everywhere, as not all DE teams use them. That need factor is why some DE recruiters would prefer to have programmers instead of data analysts or vice-versa, because the skillset a recruit brings meets their specific needs.

Essentially, I am saying the need criteria is so diverse, you can't predict your way into it. So specialize in something that relates to DE that you enjoy. It sounds like feel-good advice, but that is just what I figured. Someone who is able to write Java shouldn't focus on building dashboards; they can focus on learning Scala or Rust. If you have data analytics or BI interests, you are wasting your time if you attempt to learn Scala without getting paid for it.

---

So, why don't I have a DE job? I mostly plan to do DE as a side project now. I am a DevRel at a Data first SAAS where I first applied for a DE role! DevRel fits me far better than DE.",73,18,anyfactor,2022-08-28 11:32:31,https://www.reddit.com/r/dataengineering/comments/wztac0/getting_hired_as_a_de_advice_from_someone_who/,False,False,False,False
w335b7,Live Data Engineering sessions and workshops next week at Summer Community Days!,"If you're interested in getting hands-on education on data engineering, check out some of the live sessions available at our conference next week! 

Yes, it's free. No, you won't be getting aggressively sold to 😂

***👉 Workshop - All of data engineering in three hours*** *w/* [Pete Fein](https://www.linkedin.com/in/peterfein/?utm_campaign=Summer%20Community%20Days&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz---XAWgKpwAKhHmmRXud9l0Z4bw8zfDPsOweANTSoA_28MH4hlLpe6QeEJCcUyZtKQfZ8hD)*,* Consultant & Trainer @ Snakedev  
👉 ***Technical Talk - Analytics engineering with dbt: A fintech application*** w/ [Carlo Scalisi](https://www.linkedin.com/in/carloscalisi/?utm_campaign=Summer%20Community%20Days&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz---XAWgKpwAKhHmmRXud9l0Z4bw8zfDPsOweANTSoA_28MH4hlLpe6QeEJCcUyZtKQfZ8hD), Senior Data Analyst @ N26  
👉 ***Actionable advice - Data in a downturn*** w/ [Stephen Ebrey](https://www.linkedin.com/in/stephen-ebrey-456b264/?utm_campaign=Summer%20Community%20Days&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz---XAWgKpwAKhHmmRXud9l0Z4bw8zfDPsOweANTSoA_28MH4hlLpe6QeEJCcUyZtKQfZ8hD), Data Consultant @ Sawtelle Analytics  
👉 ***Actionable advice -*** **Delivering value as a data engineer** w/ [Sarah Krasnik](https://www.linkedin.com/in/sarah-krasnik/), data consultant and advisor)  
👉 ***Actionable advice -*** **Make events a first-class citizen with activity schema** w/ [Timo Dechau](https://www.linkedin.com/in/timo-dechau/), founder & data engineer @ Deepskydata  


If any of these interest you, register for the conference next week!  [🔗 Link](https://www.operationalanalytics.club/summer-community-days)",72,5,JParkerRogers,2022-07-19 20:50:02,https://www.reddit.com/r/dataengineering/comments/w335b7/live_data_engineering_sessions_and_workshops_next/,False,False,False,False
vntp4f,Streaming the Reddit API with Pyspark + Kafka on Docker.,,74,3,SJH823,2022-06-29 23:36:44,https://github.com/stevenhurwitt/reddit-streaming,False,False,False,False
u585sx,What are your thoughts on Data Lakehouses and Open Architecture?,,73,42,AMDataLake,2022-04-16 21:52:57,https://i.redd.it/pqg51tiopyt81.jpg,False,False,False,False
tt0yv7,Handbook for commom pipelines?,"Title. Is there a resource that is basically a handbook for common pipelines/use cases? For example, take a daily csv and upload/qppend to a database table. In my mind, what I want guidance on would be things like when and why to use staging tables, what to put in place to handle workflow failures, deduplicating, etc.

Like probably many of us, I just sort of happened into a role where I do data engineering, so I'm light on best practices.",71,14,senorgraves,2022-03-31 14:27:40,https://www.reddit.com/r/dataengineering/comments/tt0yv7/handbook_for_commom_pipelines/,False,False,False,False
t6xni1,Learning basics Python and SQL: what's next?,"Hi, folks of reddit! 

I currently finished the Practical SQL (Anthony DeBarros) and Python Crash Course (Eric Matthes). Both great introdutory books to SQL and Python, but I don't know where to go next. Should I learn about ETL/ELT? Or should I learn how to use API's and get to know more about pipelines?   


What to do after studying Python and SQL fundamentals? 

Learned so far: 

SQL: 

1. Select, From, Where, Group By, Order By 
2. Subqueries, CTE, Index
3. Regular expressions

Python: 

1. Variables, algebraic expressions 
2. Lists, tuples, dictionaries 
3. For and While loops
4. If, else, elif statements 
5. Functions   


Should I study more deeply the topics above to get a solid knowledge before getting into Data Warehousing, Pipelines, etc?",70,24,DatBoi1337,2022-03-05 00:56:26,https://www.reddit.com/r/dataengineering/comments/t6xni1/learning_basics_python_and_sql_whats_next/,False,False,False,False
qmtkzj,Python and ETL,"Just spent the day dedicated to learning python, specifically pandas and numpy to get a better idea on how I could use this for ETL in the future (no immediate needs). 

I understand there are endless opportunities with a powerful language like this and I’ve literally only scratched the surface, but I wanted to gauge if I’m understanding more or less how pipelines are set up and also get some best practices.

Not to oversimplify, but is the basic gist of ETL in python reading files (or other sources) into dataframes, making manipulations on those dataframes, then spitting back out?

For the load aspect, how easy is connecting to an SQL Server? Does python have some username/proxy it will use on the server to read/write against the database? What will our DBA need to do server side to permit the connection?

Related to SQL, I’d say I spend most of my time there and as a result I find some aspects of working with dataframes weird syntactically speaking. My brain hates denoting fields like [‘Field’] and while using arrays to pass  conditional logic / corresponding choices with numpy is pretty cool, it isn’t as intuitive to me as CASE WHEN. Is it a good practice to get something like pandasSQL or sqlalchemy or is this generally avoided in DE? You can tell me to just suck it up and learn more python if that’s the case.

Thanks in advance to anyone who can help guide my thinking here. I’m completely self taught in this crazy world of data so I’d like to at least start off with python on sure footing.",72,56,PutCleverNameHere69,2021-11-04 20:19:05,https://www.reddit.com/r/dataengineering/comments/qmtkzj/python_and_etl/,False,False,False,False
qev11p,How would you design a pipeline that ingests and processes TERABYTES of data every hour and delivers the data to DWH?,"My company is the developer of a web/mobile application that has 100k+ daily active users. The app is hosted entirely on AWS. I am a junior DE trying to build a pipeline that can do the following:

* The pipeline must be deployed entirely on AWS (like the app backend)
* Ingest and process between ~~1 to 2~~ 0.5 to 1 TERABYTE of data (mainly user action/event logs) per hour every day.
* Deliver the processed data to a DWH like redshfit in near real-time. For example, if a user does something, the action log should be in the DWH within 3 minutes-ish. This is a must for our use-cases, so hourly batch processing is not an option.

I don't have much experience and would love to hear how you would design this pipeline end-to-end. Thank you very much.

EDIT: it's more like 2 TB Maximum possible amount, and on an average day it's more like 0.5 TB - 1 TB per hour, but I guess that's still a lot...",74,53,None,2021-10-24 16:01:01,https://www.reddit.com/r/dataengineering/comments/qev11p/how_would_you_design_a_pipeline_that_ingests_and/,False,False,False,False
gmujcf,"DataCamp is completely free through 5/22, no credit card is required! There are 330+ courses on Python, SQL, Scala, and all the technologies you need mastery of as a data engineer. No risk, all reward!",,71,13,Elvish__Presley,2020-05-19 18:43:58,https://www.datacamp.com/freeweek,False,False,False,False
1camy0x,Informatica vs modern tech stack,"I interviewed for a DE position where their tech stack was AWS and Informatica. 

My current tech stack is Azure, Python, Scala and Databricks. 

I asked why they chose to use Informatica because I've noticed quite a few companies migrating to newer technologies. Their response was basically said ""because they could afford it"".

I've personally not used Informatica so have limited knowledge, however from my research its market leading and they've recently moved to cloud so they seem to be staying relevant. 

However I still think it'd negatively impact my career if I moved from my current tech stack to AWS/Informatica.  Any other thoughts on this?",70,47,elotrovert,2024-04-22 21:42:13,https://www.reddit.com/r/dataengineering/comments/1camy0x/informatica_vs_modern_tech_stack/,False,False,False,False
1bzo6nm,Every DE must be a DA first?,"Hi, I am a computer engineering student trying to get into the data field. 

I was scrolling through this sub and I found that there's what seems to be an implied agreement that every data engineer must start as a data analyst and then become a data engineer as an upgrade.

Just wanted to double check on that to see if I should start as a data analyst or I can just be a data engineer.

Edit: I gotta say how much I appreciate this sub and all the people here for being very helpful and able to share their opinions and experiences so fast.

For anyone seeing this post in the future wondering what is the answer and don't wanna read the whole comment section. Long story short, it's not necessary but it could help, whether by exposing you to more business related use cases, or by helping you land your first data related job as not all organizations hire junior DEs. Also it's not the only option to transition from, it really helps if you are transitioning from being a SWE (most of the comment went through that path)",73,81,sleeping-computer,2024-04-09 09:51:51,https://www.reddit.com/r/dataengineering/comments/1bzo6nm/every_de_must_be_a_da_first/,False,False,False,False
1arpamc,Guiding others to transition into Azure DE Role.,"Hi there,

I was a DA who wanted to transition into Azure DE role and found the guidance and resources all over the place and no one to really guide in a structured way. Well, after 3-4 months of studying I have been able to crack interviews on regular basis now. I know there are a lot of people in the same boat and the journey is overwhelming, so please let me know if you guys want me to post a series of blogs about what to do study, resources, interviewer expectations, etc. If anyone needs just a quick guidance you can comment here or reach out to me in DMs.

I am doing this as a way of giving something back to the community so my guidance will be free and so will be the resources I'll recommend. All you need is practice and 3-4 months of dedication.

PS: Even if you are looking to transition into Data Engineering roles which are not Azure related, these blogs will be helpful as I will cover, SQL, Python, Spark/PySpark as well.

&#x200B;

**TABLE OF CONTENT:**

1. [Structured way to learn and get into Azure DE role](https://www.reddit.com/r/dataengineering/comments/1asegcy/blog_1_structured_way_to_study_and_get_into_azure/?utm_source=share&utm_medium=web2x&context=3)
2. [Learning SQL](https://www.reddit.com/r/dataengineering/comments/1asxqdx/blog_2_learning_sql/?utm_source=share&utm_medium=web2x&context=3)
3. [Let's talk ADF](https://www.reddit.com/r/dataengineering/comments/1atuvav/blog_3_lets_talk_adf/?utm_source=share&utm_medium=web2x&context=3)",71,48,Vikinghehe,2024-02-15 20:29:02,https://www.reddit.com/r/dataengineering/comments/1arpamc/guiding_others_to_transition_into_azure_de_role/,False,False,False,False
18f0tju,What is DuckDB Used For?,"Sorry for the noob question if it sounds ignorant. Here is the situation. 

I joined a new company, and inherited a lot of legacy codes (ETL jobs, written in python) from another colleague who already left before I got to talk to him. Throughout his code, he made extensive use of duckdb just to query pandas dataframes, basically, something like this.

```python3
import duckdb, pandas as pd
df:pd.DataFrame

df.loc[df['value']>20] # Pandas form

query:str='SELECT * FROM df WHERE value>20;'
duckdb.sql(query=query).df() # SQL Form
```

So it seems duckdb gives a SQL like interface to pandas, but is that all it does? Any trade off between just using pandas semantics as opposed to writing a raw SQL query in the code? The pandas semantic looks a lot cleaner to me, and introducing another dependency seems rather superfluous. 

So the basic questions are 
* Why would a developer use duckdb? Does it provide better performance than pandas? Or is the SQL like interface the most important purpose here? 
* The examples are for some legacy codes that I am going through, but for myself, I use polars or dask for tabular data manipulation. Does duckdb work with those frameworks?",71,35,None,2023-12-10 10:35:19,https://www.reddit.com/r/dataengineering/comments/18f0tju/what_is_duckdb_used_for/,False,False,False,False
16xxu15,"Any tool, you regret buying or deploying in the data infrastructure?","For me: I was pushed to use FiveTran and from there the pain started, our simple infra became complicated. Took me almost 8 months to convince my boss to replace that with ELT approach where sink to S3 -> dbt->Redshift",72,108,de4all,2023-10-02 14:17:15,https://www.reddit.com/r/dataengineering/comments/16xxu15/any_tool_you_regret_buying_or_deploying_in_the/,False,False,False,False
12o0euv,Is data lake just a theoretical construct? How does it look on a code level when we say implement in GCP?,on an architectural level...,73,38,Ok-Tradition-3450,2023-04-16 07:56:57,https://www.reddit.com/r/dataengineering/comments/12o0euv/is_data_lake_just_a_theoretical_construct_how/,False,False,False,False
12acdrk,The roast of the modern data stack?,"Just read this and loved it (who doesn't like a bit of drama?)! I'm not to well versed, so I'd be interested in hearing what other people in the industry are thinking? Given the previous posts about costs of data teams etc, I think it's quite interesting.   


[https://medium.com/@laurengreerbalik/customer-empathy-is-dead-10f412782b5e](https://medium.com/@laurengreerbalik/customer-empathy-is-dead-10f412782b5e)  


*Airbyte is such a joke that Reddit* r/dataengineering *users “MyDixonsCider” (say that out loud) and “ThunderCuntAU” seem to have done more due diligence on Airbyte than any venture capitalist willing to give tens of millions of dollars of other peoples’ money to Airbyte’s founders to make the twentieth-worst version of a data replication EL tool that exists on market.*  


*I should be firing Fivetran potentially, when all we mostly use it for is Postgres and Salesforce replications, and I should look into Keboola (or similar) which can give me the same thing with better SLA adherence and better uptime for $10k a year and allows me to “transform and denormalize” before dumping data into my Snowflake, which also will get rid of 25 dbt models and lower my Snowflake bills by a run rate of $30k this year.*",70,75,CalleKeboola,2023-04-03 07:04:24,https://www.reddit.com/r/dataengineering/comments/12acdrk/the_roast_of_the_modern_data_stack/,False,False,False,False
125cd6n,Mountpoint for S3,https://github.com/awslabs/mountpoint-s3,71,13,Sweet-Butterscotch11,2023-03-29 03:56:17,https://i.redd.it/tjtdex6f7nqa1.png,False,False,False,False
z2jh8f,Difference between Data Warehouse and Data Lake?,"Hi,

I'm still confused about the difference and use cases for a data warehouse and data lake. In my understanding what differs a database and data warehouse is OLTP and OLAP. While a database is more transaction and consitency focused, a data warehouse is optimized for big queries which makes it efficient for searching through big data. But why would I use a Data Warehouse like for example the Synapse Warehouse in Azure when I can create a Databricks solution with it's Lakehouse Architecture and Delta Tables that provide ACID? As far as I understand a Data Lake is just a dump for non relational data but you can still load from it since there a connector for Power BI also without the delta layer. So why not load directly from the data lake instead of putting the tables in a data warehouse as a intermediary step? Further, it is recommended to have around 3-4 stages (raw, curated, enriched), making the data lake also structured.  Another point is that a data Warehouse is very costy in Azure at least, while a data lake is quite cheap, so I don't really see the value. Can someone perhaps elaborate? Thanks!",73,34,Ok-Inspection3886,2022-11-23 08:15:48,https://www.reddit.com/r/dataengineering/comments/z2jh8f/difference_between_data_warehouse_and_data_lake/,False,False,False,False
yndu7k,Data Engineering Project - Gmail Manager,"According to Statista, nearly half of the emails sent worldwide are spam. In 2021, it was estimated that nearly 319.6 billion emails were sent and received daily.

Though Gmail marks most of the emails as spam, still we receive bunch of marketing and promotional emails. I have tried to develope the datapipeline to see from what all domains, I receive emails daily. I have created dashboard where I can see all these stats and I can go and block the particular domains which makes my task lil easier instead of going through each and every email and blocking.

Tech Stack :

Python

Airflow

Grafana

&#x200B;

Dashboard Link : [https://snapshots.raintank.io/dashboard/snapshot/E3bVrLkkPYU0XzpfjPRbwZXsLCMlwg7t](https://snapshots.raintank.io/dashboard/snapshot/E3bVrLkkPYU0XzpfjPRbwZXsLCMlwg7t)

&#x200B;

&#x200B;

&#x200B;

[Dash Board](https://preview.redd.it/d7oeprsn09y91.png?width=2742&format=png&auto=webp&s=91692867b9920c391d5eeaa62085576f1ee89950)

GitHub :

[https://github.com/amrgb50/MANAGE-GMAIL](https://github.com/amrgb50/MANAGE-GMAIL)

&#x200B;

[App Flow](https://preview.redd.it/dck7scg8l8y91.png?width=1804&format=png&auto=webp&s=0b4400dfd0678a46ea01f00d295f8b7f7f7b3580)

&#x200B;

Improvements and next plan :

1. I am learning docker and kubernetes. So next step will be containerizing this app and run in cloud.
2. Implementing DQ checks.

Any and all feedback is absolutely welcome! This is my first project and trying to hone my skills for DE profession. Please feel free to provide any feedback!",71,11,Educational-Log-2723,2022-11-06 02:09:25,https://www.reddit.com/r/dataengineering/comments/yndu7k/data_engineering_project_gmail_manager/,False,False,False,False
xmcxi6,Understanding the Snowflake Query Optimizer (one of the best article I’ve read in a long time),,72,5,blef__,2022-09-23 23:39:35,https://teej.ghost.io/understanding-the-snowflake-query-optimizer/,False,False,False,False
whvtnh,5 things I wish I knew about Databricks … before I started.,,70,16,None,2022-08-06 19:11:47,https://www.confessionsofadataguy.com/5-things-i-wish-i-knew-about-databricks-before-i-started/,False,False,False,False
v70tod,What are your hottest dbt repositories in 2022 so far? Here are mine!,"Here are my top5! 🚀  


&#x200B;

https://preview.redd.it/4xun4kj898491.png?width=498&format=png&auto=webp&s=caa1bc10f10ad3cd902d622b4f282a068576a1a3

&#x200B;

\- ⚡️ [Lightdash:](https://github.com/lightdash/lightdash) Lightdash converts dbt models and makes it possible to define and easily visualize additional metrics via a visual interface.  


\- ⏎ [re\_data:](https://github.com/re-data/re-data) Re-Data is an abstraction layer that helps users monitor dbt projects and their underlying data. For example, you get alerts when a test failed or a data anomaly occurs in a dbt project.  


\- 📗 [evidence:](https://github.com/evidence-dev/evidence) Evidence is another tool for lightweight BI reporting. With Evidence you can build simple reports in ""medium style"" using SQL queries and Markdown.  


\- 🧱 [Kuwala](https://github.com/kuwala-io/kuwala): With Kuwala, a BI analyst can intuitively build advanced data workflows using a drag-drop interface on top of the modern data stack without coding. Behind the Scenes, the dbt models are generated so that a more experienced engineer can customize the pipelines at any time.  


\- 🐍 [fal ai:](https://github.com/fal-ai/fal) Fal helps to run Python scripts directly from the dbt project. For example you can load dbt models directly into the Python context which helps to apply Data Science libaries like SKlearn and Prophet in the dbt models.",72,11,kuwala-io,2022-06-07 16:40:21,https://www.reddit.com/r/dataengineering/comments/v70tod/what_are_your_hottest_dbt_repositories_in_2022_so/,False,False,False,False
uhohlv,"Is Kimball's Dimensional Modelling dead in 2022? Is OBT (""one big table"") the way to go?","Seems like his approach to data modelling was more for relational OLAP databases.

Modern data warehouses are column oriented, which means a lot of issues that star schemas were supposed to solve isn't really a problem these days.

In fact, Fivetran released a study that found OBT is 25% - 50% faster than a traditional star schema, which makes sense because it reduces the number of joins you need significantly.

OBT would also make it extremely simple for end users to query.

So, why use Kimball's dimensional modelling in the modern data stack, when we can use OBT or other, newer architectures that are better suited for cloud based data warehouses?

This is a genuine question, not trying to cause drama.. I am new to all of this so I thought I would ask you folks who are experienced. Thanks!",72,31,None,2022-05-03 19:58:30,https://www.reddit.com/r/dataengineering/comments/uhohlv/is_kimballs_dimensional_modelling_dead_in_2022_is/,False,False,False,False
udxpmj,MLOps Zoomcamp from DataTalks.Club - free course about productionizing ML,"Our Data Engineering course is almost over and we're starting a new course about putting ML to production.

We'll cover:

* Processes
* Model training (ML pipelines, experiment tracking)
* Model deployment (web services, batch, streaming)
* Model monitoring
* Best practices 

&#x200B;

Here's more information: [https://github.com/DataTalksClub/mlops-zoomcamp](https://github.com/DataTalksClub/mlops-zoomcamp)",70,4,stolzen,2022-04-28 15:31:34,https://www.reddit.com/r/dataengineering/comments/udxpmj/mlops_zoomcamp_from_datatalksclub_free_course/,False,False,False,False
t4clep,Quarterly Salary Discussion,"This is a recurring thread that happens quarterly and was created to help increase transparency around salary and compensation for Data Engineering. Please comment below and include the following:

1. Current title

2. Years of experience (YOE)

3. Location

4. Base salary & currency (dollars, euro, pesos, etc.)

5. Bonuses/Equity (optional)

6. Industry (optional)

7. Tech stack (optional)",73,175,AutoModerator,2022-03-01 17:00:21,https://www.reddit.com/r/dataengineering/comments/t4clep/quarterly_salary_discussion/,False,False,False,False
r1xvbe,The Missing Semester of Your CS Education,,72,6,killer_unkill,2021-11-25 14:34:31,https://missing.csail.mit.edu/,False,False,False,False
jdp74r,/r/dataengineering hit 20k subscribers yesterday,,70,1,TrendingB0T,2020-10-18 21:55:13,https://frontpagemetrics.com/r/dataengineering,False,False,False,False
ixdtl2,What personal projects would you guys recommend for somebody who wants to do more data engineering?,"I will leave this open ended so that others can benefit too. Let's assume I have a website/GitHub.

Tell me some personal projects that can be done with data that is available which would impress you! Also be sure to mention all the technologies you would use to accomplish said project.",69,35,Black_Magic100,2020-09-22 01:23:35,https://www.reddit.com/r/dataengineering/comments/ixdtl2/what_personal_projects_would_you_guys_recommend/,False,False,False,False
1cfz3r0,To ETL or to ELT? that is the question.,"Do you prefer ETL or ELT, what are the green flags to signal one is the better option for a particular use case?",67,67,AMDataLake,2024-04-29 13:27:24,https://www.reddit.com/r/dataengineering/comments/1cfz3r0/to_etl_or_to_elt_that_is_the_question/,False,False,False,False
1br8jrl,Transforming data with DuckDB in under 3 minutes!,,68,7,Background_Call6280,2024-03-30 03:59:14,https://v.redd.it/kc1i0nzbaerc1,False,False,False,False
1bp4d61,From DA to DE unintentionally ,"So long story short I got hired as a Data Analyst and my manager said they’re changing my role to a Data Engineer. 

I’m slightly freaking out because I feel like I’m not ready or I don’t know anything. The team is super nice and helpful and so is my manager, but the fact that I’m still in school pursuing my masters on top of it makes me feel nervous. 

I feel like I don’t know the first thing about being a DE so I’m constantly on YouTube just watching videos.

Any advice to help me calm down?",70,42,Unlikely_Associate_2,2024-03-27 15:16:08,https://www.reddit.com/r/dataengineering/comments/1bp4d61/from_da_to_de_unintentionally/,False,False,False,False
1b9oqr9,Stop Paying Snowflake for Failing Workloads,,71,16,sahil_singla,2024-03-08 14:05:52,https://baselit.ai/blog/stop-paying-snowflake-for-failing-workloads,False,False,False,False
1ablzb7,Why you're not getting hired -- Tips for those looking for a new job.,"## Context
I've seen a few posts/comments on the top of getting a new job. Talking about a tough job market, learning skills, etc. The comment I wanted to write on a few of those deserved a top level post IMO.

The market is somewhat saturated at the moment with the recent layoffs and hiring freezes/slowdowns, so you won't just get an offer or three thrown at you just because you applied a few places. Here's the simplest way to get hired.

## Differentiate yourself

A hiring manager or recruiter has to sift through tens to hundreds of resumes to fill a position; your resume needs to be have a reason why it should be chosen over your fellow applicants. **If your resume reads like every other applicants', then you're likely not going to get interviews**. The follow up question, then, is how do you differentiate yourself? Here are a few suggestions

1. Tailor your resume to the job description. At the very least, use keywords that the company is looking for. If the company isn't looking for it or it isn't relevant, take it off your resume; it's wasted space.

2. Write each bullet in your job descriptions demonstrating one of either two things (1) business impact or (2) differentiating skills. For business impact bullets, convey the outcome that your actions had on the business and not a description of what you did. For skills-related bullets, mention how you used specialized technology to solve a business problem. You're probably not being hired to be a technologist but to help drive business success.

3. Clearly demonstrate your skill sets!  Saying you can write Python is fine. Having a link to an easy to navigate Github.

4. Have side projects. This is an important carve out of #2 above. If your skill is passion for the industry or willingness to tackle unsolved problems, having a side project is the most effective way to demonstrate this and stand out from the crowd. 

5. Network. If you don't have enough experience to have differentiated job descriptions nor are you willing/able to put in time on a side project, your best bet to get interviews is to bypass the resume screening altogether. Go to events, meet people, tell them you're looking for a job, and/or offer to help them out however you can. If you can't travel, there are online meetups.",69,27,butwhhyy,2024-01-26 16:13:39,https://www.reddit.com/r/dataengineering/comments/1ablzb7/why_youre_not_getting_hired_tips_for_those/,False,False,False,False
18of52n,Was I unprofessional when I said I needed help?,"Im a contractor and I was asked by someone at my client's to help out with another project I wasn't originally contracted for. It's an ERP migration project, so I really only have to write SQL statements and extract some data based on the new ERP system's template from the legacy ERP's backend (SQL Server).

I've never done this kind of work before and largely unfamiliar with ERP systems, but since it's mostly just SQL work I thought I could help out. Project Manager gave me a list of fields I need to look for, but there are about 200 tables in the backend. I said I'd like to speak with the finance team managing this legacy ERP system to narrow down my search, and Project Manager says ""well I saw the table names, they all look pretty straightforward, I think you can just look through all those tables"". But I said ""I've never worked on this domain before so I would really appreciate some guidance"". And she didnt' respond.  I tried searching blindly but ended up finding a table with 10 addresses per customer and I don't know which of these are needed etc. 

Was I being unprofessional for wanting to ask for help to make sure I'm looking for the data in the right places? ",71,59,DataScienceIsScience,2023-12-22 13:39:17,https://www.reddit.com/r/dataengineering/comments/18of52n/was_i_unprofessional_when_i_said_i_needed_help/,False,False,False,False
17d3xk3,What is data engineering *not*?,"Just curious what everyone thinks is more or less “outside” of the scope of a data engineering role (for example, that you may typically expect in a top 500 company, with the understanding that there is some natural variation in exact definition…).

There’s an endless amount of work to be done across any data team, but what sorts of things would you say no to in order to avoid scope creep of your duties as a data engineer?",70,76,No_Newspaper3209,2023-10-21 14:34:12,https://www.reddit.com/r/dataengineering/comments/17d3xk3/what_is_data_engineering_not/,False,False,False,False
179x2d7,Leetcode and System Design for data engineers vs. software engineers?,"How does everyone interview prep, since it seems that all interview prep resources are more for software engineers than data engineers?



In general, software engineering interviews will ask more difficult data structure and algorithm questions.  But for data engineering interviews, I've noticed that the questions ask easier or moderately difficult Python questions involving string manipulations or dictionaries, followed by more difficult SQL questions.




Also, system design for software engineers, it seems they are asked to design a larger variety of services.  For data engineering, I've only been asked about data pipeline design.",70,19,level_126_programmer,2023-10-17 12:20:05,https://www.reddit.com/r/dataengineering/comments/179x2d7/leetcode_and_system_design_for_data_engineers_vs/,False,False,False,False
16vnj0c,"ELI5, what are the main differences between software engineers and data engineers?","May not like I’m 5 y/o per say, but how would you explain the difference to someone who has no clue what data engineers do?",68,39,NFeruch,2023-09-29 21:12:53,https://www.reddit.com/r/dataengineering/comments/16vnj0c/eli5_what_are_the_main_differences_between/,False,False,False,False
14tdmv2,Thoughts on the data janitor (youtube)?,"I recently discovered a YouTuber called ""the data janitor"" who articulates very clearly things that I've rarely heard elsewhere when it comes to getting into data engineering. He has very strong opinions on what are the ways of getting into data engineering and machine learning engineering. I was wondering if some of you know him and if, for those of you who are in a data engineer role, if his takes make sense or not from your point of view. I know the guy’s very assertive “no BS” tone is not everyone’s cup of tea, but I would like to have a discussion on what he actually says instead of his style or the fact that he also promotes his own education platform in his videos.  
  
  
Basically the takeaways from his videos are as follows:  
  
1) Data engineer is not an entry-level role. If you don't have at least one year of experience in a data-related role (data analyst, DBA, etc), there's 99% chance you won’t be hired as a data engineer.  
  
2) A person who wants to become a data engineer shouldn't try to become that first (almost impossible), but should focus instead on a real entry level role such as data analyst.  
  
3) Data roles (DE, DA, MLE, etc) are primarily SQL heavy roles. You can't get away from SQL. Because SQL is not sexy, bootcamps want you to believe that you’ll also need a significant amount of Python (more sexy), but 90% of the time, you don’t.  
  
4) Data roles are very different from software engineering roles. A data analyst is better suited at becoming a data engineer than a DevOps or a Back-end dev.  
  
5) Certifications and certificates of completion are totally different. Certificates of completion (Coursera, Datacamp, etc) that you obtain by simply watching videos and filling blanks are worthless to recruiters. On the other hand, certifications, i.e. you have to take an exam in a physical test center or online proctored and you pass/fail the exam, can definitely have some value, but mostly if they come from the big three (Google, Microsoft, AWS) or traditional tech corporations (Oracle, Cisco, IBM, …). Some of those certifications are very hard to get and thus very respected (example: MySQL 8.0 Database Developer 1Z0-909 from Oracle). Certifications are not worth as much as actual work experience, but it’s still a non-falsifiable signal that you know a tool/framework well enough for a job.  
  
6) He thinks that without prior data experience, if you want to get into data engineering, your primary objective should be to get into a data analyst role first, and to get this role, you need two skills, Power BI and SQL. To signal those skills, two recognized certifications can help if you don’t have any professional experience with them: **PL-300 Microsoft Power BI Data Analyst**, and (since Microsoft deprecated most of its former SQL certs) **DP-300 Administering Microsoft Azure SQL Solutions**. He claims that having those two certifications on a resume can definitely get you interviews for entry-level data analyst roles if you don't have any experience in the field.  
  
  
Thoughts?  
  
For those who are/have been data engineers, do you agree with him or not? Does it depend on the field we're talking (big/legacy tech VS smaller companies maybe)? Or is it broadly true/false?  
  
What I like about him is that he seems very frank and honest about his view of the professional data world, very different from the typical too-good-to-be-true takes that you see here and there that sounds like ""don't worry anon you'll find a job in data if you send enough resumes, plenty of opportunities out there :3"", either because people want you to sign-up to their bootcamp, or just not hurt your feelings.",73,59,Nabugu,2023-07-07 17:27:48,https://www.reddit.com/r/dataengineering/comments/14tdmv2/thoughts_on_the_data_janitor_youtube/,False,False,False,False
13695y9,Checking in: lake houses don't seem to be replacing data warehouses,"Casual observer was thinking the tooling and or feature sets around lake houses would be something to weigh against an actual data warehouse.

I can't seem to find much that would suggest a lake house comes anywhere close to the performance and or features, complex SQL syntax, of a data warehouse. 

Am I missing something?",69,63,hownottopetacat,2023-05-03 03:29:27,https://www.reddit.com/r/dataengineering/comments/13695y9/checking_in_lake_houses_dont_seem_to_be_replacing/,False,False,False,False
11fhmqu,"If dbt is the ""T"" part of an ""ELT"", what do you use for ""EL""?","I'm working on an API data ingestion project, and I usually use AWS Lambda or Python on Databricks for the whole process, but I was wondering if there are any better options/services for Extract and Load part of the process, like dbt is for data transformation.",67,108,we_need_more_lumber,2023-03-01 20:50:13,https://www.reddit.com/r/dataengineering/comments/11fhmqu/if_dbt_is_the_t_part_of_an_elt_what_do_you_use/,False,False,False,False
119oxil,Building a better local dbt experience,"Hey everyone 👋 I’m Ian — I used to work on data tooling at Stripe. My friend Justin (ex data science at Cruise) and I have been building a new free local editor made specifically for dbt core called Turntable ([https://www.turntable.so/](https://www.turntable.so/))

I love VS Code and other local IDEs, but they don’t have some core features I need for dbt development. Turntable has visual lineage, query preview, and more built in (quick [demo](https://www.loom.com/share/8db10268612d4769893123b00500ad35) below).

Next, we’re planning to explore column-level lineage and code/yaml autocomplete using AI. I’d love to hear what you think and whether the problems / solution resonates. And if you want to try it out, comment or send me a DM… thanks!

[https://www.loom.com/share/8db10268612d4769893123b00500ad35](https://www.loom.com/share/8db10268612d4769893123b00500ad35)",68,43,StartCompaniesNotWar,2023-02-23 04:55:25,https://www.reddit.com/r/dataengineering/comments/119oxil/building_a_better_local_dbt_experience/,False,False,False,False
10qzicp,Uber Interview Experience/Asking Suggestions,"I recently interviewed with Uber and had 3 rounds with them:

1. DSA - Graph based problem
2. Spark/SQL/Scaling - Asked to write a query to find number of users who went to a same group of cities (order matters, records need to be ordered by time). Asked to give time complexity of SQL query. Asked to port that to spark, lot of cross questioning about optimisations, large amount of data handling in spark with limited resources etc.
3. System Design - Asked to design bookmyshow. Lot of cross questioning around concurrency, fault tolerance, CAP theorem, how to choose data sources etc.

My interviews didn't went the way I hoped, so wanted to understand from more experienced folks here, how do I prepare for:

1. Big O notation complexity calculation on a sql query
2. Prepare of system design, data modeling for system design. I was stumped on choosing data sources for specific purposes (like which data source to use for storing seats availability)",73,37,bha159,2023-02-01 16:52:02,https://www.reddit.com/r/dataengineering/comments/10qzicp/uber_interview_experienceasking_suggestions/,False,False,False,False
10g3g1r,What mistake have you vowed never to repeat?,Could be something very minor like a specific gotcha in a library or a major architecture decision that bit you in the behind.,69,104,pescennius,2023-01-19 14:44:44,https://www.reddit.com/r/dataengineering/comments/10g3g1r/what_mistake_have_you_vowed_never_to_repeat/,False,False,False,False
106f68v,Are you using an orchestrator like Airflow or Prefect for your project? Why?,"* Which use case/project made you choose a data orchestrator?
* How did you choose one? 
* How was your experience adopting it?
* How do you feel about it now?",71,66,bhavaniravi,2023-01-08 09:35:13,https://www.reddit.com/r/dataengineering/comments/106f68v/are_you_using_an_orchestrator_like_airflow_or/,False,False,False,False
xfhvcw,How to move from BI to DE?,"Right now I mostly cobble sql queries together into stored procedures. This is using either a kimball style data warehouse or against transactional databases. These procedures are then called in ssrs or PowerBI for visualization.

What is next from here - how do I level up?

Should I go further into PowerBI or try to get more into the warehousing side? SSIS is used for etl.",68,44,workthistime520,2022-09-16 04:48:22,https://www.reddit.com/r/dataengineering/comments/xfhvcw/how_to_move_from_bi_to_de/,False,False,False,False
x0rxw1,My list of the best Data Eng podcasts,,68,2,swyx,2022-08-29 15:42:08,https://airbyte.com/blog/best-data-podcasts-2022,False,False,False,False
wx484r,Is your job comprised of a lot of batch ETL work? How are you managing your workflows? How are you handling your parameters?,"Hello, I am a Data Engineer at a company that ingests data in batches from a few large clients. Our general workflow is:

Grab Data From S3 -> Transform -> Upload to our internal platform (mongodb with rest api frontend)

Our transformation process is typically composed of the following things:

- Data cleaning and normalization, which changes on a client-to-client basis depending on how the data was 
originally created. This is typically done with a few python scripts that are tailored to the client

- Enriching our clients' data by pulling from a multitude of different sources (in-house postgres databases, lookup tables, etc), these sources vary depending on the need of the client

- Some orchestration component to string all of the complicated transformations together. We're currently using Argo Workflows in Kubernetes, and each major ETL step is a container that spits out one or more files. We were using Airflow for a bit but felt that it was not fit for our use case. 

If this workflow is at all similar to that of your company, I'm curious as to how (if) you've solved a few of the following things:

- External connection configuration. Most of our ETLs require more than 5 different configuration files and credentials, typically loaded as secrets or configmaps in kubernetes 

- Handling configuration parameter complexity down to individual steps. Many steps in our transformation/cleaning process require 10+ parameters, which typically handle where data is saved, what directory files are loaded from, postgres connection information (see above), etc. It's a lot to orchestrate and manage. These are also typically managed as secrets or configmaps in kubernetes (we have a lot of secrets and configmaps)

- CI/CD: How do you go about intergration testing such complicated pipelines? We typically have a suite of test configs that connect our ETL workflow to dev sources that mimic our prod sources, but this gets complex as we're attempting to mimic the entire system as close as possible. What do you use to deploy changes to your batch ETL processing job? (This is getting more into DevOps, I know, but sometimes it feels like the whole industry is as well...)

This is more out of my own curiosity, as we have a few solutions so far, but I can't help but feel unsatisfied with them, as I often find myself in a sea of different configurations, or struggling to create a fully fleshed out integration pipeline for some of these workflows.",67,30,TheBuddhist,2022-08-25 04:12:01,https://www.reddit.com/r/dataengineering/comments/wx484r/is_your_job_comprised_of_a_lot_of_batch_etl_work/,False,False,False,False
vg3384,How much of python should I know to get a DE junior role?,"I'm currently learning python pandas, numpy,matplotlib.

What else should I focus specifically in python?",70,92,redfaf,2022-06-19 19:51:18,https://www.reddit.com/r/dataengineering/comments/vg3384/how_much_of_python_should_i_know_to_get_a_de/,False,False,False,False
t9i1fx,Have any of you done Data Engineering for small businesses?,"I am wanting to expand my toolbelt and become familiar with tech that my job doesn't allow for (shoutout big brother). Have any of you tried consulting for small businesses in your area? I would be planning to do a full data pipeline and reporting for the customer. I just don't know if they have software from their vendors that might already provide reporting or if their existing software wouldn't allow for a data pipeline to be built. I am also just nervous to be testing new frameworks to work in. 

I am wanting to raise my income and grow my tech stack by consulting small businesses, has anyone had luck doing this?",71,30,Doyale_royale,2022-03-08 14:30:52,https://www.reddit.com/r/dataengineering/comments/t9i1fx/have_any_of_you_done_data_engineering_for_small/,False,False,False,False
t6lpul,Dbt Tests Vs Great Expectations,"Hello,   
Can anyone explain to me the difference between dbt tests and great expectations, what's the perimeter of each one in a pipeline, and how can I get most of them both.  
   
If anyone uses them in production , do you mind share your setup.

Thank you",70,19,dataguy404,2022-03-04 15:40:42,https://www.reddit.com/r/dataengineering/comments/t6lpul/dbt_tests_vs_great_expectations/,False,False,False,False
sqwcq6,Change Data Capture (CDC),,72,15,luminoumen,2022-02-12 17:18:29,https://luminousmen.com/post/change-data-capture,False,False,False,False
r7dyom,"What percent of your work would you say is done using Python, SQL, etc.?",What is your job title?,73,111,jana_50n,2021-12-02 18:34:47,https://www.reddit.com/r/dataengineering/comments/r7dyom/what_percent_of_your_work_would_you_say_is_done/,False,False,False,False
pdt9z1,ETL guy trying to be Data Engineer,"Hello All, i am having 3.4 years of experience in ETL informatica and iam planning to upskill and change my company.

I got a week off from work as a break from burnout and planning to learn some data engineering tools or platforms, which can help me in getting better salary package during my on going job change..

what do you guys recommend and what's your opinions on this.",71,44,Right-Bathroom-5287,2021-08-29 11:02:55,https://www.reddit.com/r/dataengineering/comments/pdt9z1/etl_guy_trying_to_be_data_engineer/,False,False,False,False
o5jjig,Is it just me or ELT seems over hyped?,"I understand how ELT scheme can be beneficial over ETL, but ELT comes with ""conditions"" such as clean data ready at hand to be loaded directly to the DW, and what about the situations with multiple OLTP sources?

ETL scheme is here since the 1980s and it will always work for virtually any situations. Yes, it may not be the most optimal solution in some cases where ELT would shine, but the success assurance of ETL can not be denied.

Can we advocate for a rational approach in choosing the data integration scheme, where the words ""depending on specific business needs"" be bolded everywhere? I have seen ELT schemes that uses extensive Spark workload to clean/wrangle the data, then load to warehouse. If you are already employing Spark then why not use it for Transformation as well?? SQL transformation becomes very costly in MPP due to the sheer number of join operations, which could be avoided in ETL.

Are we going to continue promoting elegance of a solution over practicality?",68,82,AMGraduate564,2021-06-22 10:08:54,https://www.reddit.com/r/dataengineering/comments/o5jjig/is_it_just_me_or_elt_seems_over_hyped/,False,False,False,False
khmfqi,Almost done with my DE Project. Mind Taking a look?,"Hey, everyone! I am in the last part of finalizing my first DE project. I've been working on this for a while now trying to make it the best I possibly can. I know it is far from perfect but, I am super proud of how it turned out and I wanted to share it here so I could get some feedback. I am still new to the field so keep that in mind while you're roasting me for my code not being the work of a god haha.

&#x200B;

I welcome any and all feedback!

&#x200B;

[https://github.com/dylanzenner/business\_closures\_de\_pipeline](https://github.com/dylanzenner/business_closures_de_pipeline)",69,29,TheKoalaKeys,2020-12-21 18:00:24,https://www.reddit.com/r/dataengineering/comments/khmfqi/almost_done_with_my_de_project_mind_taking_a_look/,False,False,False,False
gj722d,"Why GitLab is building Meltano, an open source platform for ELT pipelines",,69,26,MeltanoDouwe,2020-05-13 20:29:17,https://meltano.com/blog/2020/05/13/why-we-are-building-an-open-source-platform-for-elt-pipelines/,False,False,False,False
1bjvuwy,Data Processing in 21st Century ,"Timeline of Data Processing technologies covering from MapReduce to Polars. 

Covering distributed frameworks to single node libraries, mature and recent development. Let me know which one missed in the comments.

Let me know which one have you used, and which one I have missed.

https://www.junaideffendi.com/p/data-processing-in-21st-century",68,22,mjfnd,2024-03-21 02:29:32,https://i.redd.it/ag83iybamlpc1.jpeg,False,False,False,False
1bg8vpi,A Definitive Guide to Using BigQuery Efficiently,"Hi, since I worked on several projects with BigQuery in the past months, I decided to share my learnings in a comprehensive blog posts, helping other Data Engineers to make the most out of their BigQuery usage, burning data rather than money to create real value with some practical techniques.

Looking forward to your experiences and feedback. Enjoy reading:

[https://medium.com/towards-data-science/burn-data-rather-than-money-with-bigquery-the-definitive-guide-1b50a9fdf096](https://medium.com/towards-data-science/burn-data-rather-than-money-with-bigquery-the-definitive-guide-1b50a9fdf096)",67,17,prenomenon,2024-03-16 15:40:38,https://www.reddit.com/r/dataengineering/comments/1bg8vpi/a_definitive_guide_to_using_bigquery_efficiently/,False,False,False,False
169slfu,Been working as a data engineer for 2 years,"(Sorry for the long post….)

I don’t have a technical background but I started as an intern in my current company (IT Fortune 500 and my first and only professional experience since graduation) doing unrelated work.

I realized that I really liked programming so I started taking Udemy courses for that since my company was offering Udemy for free. I also realized that I like data so starting working on that too.

There was an initiative from the company to start developing Data skills, but It fell short because of the lack of interest from my co-workers (we are 1000+ just in my country). I soon took notice of this and knew it was an opportunity to make my way. 
Did the courses and talked to the chapter leader and, like a miracle, there was one POC project that they were working on for a contest to win a project with a client (almost like an auction) but the guy that was the data engineer, and only member of the team, left so they immediately let me work on it. I completed the POC and the client loved it but my company’s price for the whole services package was too expensive.

After that POC I was still working on my previous role (no data or IT related) so I started applying to jobs and got an offer to be a Data Analyst in an important cosmetic company in my country that is very well known for it’s great data department. I told my company about this and they told me not to leave and offered me a 50% increase in salary and the opportunity to lead the data engineering department when it’s founded, so I would be one of the founders of that department, but in the meantime I was gonna be part of the automation and innovation department. I accepted the counter offer and stayed.

Fast forward 1 and 1/2 years and now I’m part of the Data and Analytics team, actually there’s not really a department but the company really wants to start offering Data Consulting services to their costumers so we have one big project that we are working on right now (Databricks, Data Factory, Power BI) and one small one (Azure functions, Docker, Azure VM and Power BI), neither of those work with more than GB’s of data.

The thing is that I’m the only data engineer. There’s one data architect who is really great at data governance, two data analyst and many project managers. A lot of projects are coming our way with many interesting technologies like machine learning, AI, etc. and I’m already burn out because of all the hours that I have to put in only in those projects where I’m the sole builder of the Infra, pipelines, etc. it’s really not a problem because I really like it, but I’m starting to worry because of what’s coming.

So today I have two offers from two different companies to join as a data engineer. None of them are as big as my current company but aren’t small either, but work with bigger data.

I have to make a decision whether to stay here and be one of the pioneers of the data department, be considered for everything (learning and projects) and work on interesting projects because I’m the only one that is available with the skills to do those, or work for any of the other two companies that have a very mature data team that also work with cool projects and where I could also learn a lot from more experience data engineers/architects and work with bigger data.

What path would you take?

Edit: My current company is partner of the year with Google, AWS and Azure, so training and certs are covered.

Edit2: For the people asking what courses did I take for Python, I took Jose portillas Python course and Automating the boring stuff. That being said, I learned more by doing. I made two projects that earned me the offers from the other companies.",69,61,WarNeverChanges1997,2023-09-04 13:53:01,https://www.reddit.com/r/dataengineering/comments/169slfu/been_working_as_a_data_engineer_for_2_years/,False,False,False,False
143t8vc,Dashboards best practices - how much transformation should PowerBI or Tableau be doing?,"Our dashboards at work basically have powerbi doing all the merging and aggregation.

I am not positive if it's done by query or after pointed to the sql tables.

Is that normal, or best practices? I would have built a view and pointed to the view.",70,47,DifficultyNext7666,2023-06-07 23:54:49,https://www.reddit.com/r/dataengineering/comments/143t8vc/dashboards_best_practices_how_much_transformation/,False,False,False,False
1415jdb,How do I get better in SQL,"Apologize if this question has been asked countless times. As a newly hired DE I was advised by my Supervisor to improve my SQL knowledge from a 5 to 8/10 and be more versed in BigQuery and GCP

My previous experience as a Software Developer didn't provide difficult database tasks, so my SQL skills are limited to the basic CRUD and creation of schema and tables. 

Going back to my question, how do I proceed from here? I checked the links in the faq but I still feel lost.",70,22,None,2023-06-05 07:03:23,https://www.reddit.com/r/dataengineering/comments/1415jdb/how_do_i_get_better_in_sql/,False,False,False,False
13492h6,"This reeks of ""we need free work.""","I already accepted a job offer, but emails from places I've applied before are still coming (and will for a while, I imagine). So fortunately I don't feel compelled to do their take-home assignment. There are some who will say that **all** take-home assignments are a scam. I'm not sure about that, but does this raise any red flags for anyone else?

[https://github.com/RiskThinking/work-samples/blob/main/Data-Engineer.md](https://github.com/RiskThinking/work-samples/blob/main/Data-Engineer.md)

The company only has 24 employees and all the data people are data scientists. There are no data engineers, data architects, or data governance. The ""homework"" has some extremely specific parameters, and all of your code (not screenshots) needs to be posted somewhere they can be easily swiped. Or am I being too paranoid?",71,66,lengthy_preamble,2023-05-01 01:31:01,https://www.reddit.com/r/dataengineering/comments/13492h6/this_reeks_of_we_need_free_work/,False,False,False,False
133qlow,Datasets for data engineering projects,"Hi Reddit,

I am new here. Recently I joined a course from udemy to learn more about databricks. Course instructor was using the F1 race dataset from the Ergast website. The website had a well documented table and all the database details. I have completed the course now and I am looking for some random dataset to test my skills. But it is hard to find dataset like that. 

In search of dataset I am writing this post to get views and comments from community who have been in this field for long time, 
Have you guys come across any dataset like that in your career?",72,24,hpal007,2023-04-30 13:27:16,https://www.reddit.com/r/dataengineering/comments/133qlow/datasets_for_data_engineering_projects/,False,False,False,False
